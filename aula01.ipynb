{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### [PPGCC / LADESC] MDA 2024\n",
    "##### Aprendizagem de transferência (TL - Transfer Learning) e Adaptação de Domínio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Introdução\n",
    "\n",
    "Neste tutorial, abordaremos:\n",
    "\n",
    "- Contexto(s) de aprendizagem de transferência\n",
    "- Aproveitamento de modelos pré-treinados\n",
    "- Adaptação de domínio não supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "##### Lembretes da teoria"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### O contexto de aprendizagem supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Temos um conjunto de dados rotulado de $N$ amostras rotuladas: $\\left\\{ (\\vec{x}^i,y^i) \\right\\}_{i=1}^N$, onde\n",
    "- $\\vec{x}^i = \\left(x^i_1, \\dots, x^i_D\\right) \\in \\mathcal{X}$ é uma **amostra** ou **vetor de características**.\n",
    "- $y^i \\in \\mathcal{Y}$ é o **rótulo**.\n",
    "- Para classificação com $C$ classes, $\\mathcal{Y} = \\{0,\\dots,C-1\\}$, então cada $y^i$ é um **rótulo de classe**.\n",
    "- Normalmente, assumimos que cada amostra rotulada $(\\vec{x}^i,y^i)$\n",
    "é extraída de uma distribuição conjunta\n",
    "$$P(X|Y)=P(X)\\cdot P(Y|X)$$\n",
    "- Assumimos que alguma distribuição marginal de amostra $P(X)$ existe.\n",
    "- Queremos aprender $P(Y|X)$ a partir dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Até agora, consideramos principalmente a configuração tradicional de **aprendizado supervisionado**:\n",
    "\n",
    "Assumimos que os conjuntos de **treinamento** e **teste** (que supostamente representam dados futuros não vistos)\n",
    "são ambos da mesma **distribuição** e ambos rotulados.\n",
    "\n",
    "Conseguimos assumir isso porque queríamos resolver uma tarefa com um conjunto de dados e poderíamos\n",
    "portanto, dividir nosso conjunto de dados em tais conjuntos.\n",
    "\n",
    "O que acontece quando esse não é o caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transfer learning (Aprendizagem de transferência)\n",
    "\n",
    "No mundo real, muitas vezes não temos o conjunto de treinamento perfeito para o nosso problema.\n",
    "\n",
    "O que devemos fazer quando a suposição de aprendizado supervisionado é inválida?\n",
    "\n",
    "<img src=\"./img/transfer_learning_digits.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Domínios e tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vamos começar com algumas definições para explicar o problema.\n",
    "\n",
    "- Imagine que temos um **espaço de características**, $\\mathcal{X}$\n",
    "- Por exemplo, $\\mathcal{X}$ é o espaço de imagens coloridas de tamanho 32x32, cada pixel no intervalo de 0-255"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.523013Z",
     "start_time": "2024-10-09T22:03:12.509129Z"
    }
   },
   "source": [
    "import math\n",
    "# tamanho deste espaço de recurso \"limitado\"\n",
    "print(f'10^{math.log10(256**(32**2*3))}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10^7398.1131734380015\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como de costume, temos um conjunto de treinamento $X=\\{\\vec{x}^{(i)}\\}_{i=1}^{N},\\ \\vec{x}^{(i)}\\in\\mathcal{X}$.\n",
    "- Por exemplo, CIFAR-10\n",
    "\n",
    "<img src=\"img/cifar10.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Existe alguma **distribuição de probabilidade** $P(X)$ (também conhecida como $P_{X}(\\vec{x})$) sobre nosso conjunto de treinamento.\n",
    "    - Note que não nos importamos com a distribuição sobre $\\mathcal{X}$.\n",
    "    - Por exemplo, se $X$ for CIFAR-10, a probabilidade de uma imagem toda preta deve ser muito baixa\n",
    "    - Se as classes forem desbalanceadas, probabilidades muito diferentes para membros de classes grandes e pequenas\n",
    "\n",
    "    <img src=\"img/data_dist.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Nosso **espaço de rótulos**, $\\mathcal{Y}$ inclui os rótulos possíveis para amostra em nosso problema.\n",
    "    - Por exemplo $\\mathcal{Y}=\\{0,1\\}$ em classificação binária.\n",
    "    - Podemos ter também $Y = \\{y^{(i)}\\}_{i=1}^{N}$, o conjunto de rótulos para nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Queremos aprender a função alvo $\\hat{y}=f(\\vec{x})$ que prevê um rótulo dada uma imagem.\n",
    "    - Da perspectiva probabilística, aprenda $P(\\hat{y}|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finalmente,\n",
    "    - Um **domínio** de aprendizagem $\\mathcal{D}$, é definido como $\\mathcal{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "    - Uma **tarefa** de aprendizagem $\\mathcal{T}$ é definida como $\\mathcal{T}=\\{\\mathcal{Y},P(Y|X)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Configurações do Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definição** (Pan & Yang, 2010):\n",
    "\n",
    "Dado\n",
    "- Um domínio de **origem** $\\mathcal{D}_S$ (source) e a tarefa de aprendizagem $\\mathcal{T}_S$\n",
    "- Um domínio de **alvo** $\\mathcal{D}_T$ (target) e a tarefa de aprendizagem $\\mathcal{T}_T$\n",
    "\n",
    "*A aprendizagem por transferência* visa melhorar a aprendizagem da função alvo\n",
    "usando *conhecimento* em $\\mathcal{D}_S$ e $\\mathcal{T}_S$, quando\n",
    "- $\\mathcal{D}_S \\neq \\mathcal{D}_T$, ou\n",
    "- $\\mathcal{T}_S \\neq \\mathcal{T}_T$\n",
    "\n",
    "Normalmente também há outras restrições no domínio alvo, como poucos ou nenhum rótulo disponível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quando $\\mathcal{D}_S=\\mathcal{D}_T$ e $\\mathcal{T}_S=\\mathcal{T}_T$ estamos no cenário regular de aprendizado supervisionado\n",
    "que vimos até agora.\n",
    "\n",
    "Por exemplo, dividindo o CIFAR-10 aleatoriamente em um conjunto de treinamento e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": "##### Mesmo domínio, tarefa diferente"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se, uma **tarefa** de aprendizado $\\mathcal{T}$ é definida como $\\mathcal{T}=\\{\\mathcal{Y},P(Y|X)\\}$.\n",
    "\n",
    "Então há dois casos (não mutuamente exclusivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 1: Os espaços de rótulo são diferentes, $\\mathcal{Y}_S \\neq \\mathcal{Y}_T$\n",
    "\n",
    "Por exemplo, o domínio alvo tem mais classes.\n",
    "\n",
    "<img src=\"img/cifar10_100.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 2: As distribuições condicionais do alvo são diferentes, $P(Y_S|X_S)\\neq P(Y_T|X_T)$.\n",
    "\n",
    "Este pode ser o caso quando o equilíbrio de classe é muito diferente nas distribuições de origem e de destino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Mesma tarefa, domínio diferente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se, um **domínio** de aprendizagem $\\mathcal{D}$, é definido como $\\mathcal{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "\n",
    "Novamente, dois casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caso 1: Diferentes espaços de características, $\\mathcal{X}_S \\neq \\mathcal{X}_T$.\n",
    "\n",
    "Por exemplo: $\\mathcal{X}_S$ é um espaço de imagens em tons de cinza enquanto $\\mathcal{X}_T$ é um espaço de imagens coloridas;\n",
    "documentos em diferentes idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 2: Distribuições de dados diferentes, $P(X_S)\\neq P(X_T)$.\n",
    "\n",
    "Por exemplo: o domínio de origem contém imagens desenhadas à mão, enquanto o domínio de destino contém fotografias;\n",
    "documentos no mesmo idioma sobre tópicos diferentes.\n",
    "\n",
    "<img src=\"img/tl_example.png\" width=\"400\"/>\n",
    "\n",
    "Este é um cenário muito comum e geralmente chamado de **adaptação de domínio**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TL é um campo de pesquisa enorme, divido em:\n",
    "\n",
    "<img src=\"img/pan_yang.png\" width=\"1000\" />\n",
    "\n",
    "Neste tutorial, veremos dois exemplos simples, mas comuns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Parte 1: Ajuste fino de um modelo pré-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nós treinamos um modelo em um domínio de origem,\n",
    "e agora queremos usá-lo para acelerar o treinamento para um domínio diferente.\n",
    "\n",
    "Em algumas aplicações, podemos ter muito menos dados rotulados no domínio alvo, tornando inviável treinar um modelo profundo do zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exemplo comum: pré-treinar no ImageNet (1M+ imagens, 1000 classes) e então classificar, por exemplo, imagens médicas.\n",
    "\n",
    "<img src=\"img/transfer-learning-medical.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por que isso funcionaria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs capturam características hierárquicas, com camadas mais profundas capturando características de nível superior e específicas de classe\n",
    "(Zeiler & Fergus, 2013).\n",
    "\n",
    "<img src=\"img/zf1.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/zf2.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Portanto, podemos começar de um modelo pré-treinado e,\n",
    "    - \"Ajustar\" os filtros convolucionais, principalmente nas camadas mais profundas.\n",
    "    - Alterar o classificador (ou removê-lo completamente) para se adequar à nossa tarefa e treiná-lo do zero."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.647252Z",
     "start_time": "2024-10-09T22:03:12.632011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# remove warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('D:/Downloads/mda-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.881396Z",
     "start_time": "2024-10-09T22:03:12.649268Z"
    }
   },
   "source": [
    "import torchvision as tv\n",
    "\n",
    "# Carregando uma CNN profunda pré-treinada no ImageNet\n",
    "# Usando ResNet18, para reduzir o tamanho do download, se necessário use algo mais profundo\n",
    "resnet18 = tv.models.resnet18(pretrained=True)\n",
    "resnet18"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.896927Z",
     "start_time": "2024-10-09T22:03:12.883398Z"
    }
   },
   "source": [
    "# Congelando todas as camadas: desabilitar o rastreamento de gradiente\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.911926Z",
     "start_time": "2024-10-09T22:03:12.898925Z"
    }
   },
   "source": [
    "# Descongelando a última camada (ou o que for relevante para você)\n",
    "for p in resnet18.layer4.parameters():\n",
    "    p.requires_grad = True"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.926927Z",
     "start_time": "2024-10-09T22:03:12.913923Z"
    }
   },
   "source": [
    "import torch.optim\n",
    "\n",
    "# Outra maneira de congelar: taxas de aprendizagem zero para parâmetros específicos\n",
    "opt = torch.optim.SGD([\n",
    "    dict(params=resnet18.layer1.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer2.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer3.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer4.parameters(), lr=1e-4),\n",
    "    dict(params=resnet18.fc.parameters()),\n",
    "], lr=1e-2, momentum=0.9)"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:12.942439Z",
     "start_time": "2024-10-09T22:03:12.927927Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Substituir a parte totalmente conectada por algum outro classificador, por exemplo:\n",
    "\n",
    "cnn_features = resnet18.fc.in_features\n",
    "num_classes = 13\n",
    "\n",
    "resnet18.fc =  nn.Sequential(\n",
    "    nn.Linear(cnn_features, 100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes, bias=True),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:14.488697Z",
     "start_time": "2024-10-09T22:03:12.944442Z"
    }
   },
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "# Importante: precisamos dimensionar nossos dados da mesma forma que os dados de treinamento do ImageNet\n",
    "tf = tvtf.Compose([\n",
    "    tvtf.Resize(224),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carregando os dados do domínio alvo (CIFAR-10 usado apenas como um exemplo simples)\n",
    "ds_train = tv.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tf)\n",
    "ds_test = tv.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tf)\n",
    "\n",
    "batch_size = 8\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=2)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:14.550736Z",
     "start_time": "2024-10-09T22:03:14.489697Z"
    }
   },
   "source": [
    "resnet18(ds_train[0][0].unsqueeze(dim=0))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1595,  0.2325,  0.0834,  0.2905, -0.1117, -0.0960, -0.1420,  0.1619,\n",
       "         -0.0622, -0.2033,  0.0857, -0.4278,  0.1981]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:14.565734Z",
     "start_time": "2024-10-09T22:03:14.553739Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Importante: Somente parâmetros que rastreiam gradientes podem ser passados para o otimizador\n",
    "params_non_frozen = filter(lambda p: p.requires_grad, resnet18.parameters())\n",
    "opt = optim.SGD(params_non_frozen, lr=0.05, momentum=0.9)\n",
    "\n",
    "# O ajuste fino geralmente significa que queremos taxas de aprendizado menores do que o normal e\n",
    "# decaindo-as para continuar melhorando os pesos\n",
    "lr_sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.05, patience=5,)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, loss_fn, opt, lr_sched, dl_train, dl_test):\n",
    "    # Assim como no treinamento regular do classificador, basta chamar lr_scheduler.step() a cada época.\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        \n",
    "        model.train()\n",
    "        for x, y in dl_train:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for x, y in dl_test:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = y_pred.max(1)\n",
    "                total += y.size(0)\n",
    "                correct += predicted.eq(y).sum().item()\n",
    "            lr_sched.step(test_loss)\n",
    "            print(f'Epoch {epoch}: Loss: {test_loss/len(dl_test)}, Accuracy: {correct/total}')"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "##### Parte 2: Adaptação de domínio não supervisionada"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos considerar um problema com domínios diferentes, mas uma tarefa idêntica:\n",
    "\n",
    "- Domínio de origem: MNIST\n",
    "- Domínio de destino: MNIST-M, uma versão colorida e texturizada do MNIST\n",
    "\n",
    "A tarefa em ambos os casos é a classificação usual de 10 dígitos de classe.\n",
    "\n",
    "<img src=\"img/mnist_m.png\" />"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Exemplo de UDA com treinamento adversarial\n",
    "> DANN (Ganin et al. 2015) - Domain-Adversarial Training of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Configuração de DA **Não supervisionado**:\n",
    "\n",
    "Presumimos que não há **rótulos disponíveis** para o domínio alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Precisamos forçar uma CNN a aprender características das formas dos dígitos apenas, não das distribuições de cores.\n",
    "\n",
    "A abordagem, (com base em Ganin et al. 2015):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Treinar um classificador para o **domínio** de uma imagem com base em recursos convolucionais profundos.\n",
    "    - Tentar maximizar a perda do classificador ```domain classifier``` ao treinar a CNN (**perda de confusão** - $Loss$ $L_D$).\n",
    "    - Simultaneamente, minimizar a perda de classificação do ```label predictor``` no domínio de origem (**perda de rótulos** - $Loss$ $L_y$). usando os mesmos recursos convolucionais.\n",
    "- Treinar o classificador de dígitos com dados do domínio de origem e o classificador de domínio com dados de ambos os domínios.\n",
    "\n",
    "<img src=\"img/ganin_da.png\" width=\"1400\" />"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "L_{total} = \\frac{1}{n} \\sum_{i=1}^{n} L_y^i (\\theta_f,\\theta_y) - \\lambda \\left( \\frac{1}{n} \\sum_{i=1}^{n} L_d^i (\\theta_f,\\theta_d) + \\frac{1}{n^\\prime} \\sum_{i=1}^{n^\\prime} L_d^i (\\theta_f,\\theta_d) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Dados de domínio fonte e alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Observação: para que o próximo bloco seja executado, você deve [baixar](https://1drv.ms/u/c/1ce6b4820c5c5590/EXf0iwWLDdFPn9pxMxqSPugB9vrtoWygqlEzfJ9ws8vQjw?e=JrIcf2) manualmente o conjunto de dados MNIST-M e descompactá-lo em `data_dir` que você definiu no início deste notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:14.643780Z",
     "start_time": "2024-10-09T22:03:14.566736Z"
    }
   },
   "source": [
    "from utils.data import MNISTMDataset  # Importa a classe MNISTMDataset do módulo utils.data\n",
    "\n",
    "image_size = 28  # Define o tamanho da imagem\n",
    "batch_size = 4  # Define o tamanho do lote\n",
    "\n",
    "# Define as transformações para o domínio fonte (MNIST)\n",
    "tf_source = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),  # Redimensiona a imagem para 28x28 pixels\n",
    "    tvtf.ToTensor(),  # Converte a imagem para um tensor\n",
    "    tvtf.Normalize(mean=(0.1307,), std=(0.3081,))  # Normaliza a imagem com a média e desvio padrão do MNIST\n",
    "])\n",
    "\n",
    "# Define as transformações para o domínio alvo (MNIST-M)\n",
    "tf_target = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),  # Redimensiona a imagem para 28x28 pixels\n",
    "    tvtf.ToTensor(),  # Converte a imagem para um tensor\n",
    "    tvtf.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    # Normaliza a imagem com a média e desvio padrão do MNIST-M\n",
    "])\n",
    "\n",
    "# Carrega o conjunto de dados MNIST com as transformações definidas\n",
    "ds_source = tv.datasets.MNIST(root=data_dir, train=True, transform=tf_source, download=True)\n",
    "# Cria um DataLoader para o conjunto de dados MNIST\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "\n",
    "# Carrega o conjunto de dados MNIST-M com as transformações definidas\n",
    "ds_target = MNISTMDataset(os.path.join(data_dir, 'mnist_m', 'mnist_m_train'),\n",
    "                          os.path.join(data_dir, 'mnist_m', 'mnist_m_train_labels.txt'),\n",
    "                          transform=tf_target)\n",
    "# Cria um DataLoader para o conjunto de dados MNIST-M\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.346347Z",
     "start_time": "2024-10-09T22:03:14.644779Z"
    }
   },
   "source": [
    "from utils.plots import dataset_first_n\n",
    "\n",
    "dataset_first_n(ds_source, 3, cmap='gray');\n",
    "dataset_first_n(ds_target, 3);"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nosso modelo consistirá em três partes, como na figura:\n",
    "\n",
    "- Uma CNN \"profunda\" para extração de características de imagem (2x Conv, ReLU, MaxPool)\n",
    "- Um classificador de dígitos (3x FC, ReLU)\n",
    "- Um classificador de domínio (2x FC, ReLU), com uma **camada de reversão de gradiente** (GRL).\n",
    "\n",
    "<img src=\"img/ganin_da2.png\" width=\"1400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se: GRL não é utilizado no forward (passe para a frente), mas aplica-se o fator $-\\lambda$ ao gradiente no backward (passe para trás).\n",
    "\n",
    "Como podemos implementar isso?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.362355Z",
     "start_time": "2024-10-09T22:03:44.348352Z"
    }
   },
   "source": [
    "from torch.autograd import Function  # Importa a classe base Function do módulo autograd do PyTorch\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        # Armazena o valor de alpha no contexto para uso na etapa backward\n",
    "        ctx.alpha = alpha\n",
    "        # Retorna a entrada sem modificações\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Inverte o gradiente multiplicando por -alpha\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        # Retorna o gradiente modificado e None para o segundo argumento (não há gradiente para alpha)\n",
    "        return output, None"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.378352Z",
     "start_time": "2024-10-09T22:03:44.364352Z"
    }
   },
   "source": [
    "class DACNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define a rede de extração de características\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),  # Primeira camada convolucional\n",
    "            nn.BatchNorm2d(64),  # Normalização em lote\n",
    "            nn.MaxPool2d(2),  # Camada de pooling\n",
    "            nn.ReLU(True),  # Função de ativação ReLU\n",
    "            nn.Conv2d(64, 50, kernel_size=5),  # Segunda camada convolucional\n",
    "            nn.BatchNorm2d(50),  # Normalização em lote\n",
    "            nn.Dropout2d(),  # Camada de dropout\n",
    "            nn.MaxPool2d(2),  # Camada de pooling\n",
    "            nn.ReLU(True),  # Função de ativação ReLU\n",
    "        )\n",
    "        # Define o classificador de dígitos\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100),  # Primeira camada totalmente conectada\n",
    "            nn.BatchNorm1d(100),  # Normalização em lote\n",
    "            nn.Dropout2d(),  # Camada de dropout\n",
    "            nn.ReLU(True),  # Função de ativação ReLU\n",
    "            nn.Linear(100, 100),  # Segunda camada totalmente conectada\n",
    "            nn.BatchNorm1d(100),  # Normalização em lote\n",
    "            nn.ReLU(True),  # Função de ativação ReLU\n",
    "            nn.Linear(100, 10),  # Camada de saída\n",
    "            nn.LogSoftmax(dim=1),  # Função de ativação LogSoftmax\n",
    "        )\n",
    "        # Define o classificador de domínio\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100),  # Primeira camada totalmente conectada\n",
    "            nn.BatchNorm1d(100),  # Normalização em lote\n",
    "            nn.ReLU(True),  # Função de ativação ReLU\n",
    "            nn.Linear(100, 2),  # Camada de saída\n",
    "            nn.LogSoftmax(dim=1),  # Função de ativação LogSoftmax\n",
    "        )\n",
    "\n",
    "    def forward(self, x, grl_lambda=1.0):\n",
    "        # Expande a entrada para ter 3 canais\n",
    "        x = x.expand(x.data.shape[0], 3, image_size, image_size)\n",
    "\n",
    "        # Extrai características da entrada\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(-1, 50 * 4 * 4)  # Redimensiona as características para um vetor\n",
    "\n",
    "        # Aplica a função de reversão de gradiente\n",
    "        reverse_features = GradientReversalFn.apply(features, grl_lambda)\n",
    "\n",
    "        # Predição de classe\n",
    "        class_pred = self.class_classifier(features)\n",
    "        # Predição de domínio\n",
    "        domain_pred = self.domain_classifier(reverse_features)\n",
    "\n",
    "        return class_pred, domain_pred  # Retorna as predições de classe e domínio"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por que deixar $\\lambda$ (`grl_lambda` no código) mudar durante o treinamento (por exemplo, a cada época)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No início do treinamento, a perda de domínio é extremamente ruidosa, pois os recursos da CNN ainda não são bons.\n",
    "- Portanto, lambda é gradualmente alterado de 0 para 1 no curso do treinamento.\n",
    "$$\n",
    "\\lambda_p = \\frac{2}{1+\\exp(-10\\cdot p)} -1,\n",
    "$$\n",
    "onde $p\\in[0,1]$ é o progresso do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.410354Z",
     "start_time": "2024-10-09T22:03:44.379352Z"
    }
   },
   "source": [
    "# Instancia o modelo DACNN\n",
    "model = DACNN()\n",
    "\n",
    "# Obtém o primeiro lote de dados do domínio fonte\n",
    "x0_s, y0_s = next(iter(dl_source))\n",
    "# Obtém o primeiro lote de dados do domínio alvo\n",
    "x0_t, y0_t = next(iter(dl_target))\n",
    "\n",
    "# Imprime as formas dos dados do domínio fonte e alvo\n",
    "print('source domain: ', x0_s.shape, y0_s.shape)\n",
    "print('target domain: ', x0_t.shape, y0_t.shape)\n",
    "\n",
    "# Passa o lote de dados do domínio fonte pelo modelo\n",
    "model(x0_s)\n",
    "# Passa o lote de dados do domínio alvo pelo modelo\n",
    "model(x0_t)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source domain:  torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "target domain:  torch.Size([4, 3, 28, 28]) torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.0586, -2.3200, -2.6064, -2.4523, -2.3839, -1.5467, -2.6933, -2.6841,\n",
       "          -2.5299, -2.3705],\n",
       "         [-2.9822, -3.3740, -2.7553, -3.0067, -2.9056, -1.5346, -2.6287, -1.8299,\n",
       "          -2.7318, -1.4522],\n",
       "         [-2.6543, -2.5895, -2.4633, -2.3776, -2.5612, -1.9458, -2.8065, -1.6562,\n",
       "          -2.5208, -2.0801],\n",
       "         [-2.6944, -2.5950, -2.8209, -2.7695, -2.7241, -1.8769, -1.9581, -2.0780,\n",
       "          -2.2852, -1.9051]], grad_fn=<LogSoftmaxBackward0>),\n",
       " tensor([[-0.7825, -0.6111],\n",
       "         [-0.8194, -0.5810],\n",
       "         [-0.3430, -1.2366],\n",
       "         [-0.6632, -0.7240]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.425352Z",
     "start_time": "2024-10-09T22:03:44.411352Z"
    }
   },
   "source": [
    "lr = 1e-3  # Define a taxa de aprendizado para o otimizador\n",
    "n_epochs = 1  # Define o número de épocas para o treinamento\n",
    "\n",
    "model = DACNN()  # Instancia o modelo DACNN\n",
    "optimizer = optim.Adam(model.parameters(), lr)  # Cria o otimizador Adam com a taxa de aprendizado definida\n",
    "\n",
    "# Duas funções de perda: uma para classificação e outra para domínio\n",
    "loss_fn_class = torch.nn.NLLLoss()  # Função de perda para a classificação de dígitos\n",
    "loss_fn_domain = torch.nn.NLLLoss()  # Função de perda para a classificação de domínio"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:03:44.440358Z",
     "start_time": "2024-10-09T22:03:44.427352Z"
    }
   },
   "source": [
    "batch_size = 1024  # Define o tamanho do lote para o DataLoader\n",
    "\n",
    "# Cria um DataLoader para o conjunto de dados de origem (MNIST) com o tamanho de lote definido\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "\n",
    "# Cria um DataLoader para o conjunto de dados de destino (MNIST-M) com o tamanho de lote definido\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)\n",
    "\n",
    "# Carrega o conjunto de dados de validação (MNIST) com as transformações definidas\n",
    "dl_validation = tv.datasets.MNIST(root=data_dir, train=False, transform=tf_source, download=True)\n",
    "\n",
    "# Define o número máximo de lotes a serem treinados como o número de lotes do conjunto de dados de origem menos 2\n",
    "# max_batches = min(len(dl_source), len(dl_target))  # Alternativa: usar o menor número de lotes entre os dois conjuntos de dados\n",
    "max_batches = len(dl_source) - 2"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2024-10-09T22:05:21.810140Z",
     "start_time": "2024-10-09T22:03:44.441360Z"
    }
   },
   "source": [
    "# Treinamento com validação\n",
    "for epoch_idx in range(n_epochs):\n",
    "    # Imprime o número da época atual\n",
    "    print(f'Epoch {epoch_idx + 1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "\n",
    "    # Cria iteradores para os DataLoaders dos domínios fonte e alvo\n",
    "    dl_source_iter = iter(dl_source)\n",
    "    dl_target_iter = iter(dl_target)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        # Zera os gradientes do otimizador\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calcula o progresso do treinamento e o valor de lambda para a GRL\n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # Treino do domínio fonte\n",
    "        X_s, y_s = next(dl_source_iter)  # Obtém o próximo lote de dados do domínio fonte\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long)  # Define os rótulos do domínio fonte\n",
    "\n",
    "        # Passa os dados pelo modelo e calcula as perdas\n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
    "\n",
    "        # Treino do domínio alvo\n",
    "        X_t, _ = next(dl_target_iter)  # Obtém o próximo lote de dados do domínio alvo (ignorando os rótulos)\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long)  # Define os rótulos do domínio alvo\n",
    "\n",
    "        # Passa os dados pelo modelo e calcula a perda do domínio alvo\n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain)\n",
    "\n",
    "        # Calcula a perda total e realiza o backpropagation\n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Imprime as perdas e o valor de lambda\n",
    "        print(f'[{batch_idx + 1}/{max_batches}] '\n",
    "              f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "              f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "              )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 / 0001\n",
      "=================\n",
      "[1/57] class_loss: 2.3620 s_domain_loss: 0.9464 t_domain_loss: 0.5523 grl_lambda: 0.000 \n",
      "[2/57] class_loss: 2.1672 s_domain_loss: 0.8822 t_domain_loss: 0.5862 grl_lambda: 0.087 \n",
      "[3/57] class_loss: 2.0093 s_domain_loss: 0.8327 t_domain_loss: 0.5994 grl_lambda: 0.174 \n",
      "[4/57] class_loss: 1.9240 s_domain_loss: 0.7906 t_domain_loss: 0.6295 grl_lambda: 0.257 \n",
      "[5/57] class_loss: 1.8503 s_domain_loss: 0.7493 t_domain_loss: 0.6713 grl_lambda: 0.337 \n",
      "[6/57] class_loss: 1.7511 s_domain_loss: 0.7080 t_domain_loss: 0.6959 grl_lambda: 0.412 \n",
      "[7/57] class_loss: 1.6931 s_domain_loss: 0.6718 t_domain_loss: 0.7265 grl_lambda: 0.483 \n",
      "[8/57] class_loss: 1.6270 s_domain_loss: 0.6475 t_domain_loss: 0.7492 grl_lambda: 0.547 \n",
      "[9/57] class_loss: 1.5462 s_domain_loss: 0.6221 t_domain_loss: 0.7751 grl_lambda: 0.605 \n",
      "[10/57] class_loss: 1.4928 s_domain_loss: 0.6070 t_domain_loss: 0.7832 grl_lambda: 0.658 \n",
      "[11/57] class_loss: 1.4127 s_domain_loss: 0.5982 t_domain_loss: 0.7946 grl_lambda: 0.705 \n",
      "[12/57] class_loss: 1.4078 s_domain_loss: 0.5927 t_domain_loss: 0.8032 grl_lambda: 0.746 \n",
      "[13/57] class_loss: 1.3836 s_domain_loss: 0.5925 t_domain_loss: 0.8064 grl_lambda: 0.783 \n",
      "[14/57] class_loss: 1.3217 s_domain_loss: 0.5967 t_domain_loss: 0.8032 grl_lambda: 0.815 \n",
      "[15/57] class_loss: 1.2938 s_domain_loss: 0.6001 t_domain_loss: 0.7916 grl_lambda: 0.842 \n",
      "[16/57] class_loss: 1.2224 s_domain_loss: 0.6098 t_domain_loss: 0.7854 grl_lambda: 0.866 \n",
      "[17/57] class_loss: 1.1845 s_domain_loss: 0.6207 t_domain_loss: 0.7718 grl_lambda: 0.886 \n",
      "[18/57] class_loss: 1.1493 s_domain_loss: 0.6344 t_domain_loss: 0.7564 grl_lambda: 0.904 \n",
      "[19/57] class_loss: 1.0862 s_domain_loss: 0.6469 t_domain_loss: 0.7419 grl_lambda: 0.918 \n",
      "[20/57] class_loss: 1.0536 s_domain_loss: 0.6621 t_domain_loss: 0.7219 grl_lambda: 0.931 \n",
      "[21/57] class_loss: 1.0036 s_domain_loss: 0.6796 t_domain_loss: 0.7084 grl_lambda: 0.942 \n",
      "[22/57] class_loss: 0.9778 s_domain_loss: 0.6930 t_domain_loss: 0.6967 grl_lambda: 0.951 \n",
      "[23/57] class_loss: 0.9475 s_domain_loss: 0.7045 t_domain_loss: 0.6881 grl_lambda: 0.959 \n",
      "[24/57] class_loss: 0.9432 s_domain_loss: 0.7130 t_domain_loss: 0.6744 grl_lambda: 0.965 \n",
      "[25/57] class_loss: 0.8977 s_domain_loss: 0.7196 t_domain_loss: 0.6687 grl_lambda: 0.971 \n",
      "[26/57] class_loss: 0.8511 s_domain_loss: 0.7237 t_domain_loss: 0.6686 grl_lambda: 0.975 \n",
      "[27/57] class_loss: 0.8522 s_domain_loss: 0.7237 t_domain_loss: 0.6675 grl_lambda: 0.979 \n",
      "[28/57] class_loss: 0.7746 s_domain_loss: 0.7244 t_domain_loss: 0.6667 grl_lambda: 0.983 \n",
      "[29/57] class_loss: 0.7741 s_domain_loss: 0.7244 t_domain_loss: 0.6709 grl_lambda: 0.985 \n",
      "[30/57] class_loss: 0.7992 s_domain_loss: 0.7192 t_domain_loss: 0.6761 grl_lambda: 0.988 \n",
      "[31/57] class_loss: 0.7503 s_domain_loss: 0.7132 t_domain_loss: 0.6838 grl_lambda: 0.990 \n",
      "[32/57] class_loss: 0.7450 s_domain_loss: 0.7113 t_domain_loss: 0.6849 grl_lambda: 0.991 \n",
      "[33/57] class_loss: 0.6456 s_domain_loss: 0.7035 t_domain_loss: 0.6906 grl_lambda: 0.993 \n",
      "[34/57] class_loss: 0.6404 s_domain_loss: 0.6932 t_domain_loss: 0.6982 grl_lambda: 0.994 \n",
      "[35/57] class_loss: 0.6218 s_domain_loss: 0.6914 t_domain_loss: 0.7010 grl_lambda: 0.995 \n",
      "[36/57] class_loss: 0.6186 s_domain_loss: 0.6838 t_domain_loss: 0.7030 grl_lambda: 0.996 \n",
      "[37/57] class_loss: 0.6182 s_domain_loss: 0.6800 t_domain_loss: 0.7058 grl_lambda: 0.996 \n",
      "[38/57] class_loss: 0.5456 s_domain_loss: 0.6757 t_domain_loss: 0.7045 grl_lambda: 0.997 \n",
      "[39/57] class_loss: 0.5803 s_domain_loss: 0.6716 t_domain_loss: 0.7064 grl_lambda: 0.997 \n",
      "[40/57] class_loss: 0.5019 s_domain_loss: 0.6717 t_domain_loss: 0.7031 grl_lambda: 0.998 \n",
      "[41/57] class_loss: 0.5409 s_domain_loss: 0.6704 t_domain_loss: 0.7015 grl_lambda: 0.998 \n",
      "[42/57] class_loss: 0.5336 s_domain_loss: 0.6695 t_domain_loss: 0.7005 grl_lambda: 0.998 \n",
      "[43/57] class_loss: 0.4688 s_domain_loss: 0.6694 t_domain_loss: 0.6989 grl_lambda: 0.999 \n",
      "[44/57] class_loss: 0.4798 s_domain_loss: 0.6697 t_domain_loss: 0.6966 grl_lambda: 0.999 \n",
      "[45/57] class_loss: 0.4930 s_domain_loss: 0.6686 t_domain_loss: 0.6902 grl_lambda: 0.999 \n",
      "[46/57] class_loss: 0.4690 s_domain_loss: 0.6682 t_domain_loss: 0.6920 grl_lambda: 0.999 \n",
      "[47/57] class_loss: 0.4636 s_domain_loss: 0.6699 t_domain_loss: 0.6866 grl_lambda: 0.999 \n",
      "[48/57] class_loss: 0.4294 s_domain_loss: 0.6646 t_domain_loss: 0.6906 grl_lambda: 0.999 \n",
      "[49/57] class_loss: 0.4396 s_domain_loss: 0.6670 t_domain_loss: 0.6868 grl_lambda: 1.000 \n",
      "[50/57] class_loss: 0.4184 s_domain_loss: 0.6676 t_domain_loss: 0.6874 grl_lambda: 1.000 \n",
      "[51/57] class_loss: 0.3832 s_domain_loss: 0.6655 t_domain_loss: 0.6853 grl_lambda: 1.000 \n",
      "[52/57] class_loss: 0.3786 s_domain_loss: 0.6644 t_domain_loss: 0.6826 grl_lambda: 1.000 \n",
      "[53/57] class_loss: 0.3661 s_domain_loss: 0.6644 t_domain_loss: 0.6837 grl_lambda: 1.000 \n",
      "[54/57] class_loss: 0.3511 s_domain_loss: 0.6650 t_domain_loss: 0.6790 grl_lambda: 1.000 \n",
      "[55/57] class_loss: 0.3469 s_domain_loss: 0.6651 t_domain_loss: 0.6843 grl_lambda: 1.000 \n",
      "[56/57] class_loss: 0.3268 s_domain_loss: 0.6649 t_domain_loss: 0.6802 grl_lambda: 1.000 \n",
      "[57/57] class_loss: 0.2982 s_domain_loss: 0.6595 t_domain_loss: 0.6867 grl_lambda: 1.000 \n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualização dos embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "O espaço das features convolucionais aprendidas pelo modelo.\n",
    "\n",
    "Lembre-se, a perda de confusão de domínio deveria fazer com que as imagens de ambos os domínios parecessem iguais para o classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/ganin_da3.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Créditos de imagens**\n",
    "\n",
    "Algumas imagens neste tutorial foram tiradas e/ou adaptadas de:\n",
    "\n",
    "- Pan & Yang, 2010, A Survey on Transfer Learning\n",
    "- Zeiler & Fergus, 2013, Visualizing and Understanding Convolutional Networks\n",
    "- Y. Ganin et al. 2015, Unsupervised Domain Adaptation by Backpropagation \n",
    "- M. Wulfmeier et al., https://arxiv.org/abs/1703.01461v2\n",
    "- Sebastian Ruder, http://ruder.io/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
