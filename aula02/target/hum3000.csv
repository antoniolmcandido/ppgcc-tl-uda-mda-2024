text,sentiment,urgency,confusion
"Welcome to the discussion forum! We are hoping this will be a useful place for us to discuss anything related to the course. Let's try to keep things productive by following a few guidelines:_x0007__x0007_ - Search before you post - someone might have already posted an answer to your question, and it will be easier for everyone if there aren't a lot of duplicate postings._x0007__x0007_ - Use the sub-forums, e.g. discuss quiz questions (without posting solutions!) in the \Quiz and Review\"" forum._x0007__x0007_ - Let’s follow the golden rule in all posts – write to others as you would like them to write to you. We try to interpret each other charitably, treat each other with respect, and be civil and professional in all our posts._x0007__x0007_We are looking forward to working with you all!""",1,0,1
Will we be able to download the videos as in other edX courses?,1,0,1
FYI this is not an edX course but a Stanford OpenEdX course.,1,0,1
Is the email sent to remind me of this course every week?_x0007_I did not receive any email about the information of this course.,0,1,1
"Thanks, lots of interesting code and method to learn from the course.",1,0,1
"I am looking forward to this course, as I am a great fan of [The Elements of Statisical Learning][1]. Thanks in advance to Profs. Hastie and Tibshirani for making this possible!_x0007__x0007__x0007_  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/",1,0,1
Yay!  The class is finally starting!  I've been looking forward to this for weeks.,1,0,1
Thanks for the book apriori...!!All d best everyone for the learning sessions..!!,1,0,1
"Videos are stored in YouTube. For instance Lecture 2.1 is:_x0007__x0007_https://www.youtube.com/watch?v=WjyuiK5taS8_x0007__x0007_Any videos downloader will let you download from YOuTube. Try for instance:_x0007__x0007_https://addons.mozilla.org/en-US/firefox/addon/video-downloadhelper/_x0007__x0007_6 million users._x0007__x0007_Hope this helps,_x0007__x0007_E",1,0,1
"Seriously, have you looked at those questions in light of the material in the lecture? You either need to do a helluva lot better job of explaining the difference between supervised or unsupervised learning, or you need to write questions that are decidable based on the information available in the lecture._x0007__x0007_The first question presents four learning problems._x0007__x0007_1. Predict whether a website user will click on an ad_x0007__x0007_   no training data mentioned._x0007__x0007__x0007_2. Find clusters of genes that interact with each other_x0007__x0007_   no training data mentioned, _x0007_   clustering is mentioned explicitly in the lecture as an unsupervised training_x0007_   problem._x0007__x0007__x0007_3. Classify a handwritten digit as 0-9 from labeled examples_x0007__x0007_   measurable outcome, and training data is available - supervised!_x0007__x0007__x0007_4. Find stocks that are likely to rise _x0007__x0007_   No training data mentioned, so not supervised_x0007__x0007__x0007_And seriously, question 2?   What's up with that question?  The answer has one out of three goals for supervised learning, but because the other two are not mentioned it is incorrect?  This is like a game of simon says?   simon didn't say _x0007_\Understand which inputs a_x000B_ffect the outcome, and how\"" so the answer is incorrect?_x0007_No, the answer is correct, perhaps incomplete, but predicting IS a goal of supervised learning._x0007__x0007_You guys are just playing games here.""",0,1,1
"I have a better understanding of supervised learning after taking the two quiz questions than before, so I'd say they were a good learning tool.",1,0,0
"Hi,_x0007__x0007_I'm not sure whether it's an issue with Chrome but the slides are quite fuzzy and I'm unable to make out the text and graphics in some of the slides. Does anyone else have the same issues?_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",0,1,1
But YouTube is not available for every country.,0,1,1
"QQ Group NO: <zipRedac><zipRedac>749363_x0007_We are from the U.S., welcome you guys to join,_x0007_please note that which university are you from, thanks.",1,0,1
Great!_x0007_ +1,1,0,1
???????????????????????????????????????????,1,0,1
"You have to be able to read between the lines here, which is perfectly fine. Predicting whether a user will click on an ad, implies that you measure whether somebody clicked or not. You use this data as training data, i.e. supervised learning. The same is valid for the stocks. You know which stocks increased and decreased in the past, so you use that knowledge to make predictions about the future._x0007__x0007_As for the second question: it quite clearly states \The goal of **any** supervised learning study is...\"". This means that for the statement to be true, every supervised learning study should have accurate predictions as the main goal. And that is not the case.""",1,1,0
If you want to get the proportion of the volume of a d-dimensional hypercube in the boundary region. You need to know the volume of the interior of the hypercube first. _x0007__x0007_EDITING BY STAFF TO REMOVE ANSWER:_x0007_Can you calculate the volume of the interior of a 1-d hypercube (a line)?  How about a 2-d hypercube (a square)?  Now try to generalize that to d dimensions.,1,0,1
Thanks? +1,1,0,1
Thanks sir!  It's my pleasure to follow this course from you.,1,0,1
Cheers. :-),1,0,1
I could download the videos at work but I cannot watch them there. I can watch them at home but I cannot download at home.,0,1,1
"YouTube is blocked from my country, if I cann't not get access to the lectures, I am afraid I have to stop now. If you are the instructor or TA, can you give me some clarification of the video download issue, thank you.",0,1,1
"Ok, thanks for your reply. Best, <nameRedac_<anon_screen_name_redacted>>",1,0,1
Why we cannot download them directly from page?,0,1,1
"I can't find these lectures on YouTube..._x0007__x0007_The link above from <redacted> does point to one lecture, but I surely can't find any of the others._x0007__x0007_Can anyone post a youtube link that points to any sort of list, etc. of the class lectures as a whole?  _x0007__x0007_It's a shame that the lectures aren't easily downloadable from OpenEDX.  I've never known any other MOOC to not offer downloads of its lectures.",0,1,1
I guess the complete list of videos are here https://www.youtube.com/channel/UC4OWDcPB1peiBXDfCSZ3h-w/videos?flow=grid&view=1,1,0,1
Thanks!,1,0,1
"Duh... didn't see it under \Playlists\""... was just looking at \""Uploads\"".""",1,0,1
Looking forward to very exciting lectures. Thanks for putting this together,1,0,1
"Dear colleagues, _x0007_Thank you for this course and its materials._x0007_my internet speed is low in Iran and i have exhausting problem to online learning. may I ask you to provide a possibility to download course materials include videos, please?_x0007_Bests,_x0007_H.M.V",0,1,1
"0.05 at the left side, 0.05 at the right side.",1,0,1
"Sorry, I meet the same problem as above classmates. We're not able to access Youtube resources. Could the administrators help us out? Thanks!Jerry",0,1,1
"It'll be very helpful too in China, also very low speed and exhausting problem to online learning. Thanks!",0,1,1
"Even in High Definition mode, and with the video set to display at Full Screen, it is difficult to see the lines at the bottom of the window where each operation that you talk about is being shown._x0007__x0007_The bottom line of the output seems to disappear off the bottom of the video, so I can't see the output of one operation until you've moved on to the next operation and the previous output has scrolled up into camera view._x0007__x0007_I'm lucky that I warmed up for this course by doing a couple of weeks of Coursera's Computing for Data Analysis. Thanks to that I can grasp what you are talking about without necessarily seeing it as you do it. However, if I were coming to this video completely cold, knowing nothing about R, then I think I would literally be flying blind.",0,1,1
"I also have minor problem with slides / playing video - in Opera 12.16 (build 1860 windows 7 x64) the \Fill Browser\"" button incorrectly scales video player frame: it became even smaller!_x0007_While \""full screen\"" works OK (but there is no subtitles in such case)._x0007__x0007_Seems switch back to IE solves problem. But this is from work PC - at home I have no IE because I use Ubuntu Linux. Please document compatibility issues if any others exist.""",0,1,1
Please put the download links just below the online video screen. Just like the Introduction to Database course currently online from Stanford.,1,0,0
"Dear colleagues, _x0007_I too have problems of internet connection/low speed. I also would like to have links to videos._x0007_Thank you!",0,1,1
Hi. In line 35 of the downlodable R code for chapter two (ch2.R) appear written **pwd()** that actually looks much as python code. It should be used **getwd()** instead of pwd() as It does not work in R._x0007_Cheers:_x0007_Fer,1,1,1
"Same as the other students, but I'm here in the States.  The connection is unstable, runs for about 18 seconds and ends.  I would like a download link too as I've used it for other courses like Coursera to get around the connection.",0,1,1
It's  amazing lectures. I will enjoy it with my friends and classmates.,1,0,1
"That's my case too. Youtube is selective in our country and I fail to download the courses. I prefer URL to be put in the\Vedio download link\""_x0007__x0007_Peace!!!""",1,1,1
up,1,0,1
I am totally agreed!,1,0,1
"There's R Sessions link  in the navigation bar. I didn't look @ chapter 2 video, but I'd assume that all of R code should be there.",0,1,1
"Hi,_x0007_is there a possibility to see the videos with a 1.25x speedup? I'd like a quicker pace than 1x, but I can't follow 1.5x, and (at least in Firefox) there is no option in between.",1,1,1
"Me too. It would be very helpful to apply the download link for materials like video, slide. For part of us, the video file format \mp4\"" would be convenient for download. Thank you!""",1,1,1
Thank you sir for opening the course. looking forward to the next lectures.,1,0,1
pretty sure 50% overall,1,0,1
Me too. It would be very helpful to apply the download link for materials like video.,1,1,1
Is there any quiz from chapter 1?,1,1,1
"Is there any spanish speaking people interested in creating a group for discussing, just in case, different topics of the course in our mother tongue?",1,0,1
"Just wanted to say thank you very much for putting the course together and making it available!_x0007__x0007_Done with week 1, am blown away by the quality of the material and can't wait for week 2!_x0007__x0007_Thank you so much again,",1,0,1
"I will have to miss the first meeting, due to family commitments, but I plan to attend the meetings after that. _x0007__x0007_Thanks for setting this up, Amir!",1,0,1
Will we receive a personalized certificate in case of satisfactory work?,1,1,1
"For Q1, three of the 4 examples are mentioned either in the video or the course text (the stock rise/fall example is in Chapter 1). I presume the course lecturers assumed we'd read the assigned material not just watch the video.  As for the web ad one, if you think about it a little, it is clear that you need to know which ads have or have not been clicked on otherwise you have no basis to know whether an ad was clicked on or not._x0007__x0007_Q2 threw me & I even have slides on an introductory stats lecture on regression that I give which lists several reasons to perform a regression, so I should have not second guessed myself. Reading it again the answer is clearly FALSE. To be honest, these are just getting you to think about the general topics of the course. You can Answer the TRUE/FALSE question up to 5 times, so not something that is going to affect whether you pass or fail.",1,1,0
I entirely agree!,1,0,1
These are youtube videos. You can download them via keepvid.com and then use VLC player to play at whatever speed you are comfortable with.,1,0,0
It would be great!!!,1,0,1
"This is only my 2nd online course ever, and I just got on board. I would like to do the same thing.",1,0,1
gracias!!,1,0,1
"You can right click on the video clip, then click the first item \copy url of the video\"" on the popup menu to find the download link. This webpage (http://www.wikihow.com/Download-YouTube-Videos) describes how to download YouTube videos._x0007__x0007_Good luck!""",1,0,0
Anybody from staff course could provide us a response?,0,1,1
"The home page for the course, Course Info, says \The deadline for completing all the requirements to get your Statement of Accomplishment is March 21\"".""",1,0,0
I think the quiz questions are perfectly fine dude,1,0,0
Chapter 2 is in slides format. This makes it difficult to print out and follow along without printing a ton of extra pages.,0,1,1
"Many (most?) PDF readers will let you adjust your print settings to print multiple \pages\"" on the same sheet of paper. Perhaps this would solve your problem?""",1,0,1
Many thanks!,1,0,1
i am not been able hear the voice,0,1,1
"Sure 0.05 at each side, but why at each dimension? The definition says **not for every** j x_j is in [0; 1], but **for at least one** j. So there are a lot (2^p - 1) of boundary variants.",1,1,1
What all R packages is used in this course?,1,1,1
Me parece mejor si el foro en castellano se desarrolla en esta página._x0007__x0007_Saludos.,1,0,1
Me too. Thank you very much for this effort.,1,0,1
Think it could be around 10-15 hours a week. If you consider watching the videos and doing the assignments.,1,0,1
I'll bring this up at the staff meeting today and see what is possible.,1,0,0
Exactly right grishace.  Definitely make sure to download the R code so you can follow along!,1,0,0
"Are there any Russian based students? Please, let me know if you want to collaborate during a course.",1,0,1
"I recently completed Coursera's \Machine Learning\"" class, taught by Andrew Ng. It looks like there will be a fair amount of overlap between this class and that one. Can anyone advise on how much will be taught in this class that was not in that one? I enjoy the course so far, but just don't want to repeat too much. Thanks!""",1,1,1
Hi dc321.  I think the slides were provided in the current format because it is the most convenient for viewing digitally.  I will ask the professors about making a printer friendly version as well.,1,0,0
"Hi GJain,_x0007__x0007_There are questions about R the programming language.  Those might be hard to answer with Python..._x0007__x0007_The course/book is centered around R and has sessions that exist specifically to teach R.  You should feel free to use any language, but might not get the full benefit of those sections if you are using Python.  Also, will only be providing support for R.",1,0,0
"Hi ckueny,_x0007__x0007_If you give me a list of topics from that class, I would be happy to tell you which ones we will cover.  My guess is that there will be a fair amount of overlap.  Does Ng's class use R?",1,1,1
"Yes, They are :)_x0007_Privet! and lets collaborate",1,0,1
"??????, ??????!_x0007_? ?? ????????. ????? ???? ??????!_x0007_?? ????? ???????????? ? ??????? ????? ????? ????????._x0007_? ???? ????? ?????? ??-?????????, ?? ??????? ????, ??? ??-??????._x0007_????? ? ??????? ? ??-?????????, ????????, ?????????? ??????!",1,0,1
"Hi! My name is Edward, I'm from Moscow_x0007_Is there anybody from Russia?:)",1,0,1
"Hi! I'm <nameRedac_<anon_screen_name_redacted>>, I work and live in Geneva, Switzerland (French part, too).",1,0,1
"Hello!_x0007__x0007_It's great that I can participate in this course and be taught by the best! :)_x0007__x0007_Best wishes,_x0007_DD",1,0,1
"So far, (through lectures 2.2 in Statistical Learning) there does not seem to be much overlap. Even where the topics are the same (ie, linear regression), the emphasis and philosophy are different._x0007__x0007_I don't feel that one course builds on the other. Rather, they seem to provide different view points of data analysis, which might round out the student's overall understanding.",0,1,1
"Hi! My name is Jordi. I'm from Madrid (Spain). My last work is Credit Risk Analyst in a Bank. Actually I'm unemployed since december of 2013. I'm studying \Computing for Data Analysis\"" with Coursera.""",1,0,1
"Week 1 was just a general chat though, am I missing something?",1,1,1
"Hi everybody I'm <nameRedac_<anon_screen_name_redacted>>, I live in Venezuela and I currently work for the Bolivarian Agency for Space Activities (ABAE) as an engineer. I'm very happy to take part in this course and want to thank everyone involved in making it possible.",1,0,1
Cuenten conmigo.,1,0,1
"Hi Everyone_x0007_I am <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>> working as a consultant with a company called IMT in Winnipeg, Canada. I would like to learn about Statistics and Probability and applications in real world such as Data analytics._x0007__x0007_Regards_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Hi, I'm in Seattle and wonder if anyone in Seattle would be interested in forming a weekly study group to meet for, say, an hour to compare notes, etc, and also just to get to know people. I live in Ballard and can only meet during the week. I am also a Meetups member, and suggest that we could handle scheduling and RSVP through that online tool.",1,0,1
"On the other hand, concerning selecting the stocks that are likely to rise, I still have some doubts: one could also think about performing k-mean clustering, by using a measure of dependence as distance between stocks and see which clusters/stocks have performed well in general in the recent past...",1,0,1
Thanks a lot for the interview! I hope there are going to be a lot more interviews of other people who shaped our nowadays view of statistical learning!,1,0,1
"Hello, I am <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>, a PhD candidate in Agricultural Economics at Kwame Nkrumah University of Science and Technology, Ghana. I am also a Principal Research Assistant University of Cape Coast, Ghana. Its with great excitement that I was introduced to this online course since statistics play a major role in my daily life in terms of studies and work. I therefore hope to get a lot out of it for my studies and my work.",1,0,1
??????! ? ? ???? ?? ???????! ??????? ???????! :-),1,0,1
"Professor, I really enjoyed watching the intro video. As a first time watching these regression models, words can't explain how beautiful I find the model done with thin-plate spline. Extremely excited to start this course!",1,0,1
hey thanks for posting the link too,1,0,1
Splendid!,1,0,1
"Hello world! Me, I'm Joe <nameRedac_<anon_screen_name_redacted>>, a biostatistician working under contract for the National Institute of Child Health and Human Development in Rockville, MD. (But I live across the Potomac River, in northern Virginia.) Looking forward to this course!",1,0,1
"I'm <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>. I live in Brussels, Belgium. I'm an analytics developer working for Oracle. Looking forward to this course too.",1,0,1
"Hi Gaurav,_x0007__x0007_Same here. I am somewhat familiar with Python and do not know R. I am going to try and use Pythona and skip the R sessions. I don't see the point of learning R.  Maybe we can share exoeriences here._x0007__x0007_Regards, <nameRedac_<anon_screen_name_redacted>>.",1,0,1
"<nameRedac_<anon_screen_name_redacted>>, based in Cambridge UK, Paris FR, and sometimes Stanford CA. Learning about functional programming, category theory, bayesian, convex optimisation._x0007__x0007_Thank you for the teachers to provide such good material.",1,0,1
"??????, ??????! ?????????? ?????-?? ?????????? ??? ?????????? ? ????? ??? ?????. ??? ????? <nameRedac_<anon_screen_name_redacted>>, ????? ????, ???????. ???????? ????, ?????, ?????? ?? ????, ?????? ??? ?????.",1,0,1
Hello to everybody... My name's <nameRedac_<anon_screen_name_redacted>> I'm electrical engineer and actually i am finish my master on electrical engineering. My thesis research is oriented on data mining analysis.,1,0,1
"I'm in Seattle. I can meet during the week. An hour or so to compare notes, etc, sounds good.",1,0,1
"I think reducible error consists of bias and variance as presented in the following section. More complicated model may have high variance and low bias, any maybe the total error is higher than simple model. _x0007__x0007_PS. one way to deal with high variance error of complicated models is to use ensemble methods.",1,0,0
Thank you for this course.,1,0,1
"Creo que estaría mejor, como dijo <redacted>, que hagamos el grupo de discusión en esta misma página muchachos.",0,0,1
ok! muchas gracias!,1,0,1
Also there's an R package designed for the book.  See the book website ...,1,0,1
That's **March** ...,1,0,1
Me too! Very useful for slow internet connections!,1,0,1
Anybody else struggling with this?,1,0,1
from ex-USSR (????),1,0,1
"Once I downloaded ISLR_1.0.zip, I loaded as a package._x0007__x0007_But, now how I see the files, for example, Auto.data or Auto.csv?_x0007__x0007_Thank you_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Amazing! Thank you a lot Artib!,1,0,1
Agreed. Watching the lecture provided no information on how to answer this question.,0,1,1
"Hi everyone, _x0007__x0007_I'm <nameRedac_<anon_screen_name_redacted>>! I previously worked for Canadian Wildlife Service and am now starting a Graduate degree out of Trent University in Ontario. I will be studying population dynamics and reproductive success of bank swallows so this course will have a great application for analyzing my findings.",1,0,1
"If you have problems with eating this problem at once, I suggest that you start with lower number of dimensions to see how the area changes. Then, it might be easier to find a general solution.",1,0,1
"Olá,_x0007__x0007_I'm very happy with the course and contents and feeling guilty because I gave not the appropriate attention to this discipline before. _x0007__x0007_Thanks for the opportunity!",1,0,1
"Did you check out the \Lecture Slides (pdf)\"" link?  After clicking there, you can open the slides for Chapter 2 and scroll down to slide 30...""",1,0,0
"I would like to suggest a hint, considering the volume of the inner space, i.e. the space other than the bound region.",1,0,1
"yes, these are the slides, e.g. https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/statistical_learning.pdf The plot on page 30 has nothing to do with the question...",1,0,1
"Hello Everyone,_x0007__x0007_My name is Dave <nameRedac_<anon_screen_name_redacted>> and I'm Director of Quality and Reliability for a high tech start up company working in Rochester, NY.  I have a degree in statistics but these topics weren't widely available back then so I'm trying to stay current.  Good luck to all._x0007__x0007_http://www.linkedin.com/in/davidlee5",1,0,1
"ch2.R references Auto.csv, is this file available for download?",1,1,1
"The quiz is related to the lectures in the sense that illustrate the extent of \the curse of higher dimensions\"" when used in euclidian distance based problems like nearest-neighbor._x0007_I've never thought about the problem and doing the simple calculation proved the point very clearly!!_x0007__x0007_An hypercube is just a cube in n-dimensions... just extend the way you calculate the volume for 3d to n-d""",1,0,0
"I also took ML course and from what I have already read from ISLR and ESL I think Andrew Ng's was more oriented towards computational aspects while this course is more about actually understanding the limitations and the possibilities of different models. _x0007__x0007_In particular, two very nice topics this course is going to cover are LASSO and variable selection methodsd and trees, which were not covered in ML. And enhacement methods like bagging and boosting. _x0007__x0007_I think this course is going to be a really nice continuation of ML with more emphasis on getting an understanding on how the methods work, which are their strengths and weaknesses at a more deeper level.",1,0,1
"There is also an R package associated with the book (\Introduction to Statistical Learning with Applications in R\"").  You can find it here:_x0007__x0007_http://cran.r-project.org/web/packages/ISLR/index.html_x0007__x0007_The \""Auto\"" dataset is available within this package.""",1,0,1
"I don't know if your system means you have a different view to me, but presuming you don't: the questions aren't embedded in the video, they are accessed by clicking the \itemized list\"" icon on the right side of the bar directly above the video. Hope that helps.""",1,0,1
"That's a good hint. It may be just because I have done it, but it seems straightforward if you simply think of the problem geometrically.",1,0,1
"Hello Everyone,_x0007__x0007_My name is <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>.  I have an degree in Engineering form the University of Toronto.  I have just completed Prof Ng's course in ML on coursera.  The latter was excellent and this course looks like a fantasitc way to add to my knowledge.  Thanks profs Hastie and Tibshirani for making it available._x0007__x0007_Cheers!",1,0,1
"Hi all,_x0007__x0007_my colleagues at DataRobot and I were pretty excited when we first heard about the course. Since we are using both Python and R to build statistical learning software, we decided to create a series of blog posts that follow along with the course and show how many of the statistical learning techniques presented in the course can be applied using tools from the Python ecosystem such as [scikit-learn][1] and [statsmodels][2]. You can find the first posting here: http://www.datarobot.com/category/stat-learning/_x0007__x0007_We hope that we can give some insight to people who are new to Python but familiar with statistical learning packages in R. For my part, I've co-authored some of the algorithms in scikit-learn that are covered in this course and there are often some subtle differences between implementations in scikit-learn vs. their reference implementations in R (e.g. gbm, randomForest) that are worth knowing._x0007__x0007_best, _x0007_ <nameRedac_<anon_screen_name_redacted>>_x0007__x0007__x0007_  [1]: http://scitkit-learn.org_x0007_  [2]: http://statsmodels.sourceforge.net",1,0,1
I use Ubuntu and your answer is correct.,1,0,1
"Cool! that sure sounds interesting!_x0007__x0007_Btw, thanks for the effort you're putting in it for us to benefit!",1,0,1
"An alternative path is to look at the first few terms and then attempt a recursive or inductive approach. The attempt might, at the very least, help you get some traction on the problem and hint at the underlying pattern. More rewarding when the geometry isn't as convenient as this one.",1,0,1
"Hello Everyone,_x0007__x0007_This is <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>> from Plano, TX. I just finished PhD in Mathematics. I'm now a Statistician at Capital One. _x0007__x0007_I've been self-studying the textbook for a while and have replicated some examples in R. I'm so thrilled Dr. Hastie and Dr. Tibshirani are offering this online course. Feel free to connect with me on Linkedin: http://www.linkedin.com/in/daoyinglin_x0007__x0007_Warmest Regards,_x0007__x0007_DLin",1,0,1
It seems that the MSE formula's being shown here have an extra term that is not present in other areas (Wikipedia for one: http://en.wikipedia.org/wiki/Mean_square_error)._x0007__x0007_From the Lecture: MSE = Var(f(x_hat)) + Bias(estimator)^2 + Irreducible Variance_x0007_From Wikipedia: MSE = Var(Estimator) + Bias(Estimator)^2_x0007__x0007_Is the Irreducible Variance and the Variance of f(x_hat) a result of the expansion of the variance of the estimator Term from wikipedia? Are they relying on different underlying assumptions? Is it separated out by assuming that the Irreducible Variance and the Variance of the estimator are independent and thus have a Covariance of zero?,1,1,1
"If the slides are being typeset with the Beamer package in LaTeX, may I suggest using the handout option, as in documentclass[handout]{beamer}, to reduce the number of pages for those of us who want to print them out? For example, the statistical learning slides are 37 pages, but fit on 30 slides. At 4-to-1, that's an extra two pages. Given the number of enrollees who might want to print them, that adds up quickly. I noticed the linear regression slides are 64 pages on 48 slides. _x0007__x0007_See section 20 of the Beamer manual for more details if necessary.",1,0,1
"I can't see the videos within the Stanford application. It doesn't matter if I use the main browser or the one on the desktop. However, if I copy the URL and go to you-tube it works fine.  Hope this works for you. It seems like it is going to be an interesting class. If it matters to someone tracking this kind of thing, I am using Windows 8.1 on MS Surface Pro.",1,1,1
I used to download video and watch them latter. I do not have proper internet connection at home. So I use office computer to download teaching materials. Could you please provide an option to download videos.,0,1,1
I am also looking forward to this class. I am a Econ major at Cal state Fullerton so this class will come in handy later in my studies. I was especially attracted to the part about predicting market behaviors.,1,0,1
Thanks for offering this course.  I'm looking forward to this learning experience!,1,0,1
"My name is <nameRedac_<anon_screen_name_redacted>> and I recently finished a MSc in Applied Sociology. I'm looking forward to learning more stats methods and this will be my first foray into learning a programming language. _x0007__x0007_Grateful for this fantastic opportunity,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"I was also planning on attempting to do the exercises in both languages concurrently. I have a strong python background and have been coding it as my primary language for my own research, but my work does not require the use of libraries like scikit-learn and pandas, etc. I'm definitely curious about R, and I thought it would be a good opportunity to learn it as well as teach myself about a part of the scipy stack that I haven't played with too much. I'm looking forward to following the blog.",1,0,1
Hello!_x0007__x0007_I'm <nameRedac_<anon_screen_name_redacted>> from Malaysia but currently working in Singapore as a Research Assistant in a university research centre. My main interests are in user and usability research. This course definitely will help in my exploration of R and using stat learning to inform user requirements for product development.,1,0,1
"I have the same request. My Internet connection is not so well that the videos could not be played online, so I have to download them before watching. Could you please prove the download link for us? Thank you",0,1,1
I also clicked on the User Original Player link on the video and then I could see the entire screen. Thanks,1,0,1
wow,1,0,1
"Howdy Folks_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> here from New Delhi. Doing my masters in economics and have studied two semesters course work on regression and i hate the obsession of economists to trying put regression on every problem. So, that explains my interest in statistical learning and an opportunity to get answered from the authors themselves.",1,0,1
Hi Murali_x0007_I am also from India and am from engg background. what college are you from ??,1,0,1
"Hey Guys!!_x0007__x0007_Is there anyone from India ? Would definitely love someone from New Delhi region as unfortunately ,i have no buddy who is into statistical learning. Hoping to hear from you guys!!",1,0,1
"Me too, doing modelling at UWA, Perth",1,0,1
"Hi, this is <nameRedac_<anon_screen_name_redacted>> from Bangalore, India. I'm an engineer and I run a small learning lab at http://www.gloschol.com. Thank you, Stanford. Great to be here! Looking forward to learning from all classmates, as well._x0007__x0007_Feel free to connect at http://www.linkedin.com/in/linkedin2<nameRedac_<anon_screen_name_redacted>>",1,0,1
"I have the same problem. I've tried Chrome, Safari, FF",0,1,1
I am so excited about this course!!!,1,0,1
"Hi, Roy. I'm open to this kind of meeting, too.",1,0,1
I am from Bangalore. Good luck with the course.,1,0,1
"@Gaurav_x0007__x0007_I am also a bit more comfy with Python than with R. Still R is unparalleled in some areas, as I have seen around. I would try to learn as much as possible about R & Stat._x0007__x0007_Cheers.",1,0,1
"ditto, ta",1,0,1
Yep it is **21-MARCH-2014**! :)_x0007__x0007_Are all deadlines set at 21/March/2014? Great if yes!,1,0,1
"well, i am a masters student in economics from Delhi.",1,0,1
Im from Chennai,1,0,1
"No, audio for first few lectures are fine. Only problem is that audio volume is little less.",1,1,1
Hope to get full benefit of the course. Thanks.,1,0,1
"Hi, I am <nameRedac_<anon_screen_name_redacted>> from Lisbon, Portugal, working at open university at social science department.",1,0,1
i have a 4 mb connection and am unable to open the videos anyone else having the same problem,0,1,1
"***Assalamu Alaikum*** (Peace be Upon All of You)_x0007__x0007__x0007_I am <nameRedac_<anon_screen_name_redacted>> from Bombay, India. My job as a Statistician involves doing risk modelling, predictive analytics, primarily in the area of Credit Risk. _x0007__x0007_This course should be immensely helpful to me to refresh (and learn from a new angle) the things that we were taught in University in M.Sc. (Statistics) as well as learn some modern (and not so modern) techniques straight from the horse's mouth. I mean Prof Efron: Bootstrap; Prof Chambers: S (and R) and of course, the Course Instructors- Prof Hastie-Tibshirani: GAM, LASSO._x0007__x0007_Thank you, Stanford !",1,0,1
"Ciao! I'm <nameRedac_<anon_screen_name_redacted>> from Italy. I've got a PhD in Aerospace Enginering and I'd like to learn about data analysis and R. This looks like the perfect choice for my goals! Actually, I was suggested to attend the course as part of a Analytics Engineer program I'm following at my company. Looking forward to studying this topic together with all of you!",1,0,1
"Hi! I started reading the course textbook, \An Introduction to Statistical Learning_x0007_with Applications in R\"" (ISLR), and I challenged myself with a couple exercises. Is it possible to get the answers at least for selected exercises? I think it would help self-study a lot. Thanks!""",1,1,1
"Thank you, Sir, & Prof. Tibshirani _x0007__x0007_ - for this course_x0007_ - for the free text books (both ISL & ESL)_x0007_ - for the open source codes in the *lingua franca* of Statistics_x0007_ - and for your contributions to the Subject of Statistics_x0007__x0007_(not necessarily in that order :-))_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
I am from Mumbai,1,0,1
From Vijayawada,1,0,1
Thanks it is really helpful,1,0,1
"Actually it's 1.2 Billion strong :)_x0007__x0007_Maybe if we find couple of guys around our cities, then we can arrange a meet up and exchange notes and best way to learn ML is doing a size able project in a team. what you guys think ??",1,0,1
"Thank you , Sir! Its great to have this Stat learning class from you and providing valuable free resources. It'll be great pleasure for me to learn the subject which would be helpful in academic research.",1,0,1
"You can start with a simple line. Then, continue with a square. Then, with a cube. The \pattern\"" emerge. It took me all afternoon to figure it out.""",1,0,0
"Nice to meet you too, Lovekesh!",1,0,1
I'm on the Chrome Browser with the Mac OS. Double clicking the video does a full-screen without cutting off the bottom of the video. Clicking on the full-screen button cuts off the bottom of the video.,0,1,1
Guys!! lets work out a meetup kind of event for stats learning with specific focus on Data science. Data science being considered as the greatest buzzword 2014 hiring,1,0,1
nice,1,0,1
"Hi,_x0007_I am <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>, from Delhi.",1,0,1
I love statistics and I study Statistics in Brazil in Universidade Federal de Goiás.,1,0,1
"Test Set MSE can be decomposed into Bias, Variance, and Irreducible Error. \Bias\"" sounds worse than \""Variance\"". Are they equally bad or is one more bad than the other?""",1,1,1
up,1,0,1
"Big test error is bad._x0007__x0007_Both variance and bias (in power of 2) are positive values, so they both can affect the error._x0007__x0007_Which of two (variance or bias or both) will affect test error more depends on the data and its origin (its underlying nature).",1,0,0
lecture on bias-variance tradeoff_x0007_http://www.youtube.com/watch?v=zrEyxfl2-a8,1,0,1
"Hi, I'm <nameRedac_<anon_screen_name_redacted>> from Toronto, Canada. I have a background in software engineering and am currently working on a cancer genomics project.",1,0,1
"Let's say you have a high test error and you would like to reduce it._x0007_So you should compare test error with train error._x0007__x0007_1. If train error is high, you have high bias. It is also known as underfitting. It means you are using too simple (not flexible) model. So you should try to use more complex (flexible) model. _x0007_For example, you have high bias if you are trying to fit quadratic function with linear regression._x0007__x0007_2. If train error is low, then you have high variance. It is also known as overfitting. It means you are using too complex (flexible) model. So you should try to use more simple (not flexible) model. _x0007__x0007_Also, you should definitely have a look at https://class.coursera.org/ml-003/lecture/62",1,0,0
MSE in relation to shrinkage._x0007_http://demotrends.wordpress.com/2013/09/04/the-bias-variance-tradeoff-what-it-means-for-quantitative-researchers/,1,0,1
One more thankful student of this great course!,1,0,1
"Nice tool, specially when having slow connection.",1,0,1
"The textbook and the lectures have a very specific definition of bias:_x0007__x0007_![bias][1]_x0007__x0007_Does this definition capture the other forms of statistical bias (sample bias, omitted-variable bias, reporting bias, exclusion bias, analytical bias)? _x0007__x0007_http://en.wikipedia.org/wiki/Bias_(statistics)_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>97.png",1,1,1
ooooh felt so good getting it right. The real difficulty is in trying to understand what it's asking for!,1,0,1
nice,1,0,1
I'm having the same problem. Video not available is the message I'm getting. I'm using chrome browser.,0,1,1
<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>> from Bangalore,1,0,1
Nice answer.,1,0,1
This helped tremendously. It's just very hard to understand the question if you're not used to the math.,1,0,1
Hi There... i am from Bangalore...,1,0,1
Great +1,1,0,1
Chrome Version 32.0.1700.76 m,1,0,1
"Thanks Dmitri, it worked for me.",1,0,1
"Buenas tardes a todos. Nos quedamos aqui. He intentado entrar en la comunidad r-hispano y la verdad es bastante rollo, además has de enviar un mail para que te den una contraseña... etc. Yo creo que mejor aqui.",1,0,1
> The deadline for completing all the requirements to get your Statement of Accomplishment is March 21.,1,0,1
cool,1,0,1
Thanks :),1,0,1
"I do not think that's the case. In wikipedia $y$ is just some function. Here we think of $y$ more precisely - $y$ is a function (a sum actually) of some other function ($f(x)$) and independent random variable ($epsilon$)._x0007__x0007_If we won't use that assumption, than we'll get same result that are on wikipedia._x0007__x0007_But if we use that assumption, than we are getting more precise representation of $operatorname{MSE}$._x0007__x0007_I think it does not matter if we consider only ${x_0, y_0}$ or all of the points. Calculations would be the same.",1,0,1
"Hello world¡¡ My name is <redacted> <redacted> from Spain, <redacted: [Spanish Town]>. I am engineer and now like many spanish people i am unemployed, and machine learning seems very interestring...",1,0,1
"Me too, please allow us to download thank you.",1,1,1
"Oh, I see your problem now. Well, I suppose you could either delete the \still transitioning\"" pages from the PDF prior to printing (many PDF readers let you do this), or print the PDF in carefully-selected ranges that avoid the pages you don't want. Still somewhat time-consuming, but at least you're conserving paper and ink.""",1,0,1
Bangalore it is.,1,0,1
Hej Dan!_x0007_Roligt att se någon från Sverige här! Mina hälsningar till Stockholm :) Själv kommer jag dit om en månad // <nameRedac_<anon_screen_name_redacted>> från Moskva,1,0,1
How does  higher the flexibility of model reduces the bias and increases the model variance?,1,1,1
"Hi, Im <nameRedac_<anon_screen_name_redacted>> and Im from Colombia, and I was living two years in Argentina; Im environmental and Sanitary Egieener, and I was doing a master about Water Management in Argentina. Actually Im working in my country.  Guys Good luck to everyone, and nice to meet u.",1,0,1
"Maxed out the volume, tried different browsers.  Still no luck.  Fortunately the text of the lecture is provided so I am reading that, but it would be nice to get the audio.  I can hear the lectures on YouTube, but they are not all there.",0,1,1
"Thanks, smgross. I didn't even notice the page numbers on the slides.",1,0,1
Final answer is counted.,1,0,1
"Hats (\^\"") are generally used to represent fitted values.  For example, $hat{f}(x)$ is our fitted regression function (as opposed to the true regression function $f(x)$).""",1,0,1
Same with me. No clue how to solve this problem.,0,1,1
I can not open the R sessions chapters. Is anyone else having this problem?,0,1,1
"Is R code available for all figures, images in the book \An Introduction to Statistical learning\"" ? For example for FIGURE 2.3. etc....""",1,1,1
"I am really excited about the course._x0007_I am a great fan of ESL so taking a course based on its contents taught by its authors is just, uah!!!_x0007_Thanks for the initiative",1,0,1
Mine are opening just fine. You do need R installed on your system for these to open up though (these are code documents...),1,0,1
I would think that in general variance is worse for inference simply because more bias and less variance usually means a simpler model.  For prediction you'd probably want as much variance as still yields good results on the test data.,1,0,1
"Yeah, me too. Coursera makes this easy. Why not here?",0,0,1
"???????, ??? ??? ??? ?????! ???? ????? ???? ,)_x0007_?? ????? Coursera ????? ???????? ?????? ? FB, ???????? ? ???????????? ???????? :)_x0007_??????????? ?????? ???????, ?? ??? FB ??????? ??????? ???????? ????-?? ? ??????..._x0007_?????? ? ???? ???-?? ? ???????????._x0007__x0007_??????????, ???, ??? ??????? ??? ?????? ???????? ???? :) <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,0,1
"Guys, Thanks a lot !!! Never thought these boards were any useful. How wrong I was. Much appreciated.",1,0,1
Hi !_x0007__x0007_As many I would like to be able to download the video lectures along with the subtitles. Thanks in advance._x0007__x0007_Cheers !,1,1,1
"Hola todos,_x0007__x0007_Un gusto acoplarme al grupo, soy de La Paz - Bolivia. Sea en r-hispano o aqui,  estoy dentro.",1,0,1
Thank you. Looking forward to fortifying old learning and learning anew.,1,0,1
"I've posted an announcement in the Google Group regarding tomorrow's meetup:_x0007__x0007_https://groups.google.com/forum/#!topic/austin-statistical-learning-mooc-meetup/E-EpvKmPXJE_x0007__x0007_Please join the Google Group for further communication._x0007__x0007_Thanks,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,0,1
"While I wouldn't put it the same way as the anonymous poster, I too dislike the questions he or she refers to. The true/false one is particularly annoying. It puts the student who understands the material to the guess: is this a trick question or not? Are they testing that I know the difference between supervised learning (predict something) and unsupervised learning (cluster), or are they testing that I know that prediction isn't the only goal of supervised learning? _x0007__x0007_If they want me to show them that I know what supervised learning is, and I say FALSE, they're going to tell me I'm overthinking and I'm a smartass. If they want me to know that there are more possible goals than just prediction, and I answer TRUE, they're going to tell me that I'm simpleminded and don't realize that prediction is just one possible goal of supervised learning. I have to read the mind of the test creator to know which way to answer._x0007__x0007_I hate it when I have to do mindreading to answer a quiz question. It doesn't make any difference here in the scheme of things, because I get five tries to guess which of the two answers is right, but still. This is a poor question.",0,1,1
I am in NE Denver and would like to meet with others to study.,1,0,1
"Thank you for the course. I'm taking it because it's interesting, and maybe I'll find ways to use it later. Last year, I downloaded and started reading ESL, but while I could conceptually understand the presentation, I could not do the exercises. Most of the math was too advanced for me as I've tried to learn math on my own as a hobby. With lectures and a forum and the new ISL, I hope it will be enough for me keep up with this class, and I look forward to the challenge.",1,0,1
"The way I understand it is that the bias is how \rigid\"" your model is. So if you have a linear model, the relationship can only be linear. Even if the true function is quadratic, you have a bias for being linear since you can't match the points that are not aligned in a straight line. _x0007__x0007_If you go to a quadratic model, you increase your flexibility (and reduce your linear bias) and you can match more points. If you take this to the extreme and create a model so flexible that it will match every test point individually (like the KNN=1 in the examples) the variance of your model is increased as you will be matching more of the noise and less of the signal. _x0007__x0007_Does that make sense?""",0,1,1
"Hello Professors and TA's,_x0007_Thanks for all the hard work you've put into this class.  I'm probably a fairly typical attendee. I'm a middle age guy looking to grow some knowledge in an area that I am interested in but fairly unfamiliar with. The MOOC scene absolutely fascinates me and I find myself wanting to sign up for everything. I have lots of other things pulling at my time like my family and the need to put food on the table and wouldn't be able to risk my limited capital for a traditional bricks and mortars class with the rigidity of the benchmarking process. I just want to learn and I love the fact that this knowledge is available.  I'm grateful. Thank you.",1,0,1
"Thanks! I've successfully installed the package, how do I import the Auto dataset to the session? Sorry but I'm an R newbie.",1,1,1
"Yes, I am in Highlands Ranch",1,0,1
Does bias contribute to the irreducible error ? What factors contribute to the irreducible error ?,1,1,1
Hi am from Bangalore,1,0,1
"For some reason .R files do not open automatically in R on my computer, but if I open R and open script, they open fine.",1,0,1
"Every video has a mini-quiz. You can access quiz by clicking an icon on the right side of the video icon. There are two icons at each section - one for video , another one for quiz.",1,0,1
Type the following 2 lines of codes:_x0007__x0007_    library(ISLR)_x0007_    data(Auto)_x0007_After that (optionally) you can run the following code to see a snapshot of the data set (or data.frame as it is called in R)_x0007__x0007_    str(Auto),1,0,1
support vector machine,1,0,1
"It said, \Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative.\"" But how is the variance inherently nonnegative?""",1,1,1
Hi. <nameRedac_<anon_screen_name_redacted>> from Chennai.,1,0,1
"The data sets are available on the book site, along with PDFs of the charts [ http://www-bcf.usc.edu/~gareth/ISL/data.html ], but I don't see any code. It shouldn't be too difficult to recreate the figures. Have you used R before?",1,0,1
"I have the same problem, too.",1,0,1
Thank you.,1,0,1
"Hi,_x0007_I'm Muna <nameRedac_<anon_screen_name_redacted>> and I'm in Nigeria. I'm an Economics graduate who really needs to brush up on her statistics and analysis knowledge, it's been a while. Really determined to be completely involved in and finish this class, haven't done so well on that over on Coursera.",0,0,1
"It creates a directory called \Downloaded\"" in the current dir. Alternatively, you can pass -o=/path/to/dir argument as an output dir. Make sure you have write permissions there.""",1,0,0
I have done drawing in the same way as shown above and generalized for d dimensions but my solutions is not correct... any clue?,1,1,1
Dear staff_x0007_please insert download links in the bottoms of videos,1,1,1
Thanks for bringing all these stuff together in a user-friendly course! It seems that we're gonna have fun with statistics all over the World :),1,0,1
I think the problem is that if you have done some work in this area before then it is easy to try and read too much into these early questions.  If you look closely you will see they are simply from the first Conceptual question in Chapter 2 of the ISLR book._x0007__x0007_For me the objective of such questions is to get people thinking about how the nature of the model needs adjusting to the data set that you are using rather than a rigourous exercise in problem solving._x0007__x0007_In a sense your observation serves to exemplify the problem...by introducing extra parameters into the question you require a more flexible answer.,1,0,0
You need to intall in your computer nitro3 or adobe reader for pdf files. Just download either of these pdf readers and it will be all fine.,1,0,1
yes you could - so the Q is emphasizing that the answer is independent of n...,1,0,1
Hi! Does anyone know whether we receive something like a Certificate or Statement of Accomplishment after successfull completing the course?,1,1,1
"Thanks, I hope too",1,0,1
Sorry. The screen behind the lectures is not in focus. This was especially annoying for the R-introduction. I could not see what was on the screen. _x0007_Can you adjust the focus on the camera?,0,1,1
"?????? ????! ? ????? ?? ??????? (???????). ? ?????????? ??? ? ???????, ??? ??? ???? ???? ??? ????? ??????. ????????? ???????? ???????? ??????, ??????? ??????????? ??? ????? ??????:-) ??? ????????? ? ???????? http://vk.com/id1<phoneRedac>_x0007_???? ????-?? ????? ?????? ? ???????? ?????????? ??????, ??????????? ??????? ? ?.?. ?????? ? ????????????? (????? ?? ??????? ?????????) :-)",1,0,1
I completely agree. Getting the answer is easy; understanding the question is hell..._x0007_Thanks to all.,0,0,1
You first need to calculate the **area** of the white **square** and then the complement of that will be the area of the blue region (the required answer). Then do it for a cube (d=3) and take the complement. You get the point.,1,0,0
Si... sobretodo en español...,1,0,1
Please,1,0,1
"Second that.  Even on HD, it's too small to make out.",1,0,0
"Hello!_x0007__x0007_What is the volume of a line? I suppose it doesn't. A line has a length._x0007_Also, a square doesn't have a volume, but surface. How can I generalize this to n-dimension object?",1,1,1
"Dear StatLearners_x0007__x0007_We have created a study group in Madrid. If you are here and you want to join the group please visit the forum _x0007__x0007_http://r-es.org/forum6_x0007__x0007_We are going to schedule the firts meeting of the study group next Wednesday 29 at the end of the Madrid R local interest group sesion. You can check the program of this sesion [here][1]._x0007__x0007_We will publish the date of the first meeting in the mentioned forum and here. For further meetings of this study group we will create doodle surveys to vote the more conviniente date. Links to the doodles will be publised in the R_es.org forum for this course and here. _x0007__x0007_Best Regards Paco._x0007__x0007_Estimados colegas_x0007__x0007_Hemos creado un grupo de estudio para este curso en Madrid. Si estás interesado en dicho grupo visita este foro_x0007__x0007_http://r-es.org/forum6_x0007__x0007_El próximo miercoles 29, al final de la reunion del grupo de usuarios de R de Madrid, decidiremos la fecha de la primera reunion del grupo de estudio. Puedes consultar el programa de esta reunión [aquí][2]._x0007__x0007_Publicaremos la fecha de la primera reunion del grupo de estudio en este foro y en el foro de R_es.org. Para futuras reuniones se habilitarán encuestas de doodle para votar la fecha más conveniente. Los enlaces a dichos doodles se publicarán aquí y en el foro anteriormente mecionado. _x0007__x0007__x0007_Saludos Paco_x0007__x0007__x0007_  [1]: http://r-es.org/GILMadrid_x0007_  [2]: http://r-es.org/GILMadrid",1,0,1
i couldn't agree more,1,0,1
Thanks for this great course and ISL book! It'll be useful to flourish our statistical learning knowledge with applications. Also I'm hoping that you will offer an advanced course based on ESL soon.,1,0,1
I would appreciate it as well. So I could watch the videos during my train rides.,1,0,1
"Thank you.  Now I see that the 'next' button, which looks like a right-pointing triangle below the video, is enabled and takes you to the quiz as well.",1,0,1
Brilliant explanation!,1,0,1
"The third answer choice is: \Using the Quadratic Model will decrease the Variance of your model.\""_x0007__x0007_Seems like one's thinking would differ on whether one was talking about variance on predictions (test data) versus error variance on training data.""",1,0,1
Same difficulties. Coming from an economic background with a strong focus on sport management.,1,0,1
"I have no clue how to begin this question. I assumed that when p = 50, we would have captured the majority of the hypercube, but how would one begin to calculate something like this?",1,1,1
"Yes, got it... I was really wrong ... it is quite easy... I was just substracting another area...many thanks .just missunderstood.",1,0,1
I had a similar question. https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/<zipRedac>2dfd<zipRedac>21182f4b<zipRedac>ffc<zipRedac><zipRedac><zipRedac><zipRedac>7c,1,0,1
"I find the root url does not load - perhaps too many students are trying to access it at the same time._x0007__x0007_Instead I searched on _x0007_*\Data sets and figures\"" inurl:www-bcf.usc.edu/~gareth/*_x0007__x0007_That revealed several pages on the site, especially the one for data files nabilM mentioned. _x0007__x0007_Here's the [link for all the data files on this site][1]_x0007__x0007__x0007_  [1]: http://www-bcf.usc.edu/~gareth/ISL/data.html""",0,1,1
"Start with a 1 dimension line. If it is 1 unit long and you're capturing .05 of its length on both sides with .9 left in the middle, what is the equation to calculate the amount of the boundary you capture (as a function of what's left in the middle)._x0007__x0007_Now move to a 2 dimension square. Again if you're capturing .05 of its length and width in two dimensions, so that the part that isn't in the boundary area is itself a square inset in the 1x1 square by 0.05 on all sides, what is the equation to calculate the amount of the boundary area you capture?_x0007__x0007_Try the same thing with a 3 dimension cube. You'll end up with a cube inset into the 1x1x1 cube by 0.05 on all sides. You'll have two volumes. What is the equation for the difference between these two volumes, which is the volume of the boundary area?_x0007__x0007_The equation is the same for all three dimensions and can be generalized to 50 dimensions.",1,0,0
"Ah! When I first read this question, I thought \how am I ever going to figure this out\""... but as soon as I read your hint, it clicked. Thank you!""",1,0,1
Simple and beautifully explained. Thanks. Got it right on the first try because of you.,1,0,1
"But the true answer is not independent of N. It's highly, crucially dependent on N. And this is not a teeny detail, but a crucial and fundamental idea in model selection.",1,0,0
"Rather than think about the question geometrically, I thought about it probabilistically. I pick a random point in my hypercube. What is the chance that random point is outside the boundaries for at least one of its coordinates?",1,1,1
"The unit would be the length unit to the power. So let's say your hypercube was 1 cm on a side. A 3d cube is measured in cm^3, cubic centimeters. A 50d cube is measured in cm^50.",1,0,0
Any news on this yet. Would like to print them all out at work.,1,1,1
"Yup! I understood that if you wanted to capture 10% of the neighbors of a given data point in a 50 dimension space, you would need to include almost the entire space!",1,0,1
Worked for me. What a great tool!,1,0,1
"That's not particularly helpful; the OP wants the `auto` CSV file which comes with the **ISLR** package, not a Windows binary for R.",0,1,1
"What's bad is **test error**. You don't care if your predictions are wrong because of bias or variance. If the best model has high variance and low bias, you pick it. If the best model has low variance and high bias, you pick it. You want the model that gives you the best predictions.",0,0,0
Great Explanation!_x0007__x0007_Second Try! Thanks a lot !,1,0,1
"Labelling of some sort, yes. Supervised learning means showing input/output pairs to a learning algorithm and it finds a mapping between them. If the outputs didn't exist it would be unsupervised learning (the algorithm only sees the inputs and finds some sort of relationship/structure between them itself) or reinforcement learning (you don't give the correct output for each input, but guide the algorithm via rewards/punishments for right/wrong behaviour).",1,0,0
I believe that g(x) is often used to represent one of many theoretical *candidate* functions that we might look at the select a final f(x) that we use for our actual model.,1,0,1
"I'm excited about this course - thank you so much to the professors for offering it!_x0007__x0007_I hope there will be more R homework in future quizzes - so far, the quizzes have been on the self-evident side and the course, while fascinating, has not stimulated a lot of mental effort. Effort = learning, I hope you will make us work a little harder._x0007__x0007_I've been tinkering (and struggling) with R for a few years, and I'm hoping I'll feel a lot more proficient by the time I get through this course._x0007__x0007_Looking forward to the weeks ahead, and thanks again!",1,0,1
First try.,1,0,1
"I also had some problems with the volume._x0007_For some reason, when I checked the Windows 7 mixer, I found that the Flash Plugin had it volume near zero._x0007_Just tuning it up worked for me, and now everything is ok.",1,0,1
"I have the same or similar problem.  With R open and at the \R Sessions\"" page of the course in my browser, what do I have to do next.  I have found ch2.R in my downloads but opening it with R pulls up the R editor which just sits there.""",1,1,1
"Yes, we see that the focus is poor in the R sessions. Sorry about that. I think they get a bit better in later sessions. The R scripts are available (click on the R Sessions), so that you can have them open alongside as you watch. Even better, if you have a big enough screen, in a separate window run RStudio and execute the same commands from the script as you go along.",1,0,1
Thanks for clearing that one up mayer. (Its great to have so many smart people around answering these questions for us!),1,0,1
Very interested in learning the application part of these course. Especially that R will be in use. But reading the theoretical part of it is my problem. But i will try,1,0,1
"Hello,_x0007__x0007_How the grades are working here in this course ?_x0007_Ist there a certificate?_x0007_How many false responses are allowed at each Quiz?_x0007_How many Quizzes must be succeed all together at the end of the month??_x0007__x0007_I am looking every where since half an hour and I do not find any information ... ;-(_x0007__x0007_Thank you! ;-)_x0007_<nameRedac_<anon_screen_name_redacted>>",0,1,1
There are pretty good exercises at the end of each chapter in the book. Has anyone found answers for those exercises?_x0007__x0007_Please share if anyone finds answers to those exercises.,1,1,1
"There are exercises at the end of each chapter in the book. It has quite a few R exercises as well. _x0007__x0007_I have been trying to find the answers/solutions to those exercises, but haven't had any luck yet.",1,0,1
"I was wondering if it was a write permission issue as well, but the permissions seem to allow anyone to write to the directory (C:UserNameDesktop). The output I get after selecting the course and the videos to download is:_x0007__x0007_Processing 'some URL string'..._x0007__x0007_(repeat output for each page/vid)_x0007__x0007_[info] Output directory: C:/Users/Name/Desktop_x0007__x0007_[youtube] Setting language_x0007__x0007_[youtube] videoURLstring: Downloading webpage_x0007__x0007_(repeated output)_x0007__x0007_>_x0007__x0007_No error is thrown, it just returns to the prompt. No 'Downloaded' directory either to the Desktop (when I specify the -o option) or to the current working directory (when I omit the -o option)._x0007__x0007_:/",1,1,1
On Windows 8.1 (Surface Pro) if that gives anyone any ideas...,1,0,1
"Hi Bhabani,_x0007_I guess that genes that interact with each other is quite different than simply a cluster of genes. It is not so easy to identify the interaction between genes._x0007_Cheers_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"hi Chitraka,_x0007__x0007_I think data mining is more looking for a pattern within data and learning is to look for a function in a dataset to reach some speficic goal such as prediction_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Yes, I agree, the first necessarily leads to the second, nice point. I guess in my training the emphasis was squarely on the first meaning, so when the quiz asked if the addition of parameters to a model could do anything about bias, I thought, \No, of course not!\"" Wrong.""",1,0,1
"There's a lot of overlap between Statistics, Machine Learning, Data Mining, and even Artificial Intelligence. To some degree, researchers from each of these fields discover some of the same methods and give them different names. We use different vocabulary, but at times talk about the same things. Take a look at Brendan O'Connor's blog, [Statistics vs. Machine Learning, Fight!][1], for a few more equivalent glossary terms from the two fields._x0007__x0007__x0007_  [1]: http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/",1,0,1
"This makes sense and follows the logic of the lecture, but I was thrown by the fact that the problem says \there exists a j\"" (implying at least one) rather than \""for all j\"".  It seems to me that your explanation models the case of \""for all j\"".  Can anyone help point out where I might be confused?""",1,1,1
"Feel free to use any software you want, with the caveat that the course is designed to teach you R and that is the only language we will support.",1,0,1
"Just a minor nit-pick:  Prof Hastie would not refer to a model as being \qualitative,\"" only variables.  The dichotomy being drawn is that the variable can represent a number (quantitative, as in height), or a grouping (qualitative, such as color or binned height).""",1,0,1
"Throughout the course, if we discuss the variance of a model we mean the variance of the fits.  This is consistent with the variance mentioned in the bias-variance tradeoff.",1,0,1
"Think about what it would mean if the point was in the boundary \for all j.\""  This is not the boundary being described by the problem, but would rather correspond to something like the corners of the hypercube.""",1,0,1
I see now.  Thanks.,1,0,1
"I believe it has to do with the predictive nature of the circumstances. #2 is searching for something that already exists, while #1 serves to take given knowledge and produce a prediction or trend. I got hung up a bit on this question as well, especially as the slides don't give very clear definitions for the terms.",0,1,1
"Without going into technical procedures for estimating test error, cross validation and what not. Are there any heuristics for preferring one approach over the other (flexible vs less flexible) just by looking at your data, like number of observations, parameters, type of distribution, etc...??_x0007__x0007_Thanks _x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Thanks <redacted> and <redacted><redacted>. _x0007__x0007_I am also find it difficult to differentiate between these two examples. May be in case of #1, we can quantify the predictor and response and predict the trend based on already available sample data. _x0007__x0007_In case of #2, we can not predict the interaction among genes cluster as only supervised reference is cluster._x0007__x0007_Thanks,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"A big warm greeting. I'm <nameRedac_<anon_screen_name_redacted>>, 32 yo programmer from Rome, Italy. I have completed several online courses offered by the Stanford University on Coursera and I love this course too. Both content and presentation are very compelling. I hope I will improve anything I've already learned in the field of applied statistics so far, and also to make it through these weeks the best way I can do. Good luck to everyone!",1,0,1
"The videos are on youtube. WIthout revealing too much, it shouldn;t be difficult to get the video url and download youtube videos.",1,1,1
"Is it possible to get answer sheets for the book exercises each week, for those of us who want to practice more(especially in R)? _x0007__x0007_Thanks_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Ch 2.4 time 12:10 professor explains how we get the decision boundary in case of nearest neighbor classifier i.e k=1. I got that we take perpendicular bisectors of line joining the opposite classes._x0007__x0007_But what when k increases i didn't get the exact procedure to construct decision boundary. I know we compare with k other neighbors but how exactly is the decision boundary done.,1,1,1
"Several posts have complained about a particular quiz question being defective --- usually with some discussion of faults/errors/oversights.  I would find it useful if the complainer also proposed alternative phrasing, perhaps including an improved answer._x0007__x0007_For example, Q1.2.R1's third item could become_x0007__x0007_    Classify a handwritten character as a digit [i.e., one of 0 thru 9]_x0007__x0007_(\from labeled examples\"" specifies a training set, but does not seem necessary)._x0007__x0007_E.g., the answer for Q1.2.R2 could begin by citing a model with weak correlation_x0007__x0007_    walking an extra mile each day will lead to weight change_x0007__x0007_and comment that \""very accurately\"" is too severe.  Since part of the existing answer refers to linear regression, there is an opportunity to comment earlier that any one-variable regression to predict weight loss will be less accurate than several related models using additional variables (each with an interpretable contribution).  Alternatively, if optimistic that participants actually read the text (provide yet another link to the free PDF), then that answer could begin by referring to discussion of the Wage data on pages 1--2.""",1,1,1
"Hi John,_x0007__x0007_I'm not aware of an official solutions manual. We're working on a community-based solutions manual here:_x0007__x0007_https://github.com/<nameRedac_<anon_screen_name_redacted>>/stat-learning_x0007__x0007_Cheers,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"I'm not aware of an official solutions manual. We're working on a community-based solutions manual here: https://github.com/<nameRedac_<anon_screen_name_redacted>>/stat-learning_x0007__x0007_Cheers,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"I'm not aware of an official solutions manual. We're working on a community-based solutions manual here: https://github.com/<nameRedac_<anon_screen_name_redacted>>/stat-learning_x0007__x0007_Cheers,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
I was unable to find the plot on page 30 of the NOTES that was referred to in the Quiz to determine what would happen if I moved the point to K.  Any insight on where the notes are would be helpful for future quizzes if they refer to the notes for anything._x0007_Thank you.,1,1,1
I *love* that blog post! Thanks so much for the link. My brother and I have been having this debate for years.,1,0,1
"In the first slide of lecture 2.1 there are 3 plots:_x0007_![enter image description here][1]_x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>29.png_x0007__x0007_I wonder if the data used to plot these was like (ignore the values)_x0007__x0007_    Tv | Radio | Newspaper | Sales_x0007_    5  | 7     | 3.4       | 20_x0007__x0007_An they just plotted Tv vs Sales, Radio vs Sales, and Newspaper vs Sales, or the data was like_x0007__x0007_    Tv | Sales1_x0007_    5  | 10_x0007__x0007_    Radio | Sales2_x0007_    7     | 70_x0007__x0007_And Sales1 and Sales2 were totally unrelated",1,1,1
Is Y = B(0) + Sigma (B(j)X(j)) + error linear only for first powers of X(j) or is the linearity in the betas?,1,1,1
"I understood that the method here is NOT k nearest neighbor. _x0007__x0007_In that graph, there are not a lot of data points. There are no points that fall exactly at x=4.  Therefore, they have averaged all points that fall within the band marked by the dashed lines.  They may have chosen the area between 3.9 and 4.1._x0007__x0007_The green dot is the average of all of the points that fall between the dashed lines._x0007__x0007_You can then slide the area slightly to either side, and average all data points between 3.91 and 4.11 and place a green dot at that location. The result would be a line of green dots. However, I agree that the line may not be as smooth as shown in the graph._x0007__x0007_The method shown in this video is used to estimate a regression model. It is my understanding that in the k nearest neighbor is used for classification problems.",1,0,0
"The way I read it, Sales is the response variable and there are three input variables; TV, Radio, and Newspaper. These three plots simply plot Sales on each of the three inputs individually and fit the line Response = Beta0 + Beta1*Input + error. The beta are estimated using the method of least squares. So, I would say the data are like in your first example.",1,0,0
May I ask the outcome of the discussion ?,1,0,1
"Good idea, AlMZ.  I did this and got answers extremely close to the expected.",1,0,1
"Don't worry pnj ,_x0007__x0007_ there is no dearth of material for practicing R. Take for example, you can try to simulate some general data set and verify how flexible methods performance vary as compare to no of data sets. Try to plot the results and reproduce similar plots as given in the book. By the time you reproduce all the results in chapter 02, you would be more confident in doing similar type of simulated rigorous analysis of further chapters.  And follow the link below for more programming challenge_x0007_ http://work.caltech.edu/homeworks.html",1,0,0
I want to add that the textbook is very well written. Will order a hard copy soon. Thank you for making it available to the class!,1,0,1
"You can find the data at   _x0007_http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv_x0007__x0007_The data has the following form:_x0007__x0007_    |   |    TV | Radio | Newspaper | Sales |_x0007_    |---+-------+-------+-----------+-------|_x0007_    | 1 | 230.1 |  37.8 |      69.2 |  22.1 |_x0007_    | 2 |  44.5 |  39.3 |      45.1 |  10.4 |_x0007_    | 3 |  17.2 |  45.9 |      69.3 |   9.3 |_x0007__x0007_There's no separate Sales1 and Sales2.  The same sales is plotted_x0007_separately against each of the 3 other variables._x0007__x0007_Look carefully at the plots and you will see the highest value of_x0007_sales, 27, as the highest point in all three plots._x0007_(Towards the right in the TV and Radio plots, slightly left of centre_x0007_ in the newspaper plot)_x0007__x0007_Those three dots are really 3 different views of the following _x0007_single point in a 4-dimensional space._x0007__x0007_    |     |    TV | Radio | Newspaper | Sales |_x0007_    |-----+-------+-------+-----------+-------|_x0007_    | 176 | 276.9 |  48.9 |      41.8 |    27 |_x0007__x0007_The 4-dimensional space cannot really be visualised, but imagine there_x0007_was no Radio.  Then you could imagine the TV and Newspaper plots as_x0007_being \front\"" and \""side\"" views of a 3-dimensional cube containing all_x0007_the points (one for each row)._x0007__x0007_You can check the data out yourself with just two lines of R (be careful of the_x0007_line-wrap in the first line):_x0007__x0007_    Advertising <- read.csv(\""http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\"")_x0007__x0007_    pairs( ~ Sales + TV + Radio + Newspaper, Advertising)_x0007__x0007_- See textbook, page 49 for explanation of \""read.csv()\""_x0007_- See textbook, page 50 for explanation of \""pairs()\""_x0007__x0007_![pairs() plot of the Advertising data][1]_x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>00<phoneRedac>.png""",1,0,0
Nice plot!_x0007__x0007_Increasing n to 50 or 60 makes a much smoother plot._x0007__x0007_I believe that the graph in the video has 60 or more data points.,1,0,1
"Que te parece \asimilacion estadistica\"". es algo obscuro pero creo que funciona""",1,0,1
Saludos desde Tejas. Me uno al grupo,1,0,1
"It actually only appears to be smoother because of the large size of the points in the default R graphics. By shrinking the point size and increasing the number of points plotted you can see that when you zoom in, the function will look the same as the image above, regardless of the value of n.",1,0,0
Does anyone know if answers to the exercises at the end of each chapter are available?,1,1,1
The string I wrote in the above quote is_x0007__x0007_```_x0007_   # The hashtag for titles_x0007_```_x0007__x0007_which produces_x0007__x0007_# Partial markdown support?,1,1,1
There have been multiple queries about this. Short answer No. Someone has started to compile answers.,0,0,1
cool,1,0,1
The data set looks like this:_x0007__x0007_    head(Advertising)_x0007_      X    TV Radio Newspaper Sales_x0007_    1 1 230.1  37.8      69.2  22.1_x0007_    2 2  44.5  39.3      45.1  10.4_x0007_    3 3  17.2  45.9      69.3   9.3_x0007_    4 4 151.5  41.3      58.5  18.5_x0007_    5 5 180.8  10.8      58.4  12.9_x0007_    6 6   8.7  48.9      75.0   7.2_x0007__x0007_It is available at the book's site._x0007_[http://www-bcf.usc.edu/~gareth/ISL/data.html][1]_x0007__x0007__x0007_  [1]: http://www-bcf.usc.edu/~gareth/ISL/data.html,1,0,0
Am curious and would like to know the importance of statistical learning and how I can apply it to my work.,1,0,1
"I try to figure this out_x0007_I do_x0007_<getwd()_x0007_and found I want to change working directory _x0007_I do _x0007_setwd(\D:<nameRedac_<anon_screen_name_redacted>>MolnetBox SyncStatistical learning\"")_x0007_ and got this error _x0007_> setwd(\""D:<nameRedac_<anon_screen_name_redacted>>MolnetBox SyncStatistical learning\"")_x0007_Error: 'R' is an unrecognized escape in character string starting \""\""D:R\""_x0007__x0007_By trial and error I found out the R do not use backslash This one was excepted_x0007__x0007_setwd(\""D:/<nameRedac_<anon_screen_name_redacted>>/Molnet/Box Sync/Statistical learning\"")_x0007_and now it works!!""",1,0,1
Same for me. I'm using Mozilla Firefox on Win 7.,1,0,1
I have seen this question asked a few times but I didn't see an answer. Will textbook answers would be posted? Also I am looking at Chapter 2 Question 8 in the textbook it references a College.csv data set is that data available? I did not see it on the course pages.,1,1,1
Excellent explanation! Thanks a lot!,1,0,1
I have probably done 6 hours on ch 2 but I have just browsed through the book and the R-examples and have not finished the R-practices. It took me >1 hour just to get the auto-dataset into R. And R is not userfriendly.,0,0,1
Thanks! It really heled?,1,0,1
except that probability is tougher than simple geometry.,1,0,1
Thank you for the clarification.,1,0,1
"Hello,_x0007__x0007_Is it mandatory to score 100% in each chapter.If so what should we do if we are not able to do.",1,1,1
are you from asia,1,0,1
Thanks I'm so happy to be part of this course.,1,0,1
"Hi Bhabani_x0007__x0007_I guess your confusion lies in the nature of clustering experiments :)_x0007__x0007_The goal of gene clusters might be the following: _x0007__x0007_Let's say we analyse the expression of 1000 genes in cancer cells after drug treatment at four time points: 0, 1h, 5h, 1d._x0007_What we now get out of a cluster analysis can be results like this:_x0007__x0007_ - genes 10, 102, 569, ... are not expressed at the beginning, then increase_x0007_ - genes 1, 17, 32, ... are expressed at the beginning, but then decrease upon drug treatment_x0007_ - genes 2, 20, 40, ... reach a peak after 1 hour and then drop_x0007_ - and so on_x0007__x0007_There is actually no way to predict that before, but those clusters may give very interesting insights, the experiment is actually aiming for (for example about how the drug interacts with different intracellular processes). The only way to have a training set, would be that someone has already performed the experiment before - and then it is not interesting any more to do it (publish of parish :) )",1,0,1
"I'm taking this course too now.  He moves fast and assumes a lot so its good stuff, but not for the uninitiated.  Actually, taking both these courses together is a great bit of luck.  This course is far better in that there is a textbook, many more video's and more statistical concepts which is missing from Peng's course.  Peng assumes from the pace you've been using R before.",1,0,1
"Hello, I'm Al, a retired civil engineer from Canada. I completed two data analysis courses on Coursera by Jeff Leek and Roger Peng and am taking this course to to give me a different perspective on data analysis and to keep my brain active. Soon to turn 74, I undoubtedly am one of the oldest people doing this course. So, if I ask stupid questions, keep that in mind.",1,0,1
@asadoughi Would you like to include homework solutions and notes from this class in your repository as well? It makes sense to keep everything in the same place.,1,0,1
"Hi Roy, Sculpturearts, and BrashEQLibrium,_x0007__x0007_I'm also in Seattle. A weekday lunchtime hour meet would work for me. Vivace's Alley 24 in particular would be fine - most excellent coffee!_x0007__x0007_Jim",1,0,1
"Otherwise, people who would otherwise have to use the computers in a public library or another such place may be at a disadvantage, because of time limits on use for instance.",1,0,1
@asadoughi Just sent you a pull request.,1,0,1
"Unfortunately YOUTUBE is filtered in our country and I can't watch the video lectures :(_x0007_Would you please put a download button (with a direct link) for each video lecture?_x0007_If the answer in NO, please tell me how I can watch them?",0,1,1
"I think the key to understand this is to think about the expected test MSE E(MSEtest). _x0007__x0007_But, the variance of the error term (if I´m correct) corresponds to the unreducible error, right? So, if that variance it´s very high I´m not sure to understand how that´s a relevant information in order to decide between flexible and rigid models. _x0007__x0007_Thanks!",1,1,1
I'm using zenmate extension in google chrome browser._x0007_Or you can use hotspot shield._x0007_Both of them work in countries where Youtube is banned.,1,0,1
are we required to do the exercises or is it optional,1,1,1
Podría ser...,1,0,1
Section 2.1.2 of the book seems to suggest that the level of fit on a flexible model is determined by the degree of smoothness.  No mention is made about the effect of sample size on fit (other than a large amount of data is needed).  The 'shown answer' seems like something that cannot be determined from Chapter 2.,0,1,1
"hi,i am new to this r programming world.i want to know what is the syntax for getting two particular rows or columns may not be consecutive from a matrix _x0007_ z=matrix(seq(1,12),4,12)//creates a matrix_x0007_ z[1,2:4]                //gives row 1 and columns from 2 to 4_x0007_ but i am unable to find the syntax for getting arbitrary rows or columns like if i want row 1 and row 3 and columns 2 and 7 elements_x0007__x0007_btw we can do this for vectors by simply removing remaining columns like  x[-c(1,2)] _x0007__x0007_with regards_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
yes that's it,1,0,1
"Hello._x0007__x0007_I find it hard to see the slides as the transcription is partially covering them. If I resize the window, it moves somewhat, but not enough. I have a pretty large screen, so there should be room to have them comfortably side-by-side._x0007__x0007_Is this possible?_x0007__x0007_Thanks!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Thanks for this!,1,0,1
"Some texts, I think, uses the term \linear in the parameters\"" in that case.""",1,0,1
"In the embedded viewer I get (using Firefox), there are controls in the lower right. From left to right, they are playing speed (drop-down list), volume (slider), and toggles for full screen, high def and captioning (labeled \CC\""). Captioning is on by default, but you can click on \""CC\"" to turn it off (which I routinely do, for the reason you articulated).""",1,0,1
"Thanks, nice conceptualisation for those of us a bit rusty on maths (I then needed to remember to report the area in the boundary area, rather than the centre!)",1,0,1
"Trying $LaTeX$. Well I'll be darned. How about an equation:_x0007__x0007_$$ _x0007_sum_{i=0}^n x^i_x0007_$$_x0007__x0007_```_x0007_  # Big multiline code block_x0007_  x <- runif(10)_x0007_  plot(1:10, x)_x0007_```",1,0,1
Thanks @wmmurrah. I'm still having some unusual output though.,1,0,1
I do not see any link to download the course videos to listen off line.,0,1,1
"Yep, India in particular",1,0,1
What do you mean? We have access to the pdf of the course text book. If you want the Elements book just Google for it. The pdf is available on the author website.,1,0,1
"Use the k-nearest neighbor as an example, the model will be very flexible if you choose a very small N. However, when the dimension is high (more predictors), the neighbors will include non-local neighbors, moreover, given the sample size is small, the result will be highly impacted by the training data. thus more flexible is worse._x0007__x0007_my 0.02.",1,0,1
"Here is my understanding:_x0007_If the irreducible error is higher, the probability to use the training data to proxy the real data is lower. More flexible model means we are trying to use the model (high variant one) to simulate the result from training data which is not reliable, so the more flexible, the worse.",1,0,1
Thanks for your great explanation.,1,0,1
thank you,1,0,1
thank you,1,0,1
"Greeting;_x0007__x0007_I work in Telecom industry... NSA want all the phone and data && metadata... My job is performance Analyst, deal with mountain of data...I tracking from UE (your phone to cell site to core network).. _x0007__x0007_Statistic, model simulation, chart/graph.. is my daily task..._x0007__x0007_R is good with this kind of work._x0007__x0007_Cheers!",1,0,1
I have found a workaround. _x0007__x0007_1. On the video simply right-click and copy the video URL._x0007_2. Then go to [KeepVid](http://keepvid.com) and paste the URL. _x0007_3. Download the file format of choice. It will show you a number of different formats and you can select which one your prefer. _x0007__x0007_(_Note this requires Java to work._),1,0,1
"Indeed, you can switch them off or on. If you do not expand the video player window, then clicking on CC button actually either turns the cc off or on and then places cc on the right side of the video frame. However, when you are in a \fill browser\"" (or as PAR73 called it \""full screen\"" mode), I do not see any way to place the cc on a side. Moreover, in this \""fill browser\"" mode I have a bottom of the screen being cut off by the control bar. This is annoying. So I watch videos not in a \""fill browser\"" mode. I am using Chrome on Linux.""",0,1,1
"Hi, Is [REDACTED BY COURSE STAFF] the answer?_x0007__x0007_Please help with solution if my anyone has cracked the above Math puzzle.",1,1,1
"In the script above you have both objects x.  Try:_x0007_x<-runif(50)_x0007_y<-rnorm(50)_x0007_plot(x,y)",1,0,1
"Hi everyone. <nameRedac_<anon_screen_name_redacted>> in NYC here. I'm an editor at ProPublica, and we do investigative journalism. Many of our projects involve data analysis. You can see more at www.propublica.org. Any other journalists out there?",1,0,1
"I took that course and learned a lot. It can be tough if you don't have experience with programming, but if you hang in there you'll get it. Forums are really helpful.",1,0,1
for a plot the no of points in your variables should be same_x0007_if x has 10 values then y shold also have 10 values,1,0,1
"??????? ?????, ????????? ?????? ?? ??, ??? ?????????? ?????, ??????????? ? ??????, ???????????????! https://www.facebook.com/groups/<phoneRedac><zipRedac>1<zipRedac>62/",1,0,1
Thank you! I thought I would never get this,1,0,1
"There are also many apps and ad-ons that allow you to download videos form youtube, where these are hosted._x0007_For example, I'm using the Download Helper add-on for Firefox to download the videos and watch them more easily, as my internet connection can be jumpy at times.",1,0,1
"Thanks for reply, the link provided is the url i was referring to. I tried again today, and although the PDFs download and are viewable in browser, none of them will open once downloaded.",0,1,1
The Progress page indicates that 50% is a Pass._x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/progress,1,0,1
"Hello there, I copy here the text of the question. _x0007__x0007_> Which of the following are supervised learning problems? More than one_x0007_> box can be checked. _x0007__x0007_ >-  Predict whether a website user will click on an ad_x0007__x0007_> - Find clusters of genes that interact with each other_x0007_> - Classify  a handwritten digit as 0-9 from labeled examples_x0007_>- Find stocks that  are likely to rise_x0007__x0007__x0007_As far as I have understood, only in the third answer the Y (response variable) is labelled, also known. Unlikely, in the first answer, it is clear the response variable would be clicking or not in the ad; in the fourth answer, rising or not would be the dependent variable Y, but in the way it is written, it is not clear to me that those response variables are given in the problem. _x0007_Thanks in advance for your answers",1,1,1
"With x^2 in the model, the shape is quadratic. I think of lm() as an algorithm using the method of least squares to find the smallest error and best estimate of the coefficients. This algorithm can also be used to find the smallest error and best estimates of the coefficients when the shape of the curve is other than linear.",1,0,0
"Its a great course indeed. Please suggest something relating to the video problem mentioned in above comments, as we are unable to open any of your videos here in Pakistan.",1,1,1
Perhaps (I´m not sure) the problem is that you only know the limits of the confidence interval (0.4 & 0.5) but you don't know the probability of that interval.,1,0,1
"Ah, the frequentist vs. bayesian interpretations of confidence intervals!_x0007_Short answer: neither interpretations suggest a 95% probability of truth for the parameter._x0007_Long answer: read on._x0007__x0007_For the frequentist, there is only one true value for the parameter, which is hidden and we can only approximate based on theories like the Central Limit Theorem. _x0007_Note that with each random sample set, we expect to get different confidence intervals. Hence, the 95% confidence interval for a parameter indicates that if we repeated the fitting using multiple (& infinite) number of random sample sets, 95% of confidence intervals produced would contain the true parameter. _x0007__x0007_For the bayesian, the parameter (or its estimate) is random and has an associated probability distribution. The 95% **credible** interval communicates the level of belief the experimenter has of the parameter i.e. she is 95% sure of the parameter estimate. Note that it is a subjective belief as the interval is of the posterior distribution based on the experimenter's chosen prior. _x0007__x0007_So, if you're not confused by now, you haven't understood it. In practise, most decisions are made as you suggested: 95% probability the true parameter lies within the interval. Nothing bad has happened yet, well maybe the GFC.",1,0,0
"Many Thanks for Professors and TA, which spend they valuable time on this course._x0007_Looking forward for this exciting course.",1,0,1
"Thanks to the above, I understand the math, but I am not clear how this helps us with conducting supervised learning. I see that from a probabilistic perspective, this exercise tells us that an in an experiment with a relatively low, 10%, probability of success, you have a very high probability of observing a success at least once if the experiment is repeated 50 times.  How does this relate to the material presented?",1,1,1
"I remember on the slide 8 for chapter 2, when the dimension becomes higher, the radius of the volume will be very big. When p=10, the radius is more than 1. So in this question, when the dimension is 50, why is not it be 1.000? I do not really understand this question.",1,1,1
Same in the USA on ipad and android.  This video is currently unavailable error.,0,0,1
Ok. I think I got it. Thanks!,1,0,1
"Ok, you dont need the degrees of freedom. Think about the 95% confidence interval around the estimated parameter 0.5. How large can b be without falling out of those confidence intervals?",1,1,1
"But the listed, correct solution to \The variance of the error terms, i.e. ?2=Var(?), is extremely high,\"" is the flexible model is worse than the inflexible model._x0007__x0007_And, an *over fit* by a highly flexible model is not good._x0007__x0007_If we know we have an extremely high, irreducible error, why would we want to go with a model that will be difficult to interpret? I'd do with the less flexible model that I could understand and interpret._x0007__x0007_This question is not answered yet. Further help and comments appreciated.""",1,1,1
The t-statistic is can be calculated. Look at the tv advertising example. What I don't get at the moment is the next step after that.,1,1,1
i.e. we cannot look up the values in a t table without the degrees of freedom.,1,1,1
"<redacted>. To clarify, the *irreducible error*  is noise, it's random. The *reducible error* is how good our fhat is as a fit for f. _x0007__x0007_Feel free to write back if I'm mistaken. :-)",1,0,0
Thanks for that example.  It's appreciated._x0007__x0007_There something about it that still seems unclear ... the impact on flexibility given the size of K seems to be different than the impact of total sample size.  This is what's unclear from the text or lecture ... we did not cover the effect on MSE of total sample size given various levels of flexibility.,1,1,1
"I find the easiest way to get to the data is with:_x0007__x0007_data=read.csv(file.choose(), header=TRUE)_x0007__x0007_This allows you to navigate to where you have stored the file.",1,0,0
I have found that it is helpful to use two windows: one window to play the audio/video and another window to display the PDF slide deck. I zoom in and out and page up and down in the PDF slide deck while the audio/video is playing.,1,0,0
"I thought that as well, but the 3rd choice says \I sample data and construct a confidence interval\"" without specifying the probability of the confidence interval that was constructed.""",1,0,0
"Hi all,_x0007__x0007_I am developing a start-up which relies on statistical modelling. I would be interested to know what motivated others to join this course?_x0007__x0007_What kind of projects is everyone working on? I am quite curious..._x0007__x0007_M",1,0,1
"Hi, A total newbie in R, here.  How do I start?  I did download and managed to open R window (am using MacBook).  But clueless as to what to do next.  Is there a chapter/ section I can read before I open R Sessions ?",1,1,1
For these two questions there is a indication for (1 point possible) but there is more than one point correct.,1,0,1
"Yes... I think it´s the same. Of course, I can be wrong...",1,0,1
Questions with the square boxes indicate that there is *possibly* more than one correct answer. To get the point you need to select *all* the correct answers.,1,0,1
Are the quizes the only 'work' we turn in to get a certificate? Are the book exercises something we have the option of doing?,1,1,1
"Hi, I asked on one other thread, but are the quizes the only required 'work' to be turned in for a grade? Are the book exercises optional?",1,1,1
"@smgross -- only a simple modification is needed... add the word 'handout' to the first line of the latex files, recompile and post them to the website with separate links that say, \handout format\""_x0007__x0007_https://wiki.bath.ac.uk/display/latextricks/Making+han",1,0,1
"On Windows you need to include the file extension to open the file._x0007__x0007_No file extension:_x0007__x0007_    > read.csv(\dtemp\"")_x0007_    Error in file(file, \""rt\"") : cannot open the connection_x0007_    In addition: Warning message:_x0007_    In file(file, \""rt\"") : cannot open file 'dtemp': No such file or directory_x0007__x0007_With file extension:_x0007__x0007_    > read.csv(\""dtemp.txt\"")_x0007_      var1 var2_x0007_    1    1    2_x0007_    2    3    4_x0007_    3    5    6""",1,0,1
The volume of a cube is (side length)^dimension.  In this question we're asked to find the volume of a region that is left over when you subtract one cube from another.  So find the volume of the two cubes and subtract.,1,0,1
Maybe this will help._x0007__x0007_[http://tryr.codeschool.com/][1]_x0007__x0007__x0007_  [1]: http://tryr.codeschool.com/,1,0,0
"Yes, I am in Parker. I would also be interested in a study group._x0007__x0007_JD",1,0,1
?,1,0,1
"I likened the web-click scenario to the Netflix example. The web site operators ought to have some data collected from existing users, which can form the same sort of sparse matrix described in the Netflix challenge. The goal must be to predict what sort of behavior is exhibited by users most likely to click some button.",1,0,1
Travel well intituitivo.Thank you!,1,0,1
Thanks for posting this link!,1,0,1
"Thanks, smgross. I guess the MOOC isn't quite there yet on homework. It's good to know that I shouldn't wait for more R exercises to come up in the course.",1,0,1
"Thanks - I only have time for one course at a time, but I'll look at Peng's class when I finish with this one.",1,0,1
"getwd() -- shows current working directory, may be better to put auto.csv in that folder, then read.csv(\auto.csv)     _x0007_C:/Users/saiakhil suggu/Documents/desktop/auto.csv  -- maybe check this path, I don't think there would be /Documents/desktop/""",1,0,1
"Here's a quote from the video on 3.5: \We still call it a linear model, because it's actually linear in the coefficients. But as a function of the variables, it's become nonlinear.\""_x0007__x0007_I'm confused by this statement. I'm used to my high school algebra class were we classified a function/model by its highest degree monomial. I'm not following how the coefficients are linear vs. the variables are non-linear._x0007__x0007_I'm also confused about the example given in video 3.5 using horsepower and horsepower^2. How exactly would be interpret horsepower squared? Would we say, \""the correlation between mpg and horsepower squared is ___\"" OR, \""Given the slope __, for every one unit increase in horsepower^2, we estimate an increase/decrease of mpg\""?_x0007_Thanks,_x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"Perfect, thank you.",1,0,1
"we already know the standard error from the question,then we do not need to calculate anything",1,0,1
"This sounds great, but I am unclear on whether the publisher could or would have a complaint.  I have read about unauthorized solutions manuals for textbooks provoking suits from the textbooks publishers.  Apparently, it has something to do with \derivative works.\""  Does anybody know whether that's a problem in this case?""",1,1,1
"There are also the chapter-end exercises in the textbook, though obviously there's no grading on these.",1,0,1
"\the largest value of b  for which we would NOT reject the **null hypothesis** that hat(beta) = b   ?\""_x0007__x0007_This is confusing. The Null Hypothesis has been defined as one for which hat(beta) = 0. The question says hat(beta) is an estimate of the *slope*._x0007__x0007_Will some one clarify please?""",1,1,1
"Stanford doesn't seem to have a \How to use the platform\"" section yet. You could look at the edX demo. As Stanford is built on the edX platform they should be the same. You need to register for an edX account as it is not the same as here._x0007__x0007_https://www.edx.org/course/edx/edx-edxdemo101-edx-demo-1038""",1,0,1
"There are two functions that are not to be confused. One is the ultimate function you are trying to arrive to that is used to predict Y as a function of X. The other function is MSE as a function of the coefficients of X which you are trying to minimize. The first function can be non-linear in X so that it can take complex shapes, but the model is linear in the parameters (coefficients) that you are trying to fit, this way you can simply walk down the gradient towards the point that minimizes MSE. You can then take the best values of coefficients and plug them into f-hat._x0007__x0007_So, to reiterate, for the fitting step your coefficients are the variables, once you found the best value of the coefficients they become constants in the main function which is a function of X.",1,0,0
"In another course I took, it was recommended to try to tailor the flexibility of your approach to the amount of training data you have. The rule of thumb was to try to pick your model so that the number of training data points are in the region of around ten times the VC dimension of the model (the VC dimension corresponds roughly to the number of free parameters in the model). Having this number of points or more is usually (but not always) OK, but if you have less you might get in trouble._x0007__x0007_However, this is just a rule of thumb. It is useful to rule out models that are too flexible and are unlikely to work with the data resources you have. However, you should always pick your model through test set performance, cross-validation or similar techniques.",1,0,0
"If you're a frequentist, the parameter is either in the interval or not, so a probability does not make sense. The frequentist interpretation of a 95% interval is 'if we were to repeat this procedure repeatedly on different samples of the data to construct an interval, we would expect the true parameter to lie inside the interval 95% of the time'.",1,0,0
"I wish there was a solution manual to the exercises. I'd like to try some of them, but it would help if I could check my answers.    _x0007_EDIT -- just found it, there is one! Under the Wiki link, there is a link for students to contribute their answers to the text exercises.",1,0,1
"I think R is more comprehensive than Octave. Octave has matrices of numbers, but R can do a lot with different data types in its data frames. They can be strings for categories, or use date-times for indexing. R can be like working with tables from a database, where you can select entries based on criteria. R data frames can have column names, and has string features like grep. Does Octave have features like these? I've worked with Octave only a little.",1,0,1
other example of functions non-linear coefficients_x0007__x0007_$$ mathrm{f_3}(x) = _x0008_eta_0 * x^{_x0008_eta_1} $$,1,0,1
I am not able to access 3.5 video.,1,1,1
"Applied assignments would be very helpful, especially if answers were available or given later; otherwise I could just read a book.  The book questions, especially the conceptual ones, are not very satisfying if I don't know what I'm not understanding and why.  I did find the wiki with the chapter exercises, but it is completely empty.  For what it's worth, I'd rather read the book and have very applied lectures describing how to implement and interpret what's in the book.",0,1,1
"I've used both tools, and agree with <redacted> and <redacted> above. MatLab is an [abbreviation](http://www.mathworks.com/matlabcentral/newsreader/view_thread/<zipRedac>8313) for matrix laboratory- it's built to do fast and efficient numerical computation, especially with matrices. _x0007__x0007_R and Matlab are both niche tools. _x0007__x0007_- Statisticians use R for general purpose data analysis, say importing, cleaning, and fitting statistical learning models on heterogeneous data._x0007_- Mathematicians tend to use MatLab for heavy computations, like solving differential equations or modeling physical systems._x0007__x0007_Python, on the other hand, is a general purpose programming language with significant overlap with R through packages like `scikit-learn` and `pandas`. Also, the `numpy` package has features similar to Octave / MatLab.",1,0,1
"To supplement this explanation, what's random are the bounds of the confidence interval. Once the bounds are no longer random (e.g. fixed at 0.4 and 0.5), then it no longer makes sense (from a frequentist standpoint) to talk about the probability that the parameter is between those two numbers. That probability is either 0 or 1.",1,0,0
"May be so, but the question says beta is the *slope*. Then how can that be a Null Hypothesis? As per my understanding, Null Hypothesis is one you are trying to disprove (if you can from the sampled data)_x0007_If slope is non-zero, how can that be Null Hypothesis?",1,1,1
Thanks Smgross for the insights. I understand... they have to choose one language and R is very popular.,1,0,1
Peter... this is a great resource.... thanks for sharing with us ...,1,0,1
"Hi Rito,_x0007_I agree its good to know but I still feel python provides more power. _x0007__x0007_Thanks",1,0,1
how to change current working directory,1,1,1
how to change the working directory? btw thank you,1,1,1
thank you,1,0,1
"How to understand the sentence below?_x0007_Does anyone give a specific example to explain this phenomenon ? _x0007_Correlations amongst predictors cause problems:_x0007_- The variance of all coe_x000E_fficients tends to increase, sometimes_x0007_dramatically",1,1,1
Thank you,1,0,1
setwd(),1,0,1
"Thanks for your answers. Yes, I understood the theory: that in supervised learning we have a response variable and in unsupervised we had no response, just measurements. In option c, it says we have labeled examples (response), so I was confused with the others",1,1,1
"Professors - I have taken several MOOC via Coursera, edX. However, never seen such a comprehensive course in terms of delivery that includes free books, classes in videos and pdf format, lab sessions for hands on practice and ample time for each section with timeline till March. The content of the course is well thought through as per today's times. Both of you have set the bar pretty high for MOOC courses for other  professors. Kudos to you for such a brilliant job done for teaching and sharing knowledge across the globe. BIG thank you!!",1,0,1
"Hi, I am not very proficient in mathematics, so the following sentence from ISLR has left me confused: (pg. 67)_x0007__x0007_> \If there really is no relationship between X and Y , then we expect_x0007_that (3.14) will have a t-distribution with n ? 2 degrees of freedom.\""_x0007__x0007_does the n here refer to the number of elements in the training set, or the number of  predictors?""",0,1,1
"Hi,_x0007__x0007_I used this free software. Although not perfect you can get the files in mp4 format or even convert to avi,... _x0007__x0007_I use it to watch the video's on an iPad without Internet connection_x0007_[free youtube downloader][1]_x0007__x0007__x0007_  [1]: http://www.dvdvideosoft.com/products/dvd/Free-YouTube-Download.htm#.UuUdMdI1jeg",1,0,1
Hi! I posted that I have issues loading the R sessions even though I have R downloaded on my computer and have no idea how to fix them. Can someone please help? I really do not want to fall behind and really want to learn all the material in the course on a timely basis. I do not understand why I can not open up the R sessions even though I have R???,1,1,1
"Hi guys,_x0007__x0007_I've just came across with a sentence that left me a bit confusing. I do believe Factor analysis, for example, reduces the number of parameters in it's outcome. However ISLR says that \...non-parametric approaches do su?er from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters...\"" (p.23)_x0007__x0007_Any ideas why that?_x0007_Many thanks""",1,1,1
"what type of os do you have? if windows, towards the end of the installation it will ask you for the short cut, if you chose no, you will have to find the rgui.exe to run the program. Just go to the R installation folder, then bin, then the bit version of your os, in there, you will see the rgui.exe.",1,0,1
"See page 101 and 102 in \An Introduction to_x0007_Statistical Learning\"" (textbook) for explanation.""",1,0,1
"in lecture 2.4, we see the use of test data works better than training data in assessing model accuracy. but how do we choose test data? do we need to randomly choose some data points as test data and fit the model with the remaining data only? thanks:)",1,1,1
"I think the point of this question is to show that the method of nearest neighbors breaks down when p is very large. If this were not the case, the method of nearest neighbors would be used quite often as it comes very close to the \gold standard\"", the Bayes classifier, with respect to test error rate. The error rate of the Bayes classifier is analogous to the irreducible error, which is the portion of test MSE we cannot model. (See page 39 for a discussion of this.) Of course, this depends on the choice of k which optimizes the bias-variance tradeoff. Also, like linear regression, the nearest neighbor classifier can be used with either quantitative or qualitative responses and the result is easy to interpret. In addition, the nearest neighbor method can perform similarly or better than linear regression for both prediction and inference dependent on the true form of the function. In the case of inference, nearest neighbors can perform better as the real world is not usually linear.   Low test error rate, useful with both quantitative and qualitative responses for both prediction and inference, and simple to interpret are all desirable traits of a good statistical learning method.""",1,1,0
@AnneD108. _x0007__x0007_The question about pg 30 of the notes actually refers to pg 30 of the slides used withing the Lecture video for Chapter 2 (see last slide of 2.4).,1,0,0
"I agree with <redacted>. Statisticians prefer R. For me the availability of additional libraries/extensions for R is unmatched by any competitor. There are libraries for everything. When a statistician publishes a paper nowadays about a new algorithm, the first thing he/she does is make a library available to the public. In this course we will use many libraries to perform different tasks. _x0007__x0007_[http://cran.cnr.berkeley.edu/web/packages/available_packages_by_date.html][1]_x0007__x0007__x0007_  [1]: http://cran.cnr.berkeley.edu/web/packages/available_packages_by_date.html",1,0,1
"In addition to Ralph_W's response, does anybody know if the quizzes' deadlines are enforced in any way? In Coursera, for example, if you are late taking a quiz it simply does not count at all in the scoring. Here it looks that most quizzes have impossible due dates, as if the teaching team wasn't really using them._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"There is a bar on top of the video for navigation to the quizzes. It's hidden, so you have to hover the mouse on the bar toward the right and then click when the quizzes appear. Very bad design!",0,1,0
"I wish we were free to use whatever language we want in those online courses. As a computer scientist, I prefer Octave to R because I use another full programming language and I don't need all the bells and whistles of R.",0,1,1
I finally realised my mistake that I cannot blindly apply eq 3.14. <redacted> and<redacted> - thanks for your posts. I had to re-read them to realise where I was going wrong. Thanks - once again.,1,0,0
"If you are using the Rgui.exe, download it to your default directory and you can select the File | Open script and you should see a display of all the .R files. Select the one you want and it opens in a text editor within R. You can select one or more lines and execute them without typing. Retyping the commands may help you to learn R faster however and encourages you to try variations on each command.",1,0,0
"The term library refers to repositories of R packages. There is one main library, CRAN, that is mirrored all over the world and it contains over 3000 packages. Once you install a package (using install.packages()), you must load it during an R session using the library() function before you can access its functions. There is a package called \class\"" that has a function \""knn\"" that performs nearest neighbor classification.""",1,0,1
"I believe the third choice should be \...and construct a **95%** confidence interval...""",1,0,1
"Is it just me, or do the questions in a lot of these quizzes bear very little relationship with the course lectures and/or text? There is a ton of knowledge of statistics that is being assumed in the questions being asked, or else the question writer didn't actually read the text and listen to the lectures before writing the questions. There is terminology used that has never been introduced, and questions being asked that require knowledge beyond the material that has been presented._x0007__x0007_I don't think I'm dumb (I have a PhD in astrophysics), but I'm getting really frustrated with the quizzes in this class, as I don't have any real way to approach being able to answer them.  I could muddle through the 1st weeks questions, but the 2nd week just seems an order of magnitude more obtuse._x0007__x0007_Should I give up on being able to answer the quiz questions, and not worry about it, or should I expect a higher level of quality?_x0007__x0007_Sorry if this sounds harsh, but I'm having a hard time \getting\"" this material from the lectures and text.""",0,1,1
setwd() is the comand_x0007_help(setwd) can help you how to add the arguments,1,0,1
"if you are using r studio the packages installs itself. you need to call the package if you want to use it  library(\packagename\"")_x0007__x0007__x0007_sites like stackoverflow are very usefull""",1,0,1
The file Auto is part of package ISLR and do not have the extension csv. That is the reason of some troubles,1,0,1
"Dear ahlf_x0007__x0007_Your point is a good one: my statement was not justified.  You said it well:_x0007__x0007_\It seems that, strictly speaking, the hypothesis test result does not necessarily mean that TV advertising affects sales - only that that there is relationship between the two variables. It might be that advertising affects sales. But it might just be that bigger markets have bigger sales AND bigger advertising budgets so the two variables tend to be related\""_x0007__x0007_Thanks for pointing this out_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,0,1
"The prerequisites for this course are: \First courses in statistics, linear algebra, and computing\"". So a basic knowledge of statistics is assumed._x0007__x0007_So there will be questions asked where it is assumed you know the technique that is being talked about.""",1,0,1
<redacted>: That was spot on. Could you give a similar analogy for Bayesian's point of view?,1,0,1
I agree._x0007_<nameRedac_<anon_screen_name_redacted>>,1,0,1
"The sales numbers are not dollars, they are thousands of units (i.e. count of actual products sold).  See the caption of Figure 2.1 in the textbook.",1,0,1
"OK, looks like I got it to work in RStudio, thank you for your responses! But now what I do not understand is this seems to be only a bunch of functions, what is the actual assignment that needs to be done?",1,1,1
It was also mentioned that a confidence interval is also doing a hypothesis test (95% CI is also doing a hypothesis test at the two-tailed 5% significance level),1,0,1
"Hi, _x0007_I can't tell if this has been answered already._x0007__x0007_The #1 thing you must do every time you use R is to tell it which directory you will be using (your \working directory.\"")_x0007__x0007_In Windows, go to File --> Change dir... and tell it to use your desktop._x0007_I would actually recommend creating a new folder somewhere, naming it something obvious or meaningful, and then pointing R to that folder._x0007__x0007_In the Mac OS, in R, go to Misc --> Change Working Directory to set your working directory.""",1,0,1
"True. The staff have replied elsewhere that you can use other programs. _x0007__x0007_However only R is supported here. So if you have issues with,say, Matlab you are on your own or reliant on other Matlab users to help you out.",1,0,1
"Not sure I grok why that's a \null\"" hypothesis, and not just a \""hypothesis\"", but ok, sure, I get what's going on here.  Thanks!  (BTW, here did this example come from?)""",1,1,1
"The program is Stata and the data set is \auto\"". It's just one I had access to without having to remember R commands._x0007__x0007_The null hypothesis doesn't have to be testing that something $=0$. It's stating a default position that you then test.""",1,0,1
"You don't need to do a t-test. You have a mean, standard error, and a significance level (95%, use 2 in your calculation). That's all you need.",1,0,1
"I could use some guidance on this, too. I opened Chapter 2's R Session and see the R script in the upper left window of R Studio, but I'm not certain what my task is. Any help will be appreciated.",1,1,1
"I am working on the exercise in Chapter 2 that deals with the Auto data. We are asked to look at data that does NOT fall inside of the rows 10:85_x0007__x0007_I tried using that logic at this link [Stack Overflow][1] but am pretty stumped_x0007__x0007_Best,_x0007__x0007_S_x0007__x0007__x0007_  [1]: http://stackoverflow.com/questions/<zipRedac>1128<zipRedac>2/removing-specific-rows-from-a-dataframe",1,1,1
"I think you can use negatives to make it as \show everything but (selection)\"":     _x0007_Auto[-10:-85]     _x0007_should show rows 1 to 9, then 86 to end.""",1,0,1
"Subsetting with square brackets uses the logic `[row, column]`._x0007__x0007_So you can do either `Auto[-10:-85, ]` or `Auto[-(10:85), ]` where nothing after the comma means select all columns.",1,0,1
"I think it depends on your computer specifications. _x0007__x0007_There are various packages that you can use to help manage memory and really big data sets. I don't know anything about them, but if your Google something like \R large data set\"" you should find some info that might help.""",1,0,1
"Where is the run button I don't see it? But what is the dataset, it seems like there is just a bunch of functions out there?_x0007__x0007_Is there supposed to be some actual exercise that needs to be handed in?",1,1,1
"Strange. I can load the Carseats data set without problems. I've got RStudio 0.97.311, R 2.15.2 on Windows 8.1",1,0,1
It would be nice if this course were offered thru Coursera._x0007__x0007_Has anybody found the way to save video lectures for offline viewing onto their iPad ?,1,1,1
Thank you! Excellent explanation.,1,0,1
"This is generally pretty seamless to do from within R itself. Try the following in RStudio, or a standard R session_x0007_<code>_x0007_<br>_x0007_install.packages(\ISLR\"")_x0007_<br>_x0007_library(ISLR)_x0007_<br>_x0007_?Carseats_x0007_</code>""",1,0,1
"What's wrong with the second option? Because it says \linear regression\"", which is too restrictive?_x0007__x0007_By the way, I also thought the 3rd option is not right because no 95% is specified.""",1,1,1
"<redacted>: Thanks for the clarification.  It looks like it's best to consider the statement on slide 7 in light of the clarification on slide 8, i.e. the note \under a scenario where we got repeated samples like the present sample.\""_x0007__x0007_For me, the key p",1,0,1
library(class)_x0007_    ?knn,1,0,1
"The confidence interval applies to the range encompassing the value, not the value falling within the range. The second option speaks of the probability that the true parameter lies between 0.4 and 0.5, which is not correct.",1,0,0
I agree with Chris' comment. Another way to think of this is to remember ethnicity is qualitative. If you put all three options into a single variable you're turning ethnicity into a quantitative [continuous] variable.,1,0,1
"I just reopened RStudio, attached ISLR again, and there it is._x0007__x0007_Seems I failed the sanity test._x0007__x0007_Never mind!",1,0,1
"What do you mean by \impossible due dates\""? Everything is due on March 21st. _x0007__x0007_So those who join late can still complete assessments and get credit for them. Many courses don't let you do that, deadlines are usually 1 or 2 weeks tops.""",1,1,1
"jk, you may want to check out Udacity's statistics course at https://www.udacity.com/course/st095 It's very well done and will cover all the basics you need here.",1,0,1
"The lecture slides are also available for download under the \Lecture Slides (pdf)\"" tab if you'd like to look at them outside of the videos.""",1,0,1
"Hi all, _x0007_I am not sure what happen on other computers, but the sound volume is so low that I need to push the volume icon of my system all the way up to the max level. However this too much for other apps._x0007__x0007_Is anyone having the same problem?",0,1,1
"Thank you, I had this doubt since in pg. 61 of ISLR n was given as 200 and in pg. 68 n was taken as 30, for computing the t-statistic.",1,0,1
Actually in practical terms you can encode dummy variables as suggested.  However you must tell the statistical software *not* to interpret them as quantitative variables.  It will then  perform the calculations correctly._x0007__x0007_In R see `?factor`.,1,0,1
Hi_x0007_Can somebody provide clues on solving this question ?  I am not sure if the topic is discussed in lect 2.2 ?_x0007__x0007_TIA,1,1,1
The reference group value is the value when the other ethnicities are both set to $0$._x0007__x0007_If you want the full data set it is on the textbook's webpage:_x0007__x0007_http://www-bcf.usc.edu/~gareth/ISL/data.html,1,0,1
"To confirm, the deadline to submit the quiz is 18:00 pacific time on March 21 for each quiz.",1,0,1
"The confusion comes from the slide 7 of the lecture, where it is stated that \with 95% probability, the range will contain the true unknown value of the parameter\"". This can be understood as the value belongs to the range with that probability since the range is computed by the specific formula. I think it is not stressed enough during the lecture the real meaning of the confidence interval. Moreover, professor Tibshirani said: \""if you want a confidence interval of 95%, we take the estimate of our slope plus or minus twice the estimate of the standard error. And this, ..., this will contain the true value, the true slope, with probability 0.95.\"", which most people would understand as the probability that the true parameter lies in this interval is 95%.""",1,1,1
In what ways the goals of statistical learning differs from machine learning? If there isn't any difference then what are the possible reasons of using a different name?,1,1,1
"Failing to explicitly specify 95% confidence for each interval in that question was a mistake by the teaching staff. For that question, you may assume all confidence intervals are 95%. For why the second and third options are incorrect, see my explanation here:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/52e55be09b16c7d<phoneRedac>4d",0,1,0
I agree with both <redacted> and <redacted>. See also my explanation here:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/52e55be09b16c7d<phoneRedac>4d,1,0,0
Thanks sallysue._x0007__x0007_Ok I've worked it out.  The way the question was worded suggested to me that I should run the model myself hence why I was looking for the data and I was expecting the R package `ISLR` to contain all of the datasets as the course is based on the textbook and the package [description][1] describes this (although the datasets listed on the page you link do not match those reported by the R package itself once loaded?)._x0007__x0007__x0007_  [1]: http://cran.r-project.org/web/packages/ISLR/index.html,1,0,1
"Hi,_x0007__x0007_I've been following a couple posts, but if I select the \*Posts I'm Following\"" filter, the search doesn't return any hits. I don't remember the exact post titles now, so those threads are lost to me. Can you please fix this bug, so that once you start following a post, the filter will find them afterwards? Thanks,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"i am not able to access to any video, please provide links for downloading, just like other MOOC offers.",1,1,1
"It seems that statisticians put more weight on evaluating the accuracy of a model, which can sometimes get really mathy, but those who are from CS field(and who are more likely to use the term 'machine learning') care more about topic-specific algorithms. It also seems that most of the commonly used machine learning(or statistical learning if you like) algorithms are introduced by CS people, and stats guys then delved into the mathematical properties of these methods and also came up with evaluating and selecting schemes. Besides, some of the popular machine learning methods have solid statistical background, like Ridge Regression and PCA. _x0007__x0007_In my mind, statistical learning is machine learning in the view of the statistician, or a mathy version of the other one. _x0007__x0007_Above is my own humble understanding on this issue.",1,0,1
"I think this question can be confusing: “A fitted model with more predictors will necessarily have a lower Training Set Error than a model with fewer predictors: T/F”_x0007__x0007_I would understand that this is a complete vs. reduced model comparison (the model with more predictors considers additional explanatory variables), while from the correct answer is apparent that the sets of explanatory variables may be different._x0007__x0007_In my case (I am not native English speaker, as may be obvious…), it would be clearer to speak about a larger/less number of predictors.",1,1,1
Yes. Volume is low and I had to read the subtitles.,0,1,1
"On the latter two:_x0007__x0007_1. Yes, it's the variance, which is by definition nonnegative and equal to zero only in uninteresting situations of a perfect fit._x0007_2. The field data was generated by adding Gaussian white noise to a known curve. Variance of such noise is by definition 1.",1,0,0
"Those are from the `boxplot` command, which is called up by `plot` if the data is categorical. At least was so in earlier versions of R.",1,0,0
Ingvar_x0007_Many thanks for your lucid reply,1,0,1
The questions accompanying the 2.2 lecture section seem to refer to the material in section 2.3,1,0,1
"I agree.  I thought the course was going to be more about how to select and use appropriate statistical models to explore various data sets.  I do not know what a hypercube is, let alone have any idea how to solve for 2.2 R2.  Furthermore, it is impossible to get an explanation of the answer without getting the answer correct.",0,1,1
"Data Mining, Machine Learning, and Statistical Learning basically mean the same thing. The terms originated from different departments (Computer Science versus Statistics).",1,0,1
"Hello!  I really appreciate your taking the time and effort to arrange this MOOC and allow a wide variety of folks to take it.  For many of us, making the pilgrimage to Stanford is physically impractical - time constraints and all - so this medium makes it a possibility.  This is my 3rd exposure to R so curious to see how you handle the tricks of the language from a practical basis.  I do appreciate you making the book pdf available for free - very generous on your part.",1,0,1
"Try solving this problem in \intuitive dimensions\"" (1D, 2D, 3D) first, draw the boundaries on a 1D line, than on 2D square, you should understand it from there, and you could check it on 3D, the generalization on n dimensions applies easily (it really does, don't get discouraged).""",1,0,1
"My answer to this question was included in the real answer, but the computer marked me incorrect. Has anyone else experienced this problem?",0,1,1
"Sorry, my mistake. Am used to Igor programming where case doesn't matter and typed lower case c.",1,0,1
"I may not have listened carefully enough to 2.2, but this question seems much better fitted (pun intended ;) to the \Model Selection and Bias-Variance Trade-off\"" section (2.3)""",1,0,1
Is there a record of the solutions posted by me for question 3.2.R1? I ask because I posted the correct answer which was marked wrong.,0,1,1
Just entered the answer and got it marked correct.,1,0,1
"Good question. I'd guess that, for whatever reason, statistics tends to be taught in a more \pure\"", theoretical form. Similar to how physics classes often assume perfect spheres acting in a friction-free vacuum._x0007__x0007_Courses on applied statistics, data analysis, data mining, research methods, etc., *do* often cover missing data, imputation, etc. (They're like the civil engineering class you take after the physics class!)""",1,0,1
"I've been tearing my hair over these questions. I thought I knew something about linear regression, but I have scored so low that it is obvious I don't know anything at all. Commiserations to anyone else in the same category. Probably if I had checked off the inverse of what I thought to be true I might have succeeded. Hoping the rest of the course will be enlightening.",0,0,1
"I'm still not getting it. Suppose I flipped a coin several times but hid the results from observers A and B who have to guess whether or not it is heads. If A always makes his guess before I flip the coin, each flip guess will have a 50% probability of being correct.  If B makes his guess after I flip the coin it is no longer random, the coin flip has happened, it is either heads or it isn't.  But B doesn't know the true value of the parameter (H or T), he has to make his best guess each time. The probability that any one of B's guesses will be correct, that the coin that he can not see underneath my hand matches his guess, is still 50%.",1,0,1
If you put Y~X1*X2 then your model is Y=X1+X2+X1X2(interaction)_x0007__x0007_If you put Y~X1:X2 then you model is just Y=X1X2(interaction)_x0007__x0007_If you put Y~.+X1:X2 then your model includes all the variables and the one interaction.,1,0,1
"But that supposes that you have more than about 30 data points, no? You are essentially saying to use the normal distribution instead of a t-distribution.",1,1,1
"Hi everybody,_x0007__x0007_my colleagues at DataRobot and I started a blog series on Statistical Learning with Python. It follows along with the course and shows how many of the statistical learning techniques presented in the course can be applied using tools from the Python ecosystem instead of R._x0007__x0007_You can find the blog here: http://www.datarobot.com/blog/_x0007__x0007_best, _x0007_ <nameRedac_<anon_screen_name_redacted>>",1,0,1
"Thank you for this course, I hope to learn so much about this. I like the statitics.",1,0,1
"Do you mean you said the answer was 0? It's not. You have your formula, what do you get when the X variables are both set to 0 - this means the person is AfricanAmerican.",0,0,1
"Okay, so * basically says \test the Main effect AND the interaction\"" while Colon says \""do not test the Main effect(or rather there's nothing in there about it), but Test the interaction\"", assuming the rest of the command is the exact same?""",1,1,1
Giving all suggestions a whirl now,1,0,1
"Thanks all, worked like a charm",1,0,1
"Ff nearest neighbor classifier faces the curse of high dimensions, how do we use it properly? In the video we're told it is a power tool and best tool for 1/3 of the classification problems. So there must be some tricks we can use. Could someone please provide some guidelines or research papers? Or maybe an optional reading section? Thank you.",1,0,1
"Am I missing something in regards to the wording of this question?  It seems as though all three of the statements support a fairly strong linear relationship between x and y.  But, giving all three as an answer is marked as incorrect. is there some subtlety that I'm missing here?",0,1,1
"Thanks for all the feedback on questions!  Make sure to keep suggestions coming._x0007__x0007_As always, \\\_\\_\\_\\_ would be clearer if \\_\\_\\_\\_\"" helps more than \""\\_\\_\\_\\_ sucks.\""""",1,0,1
"To get a boxplot from plot() the x-value has to be categorical (a factor). The book uses `cylinders=as.factor(cylinders)` to make a factor. You can do it all in one command (and without the `attach()`) by using_x0007__x0007_`with(Auto, plot(as.factor(cylinders), mpg, col=\red\"", varwidth=T, horizontal=T))`""",1,0,1
"I guess this is about larger/less number of predictors, since the data and the probability assumptions remains unchanged._x0007__x0007_But the question is: It is worth considering this bunch of predictors, while with only few of them the model will perform just as good as? I mean, the training set error will strictly decrease, but how strong will be this decreasing that matters. It is about a decision regarding what is a lot or just a negligibly little gain in the precision. And depending on which predictor you withdraws from the model, it will cause a more or less impact, so the problem lies in **which** predictors is \the best\"" and not on **how many**._x0007__x0007_PS: I am not a native english speaker either, hope you understand me =P.""",1,1,1
"This is a great blog, i will definitely try to follow along... I'm a big supporter of Python",1,0,1
"On pg. 62, the book simply jumps to the RSS minimizer without the mathematical logic.  I just hate that.  Anyone know how (3.4) is derived?  Or can you point me to a resource?",0,1,1
http://isites.harvard.edu/fs/docs/icb.topic<zipRedac>98855.files/OLSDerivation.pdf,1,0,1
"I was trying to calculate VIF manually, so I could get a better feel for it - _x0007__x0007_I was confused by the definition on the book, I am not saying it is not clear, just that I did mot understand it. I try to understand the concepts versus just memorize them and was looking to get an 'intuition' for it (I took the Stamford ML class as well and Professor Ng always talked about the intuition, which worked for me)_x0007__x0007_I did some digging and this is how one can do it, for those who may be interested in understanding it_x0007__x0007_Following the example from the book to fit the model (as an R formula): balance ~ age + rating + limit_x0007__x0007_ say we want to understand if rating is collinear and calculate its VIF - this is what one does: evaluate a model in which we regress the other factors on rating_x0007__x0007_    summary(lm(rating~age+limit,data=credit)), which yields _x0007__x0007_Call:_x0007_lm(formula = rating ~ age + limit, data = credit)_x0007__x0007_Residuals:_x0007_   Min     1Q Median     3Q    Max _x0007_-31.82  -8.62  -1.30   8.61  30.43 _x0007__x0007_Coefficients:_x0007_            Estimate Std. Error t value Pr(>|t|)    _x0007_(Intercept) 3.73e+01   2.33e+00   16.02   <2e-16 ***_x0007_age         2.35e-02   3.57e-02    0.66     0.51    _x0007_limit       6.68e-02   2.67e-04  250.42   <2e-16 ***_x0007_---_x0007_Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1_x0007__x0007_Residual standard error: 12.2 on 397 degrees of freedom_x0007_Multiple R-squared:  0.994,	Adjusted R-squared:  0.994 _x0007_F-statistic: 3.17e+04 on 2 and 397 DF,  p-value: <2e-16_x0007__x0007_Now we have the R squared for this model, 0.994 and plugging that into the formula provided 1/(1+R2) we get 161 as indicated (or as obtained if we use vif in package car) (note you need to use the entire digits to get 161, one can access the value using_x0007_summary(lm(rating~age+limit,data=credit))$r.squared)_x0007__x0007_Now this makes perfect sense to me - we basically measured how much of the variation in rating we can explain by the other variables and the answer is 99.4%._x0007__x0007_In fact if you do the model for rating regressed on limit you get the 99.4% as well_x0007__x0007_Now that I understand this, I do not need to ever again do it - I am happy calling vif or other function to automate this in R_x0007__x0007_I hope this helps others",1,0,1
Ah!  I missed that one line to set cylinders as a factor.  Works now - Thanks!,1,0,1
"From the textbook (pg 67): \Roughly speaking, we interpret the p-value as follows: a small p-value indicates that it is unlikely to observe such a **substantial association** between the predictor and the response ...\""_x0007__x0007_This seems to contradict the answer they're looking for in the quiz question. (Unless one can argue that the phrase \""substantial association\"" does not imply \""strength of relationship\"".)""",0,1,1
"Yes, I also feel confuse about it.",1,0,1
I found the answer on wikipedia:_x0007_http://en.wikipedia.org/wiki/Bias-variance_dilemma,1,0,1
"In the lecture on slide 8, there is a graph showing \Fraction of volume\"" on the x-axis and \""Radius\"" on the y-axis. Then the plot shows how increasing dimensionality of the feature vector increases the radius of the boundary region. I'm unsure how the plot was calculated, but the Quiz question pertains to determining the fraction of volume for p=50.""",1,1,1
"It puzzled me too. Consider an extreme case:_x0007__x0007_Your p-value for beta_1 is .5, and in the same model your R^2 is .9. Would you call that \strong correlation\"" between X_1 and Y?""",1,1,1
"Yes, it is a bit of a definitional issue. The p-value indicates whether the result -- the association between predictor and response -- is likely to occur by chance, while the correlation describes the strength of the relationship between the two.",1,0,1
"Well, if you like, you could leave a email address so that I could send this pdf to you via email.",1,0,1
"In one case you're looking at the probability of the confidence interval encompassing a parameter. In the other you're looking at the probability of a parameter falling within a confidence interval. It's a matter of perspective._x0007__x0007_When we evaluate study results we're essentially asking if the results we see are likely to have occurred by chance, and the confidence interval defines the range of values we would expect by chance. As **alhf** writes below, the confidence interval is not about trying to predict the outcome._x0007__x0007_I hope that helps.",1,0,1
"The **attach()** command in R makes it very easy to specify objects in your code. Be careful, though, because it is possible to attach two dataframes that contain the same variable name. If you do this, you may end up using the wrong data in your analysis._x0007__x0007_There is a **detach()** command that removes an attached dataframe. Make it your friend if you choose to use attach()._x0007__x0007_I've chosen not to use attach(). Typing *data$var* is a little more work, but I don't have to keep track of what I've attached and I feel it makes my code easier to read when I'm debugging or coming back to something later. You'll figure out what works best for you. Good luck!",1,0,1
"The first two weeks ran about one hour five minutes and one hour twenty-eight minutes, respectively. I'm guessing that will be typical._x0007__x0007_I've actually spent more time reading than watching, but that's OK.",1,0,1
"I am Medical Doctor working in Nigeria. Has a good grasp of Epidemiology and Biostatistics. Familiar with Stata, looking forward to learning R and other Statistical Modelling methods in this course.",1,0,1
"I understand the epistemological point behind the question, but I'd love to have a practical real-life example on how making this mistake leads to the wrong conclusion and decision. It's easy to think of and find examples with the analogous null hypothesis interpretation, but here I can't think of any. Do you have any examples or even a better theoretical explanation on how to go wrong because of this?",1,1,1
"Hi!_x0007__x0007_Just wanted to say that the course is great. Fun to follow, and a great learning experience! Compliments to the teachers and all the staff :)",1,0,1
If you put a dollar sign before and after the epsilon you get $epsilon$._x0007__x0007_The platform uses MathJax._x0007__x0007_http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm_x0007__x0007_You need to add the $ signs before and after the commands that are given on the link.,1,0,1
Excellent! I would vote your post $infty$ times if I could :),1,0,1
"I have the same question._x0007_Just quote from the slides Linear Regression on page 7/48:_x0007__x0007_> \A 95% confi_x000C_dence interval is de_x000C_fined as a range of values such that_x0007_> with 95% probability, the range will contain the true unknown value of_x0007_> the parameter.\""_x0007__x0007_3.1 R2 option 2:_x0007__x0007_> \""If I perform a linear regression and get confidence interval from 0.4_x0007_> to 0.5, then there is a 95% probability that the true parameter is_x0007_> between 0.4 and 0.5.\""_x0007__x0007_Is there any differences between the two sentences?""",1,1,1
None of any video is opening both in my computer and laptop. I am in a big problem. Kindly suggest me and help me in this regards.,1,1,1
Great example -- really helpful to illustrate difference between a statistically significant relationship (ie reject null hypothesis of $_x0008_eta_1=0$) vs a relationship that explains much of the variance of $y$ (ie low residual sum of squares relative to total sum of squares).,1,0,1
"Fabulous hint, really helped me to grasp the problem. Thanks!",1,0,1
"I found missing data are often inadequately reported/handled in many published studies, even from prestigious journals. I am from the field of health service research, analyses of secondary data or admin data is often the norm (which is impossible to have no missing data), yet stat courses offered, even for advanced learner, seldom mentioned the handling of it._x0007__x0007_I wonder whether this is just our local situation(Malaysia)? or there is a more general reason? anyone from other field or country or region can share some insight?",0,1,1
"Hi, Doug! I seem to recall another student with the same nick either in \Statistics: Making sense of data\"" or in \""Statistics One\"". Was that you?""",1,0,1
Is there a way to download the videos to my device and view them without being connected to the internet?,1,1,1
"In section 3.4 Professor Hastie gives a brief description of variable selection methods such as forward and backward selection using a variety of measures of model quality.  I was under the impression that these methods have serious problems and should be avoided.  In his book \Regression Modeling Strategies\"", Professor Frank Harrell gives harsh criticism of variable selection,  due to the effects on the estimation bias of model coefficients and variances. Also, in the introduction to his method on the lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267-288],  Professor Tibshirani mentions the negative effects of these methods on prediction accuracy. _x0007__x0007_So, my question is, what is the state-of-the-art advice regarding the use of variable selection procedures? Is there consensus?""",1,1,1
"Another gotcha using `attach()` is that it places a **copy** of the object on to the search path. If you alter the original object, those changes are not reflected in the copy on the search path._x0007__x0007_If you are working at the command line, i.e. not programming in a function, then consider `with()` & `transform()` as alternatives to littering your code with `foo$bar`._x0007__x0007_For example:_x0007__x0007_    set.seed(1)_x0007_    df <- data.frame(A = rnorm(10, mean = 4), B = rnorm(10, mean = -3))_x0007_    with(df, mean(A)) ## mean of A, same as mean(df$A)_x0007__x0007_If you want to alter a variable in a data frame, use `transform()`, e.f._x0007__x0007_    df <- transform(df, A = sqrt(A), C = B^2)_x0007_    head(df)_x0007_    _x0007_    R> head(df)_x0007_          A      B      C_x0007_    1 1.837 -1.488  2.215_x0007_    2 2.045 -2.610  6.813_x0007_    3 1.779 -3.621 13.113_x0007_    4 2.365 -5.215 27.193_x0007_    5 2.081 -1.875  3.516_x0007_    6 1.783 -3.045  9.272_x0007__x0007_Also, use the formula interface to functions where they exist, i.e._x0007__x0007_    plot(A ~ B, data = df)_x0007__x0007_is nicer than_x0007__x0007_    plot(df$B, df$A)_x0007__x0007_etc.",1,0,0
There are a bunch of threads already on the discussion forum about this. Please post in those threads rather than any random thread.,1,0,1
"But to form a CI you need the critical value of $t$. To get that you need the degrees of freedom for the distribution. An assumption is being made that either i) an approximate 95% confidence interval is OK, or ii) that $n$ is large. So whilst to get the Answer the authors want you don't need to look up the critical value of $t$, you do need to make an assumption that they want you to use 2 as that value._x0007__x0007_The Q is a little ambiguous in this regard.",1,0,1
Thank you!,1,0,1
$epsilon$. _x0007_Whoa :),1,0,1
"I observed that in slide 16 of statistical learning is introduced a graph with de Independent variable as flexibility. Without any other explanation is after a couple of slides introduces as a variable that can be measured to MSE_Te, and_x0007_MSE_Tr. How is this flexibility calculated?_x0007_Thanks,",1,1,1
"I will say that if you want to do data analysis,  then learning R is likely a good investment in your effort. I am sure many of the things we will do can be done with Python, but it may also be that some can't be done as easy (for example say there is an existing routine to do something in R and nont in python, of course you can write your own but that may be time consuming)_x0007_ If I was taking the class and if it was in Python, I would do Pyuthon as opposed to try to force R - when I took the ML class from stamford that used Octave I used it as an opportunity to learn octave, - just my 5 cents",1,0,1
"You have all data in slide \Results for ethnicity\"" in file Ch3 Linear Regression (pdf)""",1,0,1
"I am confused by the question itself. The null hypothesis requests that beta_1 = 0 and not equal to some b. Also beta_1_hat is given with 0.5, the corresponding SE is equal to 0.2 which clearly puts zero outside the corresponding confidence interval in the first place. Can someone help me understand the question? Thanks.",1,1,1
"Well, this won't give you the same result for a linear model because the form $_x0008_eta x_i$ means that the effect of being Caucasian is restricted to being between the effects of being Asian and being African American.",1,0,1
"Yes, you are correct.And there are ways to construct other confidence intervals from the advertising data. See, for example, the R function confint() to help with this.",1,0,1
"The set.seed function is used so that so that random numbers generated following set.seed will be the same. It is often used in examples in a text book to ensure that the example in the text is reproducible. _x0007__x0007_For example:_x0007__x0007_ x= rnorm (50) ,_x0007_ y=x+ rnorm (50 , mean =50 , sd =.1) ,_x0007_ cor (x,y)_x0007_ _x0007_will return different values for cor(x,y) depending on the random numbers generated. However_x0007__x0007_ set.seed(1303) ,_x0007_ x= rnorm (50) ,_x0007_ y=x+ rnorm (50 , mean =50 , sd =.1) ,_x0007_ cor (x,y)_x0007_will generate the same result for cor(x,y)_x0007__x0007_Any integer can be used as the argument for set.seed.",1,0,0
You're welcome :),1,0,1
"> Here's a follow-up question: Let's pretend that I did_x0007_> actually obtain 100 sample data sets, and then constructed a_x0007_> 95% confidence interval for each of those 100 data_x0007_> sets. Could I then examine those 100 confidence intervals_x0007_> looking at which single value is in nearly all of those_x0007_> intervals, and then declare that value the \true\"" value?_x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007_There is no guarantee that ANY of the confidence intervals_x0007_will \""capture\"" the true value.  You'd *expect* 95% of them_x0007_to contain the true value, but it would be *possible* for_x0007_you to be very unlucky and get 100 data sets where *none*_x0007_of the confidence intervals contained the true value._x0007__x0007__x0007__x0007__x0007_Also, you'd probably find that there was no *single* value_x0007_in nearly all intervals, but rather a *range* of values that_x0007_were in nearly all intervals.  This range would be narrower_x0007_than each of the individual confidence intervals.  You could_x0007_*sort of* think of this \""range of overlap\"" as being a_x0007_narrower confidence interval.  (And \""narrower\"" is a good_x0007_thing for a confidence interval)._x0007__x0007__x0007__x0007__x0007_In practice, if you had 100 data sets each with 20 coin_x0007_tosses, you would put them all together to make a single_x0007_data set with 2000 coin tosses and construct a confidence_x0007_interval from that.  The standard error of the big data set_x0007_would be less than that of a single sample.  This_x0007_corresponds to the everyday intuition that \""the average is_x0007_more precise if you average a larger number of_x0007_observations\""._x0007__x0007__x0007__x0007__x0007_So you'd get a narrower confidence interval_x0007_when you constructed it from the big set (all the data)""",1,1,1
Thank you for sharing it!,1,0,1
"Hi,_x0007__x0007_I like the book very much!  This will be a great learning experience!_x0007__x0007_Thanks to the teachers and course staff!",1,0,1
Hi !_x0007__x0007_I have the same request since otherwise I will not be able to follow the course and will have to drop out._x0007__x0007_Thanks in advance._x0007__x0007_Cheers !,1,1,1
"In SAS, you can use the original variable for different types of comparison among the levels or categories. You may want to check out: _x0007__x0007_http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm_x0007__x0007_But I am not sure how to do this in R.",1,0,1
Does anyone have a link to the solutions for all the textbook exercises?  I would like to see if my solutions are correct or incorrect. Thanks.,1,1,1
"I think the wording of the question is a little twisted around from the normal usage.  Typically the null hypothesis would be that B1 = 0, in which case:_x0007__x0007_ - rejecting the null hypothesis would require a confidence interval that does not contain 0 and_x0007_ - NOT rejecting the null hypothesis would require a confidence interval that does contain 0_x0007__x0007_But since in this particular question, the null hypothesis takes the form of B1 = b:_x0007__x0007_ - rejecting the null hypothesis would require a confidence interval that does not contain b and_x0007_ - NOT rejecting the null hypothesis would require a confidence interval that does contain b_x0007__x0007_It's the latter case we're concerned with here, so we need a confidence interval that does contain b.  Since b = 0.5, SE = 0.2, and CI = b +/- 2*SE, the confidence interval is [0.1, 0.9].  So the \largest value of b for which we would NOT reject the null hypothesis that B1 = b\"" is 0.9, the top end of that CI._x0007__x0007_Hope this helps..._x0007__x0007_Thanks,_x0007_<nameRedac_<anon_screen_name_redacted>>""",1,0,1
"Hi, I am also having trouble to derive beta1 in equation 3.4 of the book (page 62). It is quite straight forward to reach beta1 as showed in the link that Sally provided (equation 11 on page 8 of the pdf). It is in the form I often see in text books. I was just wondering if anyone could find how beta1 can be expressed in the form presented in eq. 3.4.",0,1,1
"Thanks, Andrea. *Statistics One* it was. Did I find out about this course from your comment? If so, thanks! It's off to a great start.",1,0,1
thanks for this,1,0,1
"RSS in this case refers to the sum of the squared values of the (true y minus model-predicted Y) for each (x, y) pair in the training set.  _x0007__x0007_So, in the terminology you are using, it is the sum of the squared distance from the \least squares line\"" to each of the individual points in the training set.""",1,0,1
"The course slides (40/48 in chapter 3) state:_x0007__x0007_\If we include an interaction in a model, we should also_x0007_include the main e_x000B_ects, even if the p-values associated_x0007_with their coe_x000E_cients are not signi_x000C_cant._x0007__x0007__x000F_Speci_x000C_cally, the interaction terms also contain main eff_x000B_ects,_x0007_if the model has no main e_x000B_ffect terms.\""_x0007__x0007_I don't understand this statement.  Can anyone help me understand it?""",1,1,1
can anybody help me on this question? i did not understand what are they asking....,1,1,1
"It's asking you to use the regression model to give a value to the balance for the groups in question._x0007__x0007_The equation is on slide 33, solve it for AA and for Asian.",1,0,1
"Here's another way to think about it...Consider the advertising model from the text with the TV x Radio interaction term (Page 88).  The model yields:_x0007__x0007_Sales = 6.75<zipRedac>2 + (<zipRedac>.191 * TV) + (<zipRedac>.<zipRedac>289 * Radio) + (<zipRedac>.<zipRedac><zipRedac>11 * TV * Radio)_x0007__x0007_If you're trying to estimate the change in Sales for every 1<zipRedac><zipRedac><zipRedac> in Radio spending, you'd do:_x0007__x0007_$Delta$ Sales / $Delta$ Radio = (.<zipRedac><zipRedac>289 * $Delta$ Radio) + (<zipRedac>.<zipRedac><zipRedac>11 * TV *  $Delta$ Radio)_x0007__x0007_= (<zipRedac>.<zipRedac>289 * 1<zipRedac><zipRedac><zipRedac>) + (<zipRedac>.<zipRedac><zipRedac>11 * TV * 1<zipRedac><zipRedac><zipRedac>)_x0007__x0007_= 29 + 1.1 * TV_x0007__x0007_In the textbok, the p value for Radio is fairly low, but let's assume for a moment that it wasn't.  In that case, if it weren't for the hierarchy principle, you'd drop Radio from the model and re-run the regression with just TV and the TV * Radio interaction term.  That would yield this model:_x0007__x0007_Sales = 7.428 + (.<zipRedac>1558 * TV) + (<zipRedac>.<zipRedac><zipRedac>12 * TV * Radio)_x0007__x0007_To estimate that same change in Sales for every 1<zipRedac><zipRedac><zipRedac> in Radio spending, you'd do:_x0007__x0007_$Delta$ Sales  $Delta$ Radio = (<zipRedac>.<zipRedac><zipRedac>12 * TV * 1<zipRedac><zipRedac><zipRedac>)_x0007__x0007_= 1.2 * TV_x0007__x0007_So in the first example, you have separated out the impact of the 1<zipRedac><zipRedac><zipRedac> in incremental Radio spend to the part that is tied to TV spending (the \synergy\""), which is 1.1 times TV spending with a high degree of confidence, and the part that is due solely to Radio spend (the 29, a value in which you would be far less confident).  _x0007__x0007_In the second example, you are lumping all that in together (the 1.2 * TV includes the impact of the synergistic effect plus whatever other standalone Sales lift comes from Radio spending, regardless of how confident you are in that standalone impact).  There's nowhere else in the model for that less confident value to be reflected._x0007__x0007_I believe this is why it says in the textbook that \""leaving them out tends to alter the meaning of the interaction\"".""",1,0,0
"I have installed the ISLR library in R, but I still do not know how to access the Auto-data set.",0,1,1
"Moreover, I have trouble in importing the Auto-data set and would be grateful if I can get some help. I have tried the following things:_x0007__x0007_1. Importing by putting in the http-adress for the Auto-data set within brackets:_x0007__x0007_Auto=read.csv(\http:...\"")_x0007__x0007_2. Or copying and pasting in to a text editor and then opening in Excel and saving it as a CSV-file._x0007__x0007_In both cases R reads it in as a vector with one column and 397 cases, rather than as a dataframe. This is a very frustrating problem with R that I have often encountered, and I would be grateful for any help or suggestions how to solve it.""",0,1,1
"The null hypothesis is that $_x0008_eta = _x0008_eta_mathrm{null}$.  That is, the_x0007_unknown population parameter, $_x0008_eta$, is hypothesized to be some_x0007_specific value of interest, $_x0008_eta_mathrm{null}$._x0007__x0007__x0007__x0007__x0007_$_x0008_eta_mathrm{null}$ is sometimes equal to zero, because it's \of_x0007_interest\"" whether there's a relationship at all._x0007__x0007__x0007__x0007__x0007_But $_x0008_eta_mathrm{null}$ could easily be some other value of interest_x0007_(or benchmark)._x0007__x0007__x0007__x0007__x0007_You take a sample, and you know that if the null were true,_x0007_the sample statistic $b$ would be \""close\"" to the null value_x0007_$_x0008_eta_mathrm{null}$.  \""Close\"" means close in terms of the standard_x0007_error of $b$._x0007__x0007__x0007__x0007__x0007_Remember that $b$ is a random number --- if_x0007_you had a different sample, $b$ would be a different number._x0007_The standard error of $b$ is an estimate of how much the_x0007_statistic $b$ would vary between different samples._x0007__x0007__x0007__x0007__x0007_In practice you only have one sample, but understanding how_x0007_$b$ would vary between possible samples gives you an idea of _x0007_the precision of $b$._x0007__x0007__x0007__x0007__x0007_Anyway, if the null were true, it would be \""unlikely\"" that_x0007_the sample statistic $b$ would end up more than two standard_x0007_errors away from the null value $_x0008_eta_mathrm{null}$.  So if you find_x0007_that it *is* more than 2 standard errors away, you \""reject_x0007_the null\""._x0007__x0007__x0007__x0007__x0007_Notice that the \""two standard errors\"" is also how the_x0007_confidence interval is constructed._x0007__x0007__x0007__x0007__x0007_> So if b=1.0 or b=10, we would not be able to reject the_x0007_> null hypothesis that ?1=b._x0007__x0007__x0007__x0007__x0007_If $b=1.0$ you wouldn't be able to reject a null that_x0007_$_x0008_eta_mathrm{null}=1$ because your statistic $b$ is_x0007_exactly where you'd expect it to be if the null were true._x0007__x0007__x0007__x0007__x0007_If $b=10$, that seems like quite a distance ($10-1=9$) from_x0007_where you'd expect it to be if the null were true. But it_x0007_depends how large the standard error is.  _x0007__x0007__x0007__x0007__x0007_If the standard error is large, then that distance will be_x0007_less than 2 standard errors away so you won't reject the_x0007_null._x0007__x0007__x0007__x0007__x0007_If the standard error is small, then that distance will be_x0007_more than 2 standard errors away so you will reject the_x0007_null.""",1,0,0
"Hi,_x0007_Can you tell me how to create simulation data in R? the following ( in the paper: Penalized clas si?cation using Fisher’s linear discriminant)_x0007__x0007_Four simulations were considered. In each simulation, thereare1200 observations, equally split between the classes. Of these 1200 observations, 100 belong to the training set, 100 belong to the validation set, and 1000 are in the test set. Each simulation consists of measurements on 500 features, of which 100 differ between classes._x0007_(a) Simulation 1: mean shift with independent features—there are four classes. If observation i is in class k, then xi ?N(µk,I), where µ1j = 0:7 if j=1,..,25, µ2j=0.7 if j=26...50, µ3j = 0.7 if j=51,...,75,and µ4j=0.7 if j=75,...,100 and µkj=0 otherwise._x0007_(b) Simulation 2: mean shift with dependent features—there are two classes. For i_x0007_?C1,xi? N(0,?) and, for i?C2,xi?N(µ,?), and µj=0.6 if j=1,...,200 and µj=0 otherwise. The covariance structure is block diagonal, with ?ve blocks each of dimension 100_x0007_×100.The blocks have (i,j) element 0.6^|i?j|. his covariance structure is intended to mimic gene expression data, in which genes are positively correlated within a pathway and independent between pathways._x0007_(c) Simulation3: one-dimensional mean shift within dependent features—there are four classes, and the features are independent .For i?Ck, Xij?N{(k?1)/3,1} if j<=100, and Xij ? N(0,1) otherwise. Note that a one-dimensional projection of the data fully captures the class structure._x0007_(d) Simulation 4:mean shift within dependent features and no linear ordering—there are four classes. If observation i is in class k,then xi_x0007_?N(µk,I). The mean vectors are de?ned as follows: µ1j_x0007_?N(0,0.3^2) if 1<=j<=25 and µ1j = 0 otherwise, µ2j?N(0,0.3^2) if 26<=j<=50and µ2j_x0007_=0 otherwise, µ3j?N(0,0.3^2) if 51<=j<=75 and µ3j=0 otherwise, and µ4j? N(0,0.3^2) if 75<=j<=100 and µ4j =0 otherwise._x0007__x0007_thank you,",1,1,1
"@Ed53_x0007_I was also baffled by this question, as it appears to be un  related to the topics covered by the lecture. I would gladly try to solve it if you could explain the \intuitive dimension\"" approach in more detail. Thanks""",1,0,1
yes this should be a real relaxed class that I will love to learn about,1,0,1
How this file (ch2.R)can be opened?,1,1,1
"Interesting question._x0007__x0007__x0007__x0007__x0007_So you're thinking rather than doing this:_x0007__x0007__x0007__x0007__x0007_> $ mathtt{medv} = _x0008_eta_0 + _x0008_eta_mathtt{lstat} mathtt{lstat} + _x0008_eta_mathtt{age} mathtt{age} + _x0008_eta_mathtt{int} mathtt{lstat}	imesmathtt{age} $_x0007__x0007__x0007__x0007__x0007_doing this instead:_x0007__x0007__x0007__x0007__x0007_> $ mathtt{medv} = _x0008_eta_0 + _x0008_eta_mathtt{lstat} mathtt{lstat} + _x0008_eta_mathtt{age} mathtt{age} + _x0008_eta_mathtt{???} mathtt{lstat}/mathtt{age} $_x0007__x0007__x0007__x0007__x0007_If you group the $mathtt{lstat}$ stuff together_x0007__x0007__x0007__x0007__x0007_> $ mathtt{medv} = _x0008_eta_0 + (_x0008_eta_mathtt{lstat} + _x0008_eta_mathtt{???}/mathtt{age}) mathtt{lstat} + _x0008_eta_mathtt{age} mathtt{age} $_x0007__x0007__x0007__x0007__x0007_you can see that, with $mathtt{age}$ fixed, there is a linear relationship with $mathtt{lstat}$.  The actual \sensitivity\"" depends on_x0007_what particular value $mathtt{age}$ is fixed at.  (which is the sort of thing you expect in a regular interaction)._x0007__x0007__x0007__x0007__x0007_Now grouping the $mathtt{age}$ stuff together_x0007_> $ mathtt{medv} = _x0008_eta_0 + _x0008_eta_mathtt{lstat} mathtt{lstat} + (_x0008_eta_mathtt{age} mathtt{age} + _x0008_eta_mathtt{???} mathtt{lstat}/mathtt{age}) $_x0007__x0007__x0007__x0007__x0007_it's clear that, with $mathtt{lstat}$ fixed, there is a *non-linear* relationship with $mathtt{age}$.  _x0007_And this non-linear relationship has $mathtt{lstat}$ mixed into it as well._x0007__x0007__x0007__x0007__x0007_It's a strange sort of thing.  It's like an interaction with_x0007_a transformed (by $1/x$) variable, but with the main effect_x0007_untransformed which, apart from everything else, is arguably_x0007_a violation of the hierarchical principle (page 89 of the_x0007_textbook)._x0007__x0007__x0007__x0007__x0007_If, instead it was_x0007__x0007__x0007__x0007__x0007_> $ mathtt{medv} = _x0008_eta_0^prime + _x0008_eta_mathtt{lstat}^prime mathtt{lstat} + _x0008_eta_mathtt{age}^prime (1/mathtt{age}) + _x0008_eta_mathtt{int}^prime mathtt{lstat}imes(1/mathtt{age}) $     it would be a (different) model with $mathtt{age}$ transformed to ($1/mathtt{age}$) (you might think of this as $mathtt{newness}$) and a regular interaction.     (Not saying that the transformation would be recommended here, but sometimes such a transformation is useful.)     You could definitely fit the $ eta_mathtt{???}$ model with linear regression.  But it probably wouldn't be the best choice unless there was a very good reason for doing so.""      ",1,0,1
"hi, _x0007_i am from Hyderabad.",1,0,1
"I was curious how you capture interactions between more than two variables?_x0007__x0007_Also, it was not clear to me the term Beta3 in the Qualitative and Quantitative interaction model. Beta3 is the interaction term of the dummy variable and the income variable. Do you multiply the two together like in the previous example?",1,1,1
"In the textbook, Page 92(Page 107 in pdf) ,  a p-value for horsepower^2 is given. Then it is stated that with such a low p-value, it's good to include horsepower^2 in the model. Also shown in the textbook is that when we include horsepower^3,^4,^5 in the model, the fitted curve seems wiggly. _x0007__x0007_Thus, horsepower^3,^4,^5 is unnecessary._x0007_But I am wondering what's the p-value for horsepower^3,^4,^5 ? It's not given in the textbook. Can I judge the necessity of including horsepower^3,^4,^5  in terms of p-value ?",1,1,1
"Lets write the code and show it._x0007__x0007_    download.file(\http://www-bcf.usc.edu/~gareth/ISL/Credit.csv\""_x0007_                  , \""Credit.csv\"") #get the credit data from the site_x0007_    Credit=read.csv(\""Credit.csv\"") _x0007_    _x0007_    # code explicitly rather than trusting how R does it_x0007_    Credit$isAA = ifelse(Credit$Ethnicity == \""African American\"", 1, 0)_x0007_    Credit$isAS = ifelse(Credit$Ethnicity == \""Asian\"", 1, 0)_x0007_    Credit$isCA = ifelse(Credit$Ethnicity == \""Caucasian\"", 1, 0)_x0007_    attach(Credit)_x0007__x0007_    summary(lm(Limit~isAA+isAS)) #CA in default_x0007_    #              Estimate Std. Error t value Pr(>|t|)    _x0007_    # (Intercept)   4728.5      163.9  28.852   <2e-16 ***_x0007_    # isAA           153.1      284.3   0.539    0.590    _x0007_    # isAS          -120.6      281.5  -0.429    0.669    _x0007_    _x0007_    summary(lm(Limit~isAS+isCA)) #AA in default_x0007_    #              Estimate Std. Error t value Pr(>|t|)    _x0007_    # (Intercept)   4881.6      232.4  21.009   <2e-16 ***_x0007_    # isAS          -273.8      326.2  -0.839    0.402    _x0007_    # isCA          -153.1      284.3  -0.539    0.590    _x0007__x0007_Hope you see now why the p-values change.""",1,0,0
"This is a great way to advance along the learning curve.  I have always found that side issues like the use of \(\"" vs \""[\"" vs what not have to be mastered before getting to the meat of a system.  Code School is cool for other languages, too.""",1,0,1
"Are you sure it has not downloaded to a location of which you are not aware?  You might try searching your computer for \ISLR First Printing.pdf\"" or \""ISLR_First_Printing.pdf\"".  I was able to download with FireFox in the usual way.""",1,1,1
Release announcement for swirl 2.0 with some additional info:_x0007_http://simplystatistics.org/2014/01/28/swirl-2/,1,0,1
It is a text file. You can use any text editor (e.g. notepad) or you can open it in R by selecting File | Open Script and you will see a display of all the .R files in your default directory. If the file is not in that directory you will have to navigate to it.,1,0,1
"I just finished \Computing for Data Analysis\"" (Coursera) taught by Roger Peng. I thought the content was excellent. In order to get a firm grasp on the basics of R, I highly recommend week 1 of his lectures, which you can watch on YouTube:_x0007_https://www.youtube.com/playlist?list=PLjTlxb-wKvXNSDfcKPFH2gzHGyjpeCZmJ_x0007__x0007_Specifically, the topics of data types, subsetting, and vectorized operations are critical.""",1,0,1
"I'm new to this MOOC, but are there no homework assignments?_x0007_Have I missed them somehow?_x0007_Thanks",1,1,1
"Sally, for the LIFE of my, i cannot find the ISLR package....I am completely LOST?",1,1,1
"For some reason, I was not able to download the book when I used chrome. I tried using firefox after reading Phil's comment and it worked.",1,0,1
"Just explored more EC2 also provide some free trials, hopefully this will work!",1,0,1
"Hello All,_x0007__x0007_I am <nameRedac_<anon_screen_name_redacted>> Sr.Biostatistician at Baylor college of Medicine in Houston, TX._x0007_The class really interests me. I am glad that people with same interest get together and talk the subjects. Hope we can share our experiences here!",1,0,1
"You should probably be able to do this yourself, if not already certainly following section 3.R's video. As such I won't explain the code and what it does (watch the video and go through the R Lab in Chapter 3)_x0007__x0007_    library(\ISLR\"")_x0007_    data(Auto)_x0007_    ## fit model_x0007_    fit <- lm(mpg ~ poly(horsepower, 5), data = Auto)_x0007_    summary(fit)_x0007_    anova(fit)_x0007__x0007_The $p$-values to match the ones for the quadratic model are given by `summary(fit)`:_x0007__x0007_    R> summary(fit)_x0007_    _x0007_    Call:_x0007_    lm(formula = mpg ~ poly(horsepower, 5), data = Auto)_x0007_    _x0007_    Residuals:_x0007_        Min      1Q  Median      3Q     Max _x0007_    -15.433  -2.529  -0.293   2.175  15.973 _x0007_    _x0007_    Coefficients:_x0007_                         Estimate Std. Error t value Pr(>|t|)    _x0007_    (Intercept)            23.446      0.218  107.31   <2e-16 ***_x0007_    poly(horsepower, 5)1 -120.138      4.326  -27.77   <2e-16 ***_x0007_    poly(horsepower, 5)2   44.090      4.326   10.19   <2e-16 ***_x0007_    poly(horsepower, 5)3   -3.949      4.326   -0.91   0.3619    _x0007_    poly(horsepower, 5)4   -5.188      4.326   -1.20   0.2312    _x0007_    poly(horsepower, 5)5   13.272      4.326    3.07   0.0023 ** _x0007_    ---_x0007_    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1_x0007_    _x0007_    Residual standard error: 4.33 on 386 degrees of freedom_x0007_    Multiple R-squared:  0.697,Adjusted R-squared:  0.693      F-statistic:  177 on 5 and 386 DF,  p-value: <2e-16  and the overall fit can be found by looking at the ANOVA table via the `anova()` function:      R> anova(fit)     Analysis of Variance Table          Response: mpg                          Df Sum Sq Mean Sq F value Pr(>F)         poly(horsepower, 5)   5  16<phoneRedac>     177 <2e-16 ***     Residuals           <phoneRedac>      19                        ---     Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  In relation to your secondary question, compare the fits with `poly(horsepower, 2)` and `poly(horsepower, 5)` in terms of the regression sums of squares  or the residual sums of squares (`Sum Sq` in the ANOVA table). Then try a higher order polynomial, say `poly(horsepower, 10)`. Then think back to the lectures on bias and variance, noting that the higher the degree of polynomial fitted, the more complex the model.  At that point you should be able to answer the secondary question for yourself.  HTH""",1,0,1
"I don't think \all hope is lost\"", it is just that the interpretation of the coefficients in the case of multiple covariates that are correlated becomes more difficult. Hence in that figure, subsets regression and the LASSO are consider more interpretable than the OLS method, as these methods aim to remove some of the predictors (or reduce their effects) giving fewer \""things\"" to consider at once._x0007__x0007_There are ways to help one interpret the effect on $y$ of changes in the covariates $X$, such as partial plots, where you hold all bar 1 or 2 of the covariates at some value, vary the values of the 1 or 2 covariates not held fixed and draw the change in the response as you vary them. Of course, if there are correlations or interactions, the effect of the covariates you are varying can change with the values of the other covariates you held fixed. Hence you might repeat this exercise, say using the lower quartile, median and upper quartile values to fix covariates at._x0007__x0007_The point is not to give up and not interpret the meaning of the coefficient estimates, but to critically think through what they *actually* mean. Yes, this gets more difficult with many correlated or interacting predictors, but no-one said doing stats was easy or quick :-)""",1,0,1
"I have been using EC2 recently to process machine learning algorithms for a Kaggle competition. It takes a while to get the hang of, but is very handy and a good learning experience. I would check the pricing on the Amazon Web Services website, tho. Certain, configurations might be $4.00 an hour, but I have been using a m3.2xlarge instance (8 CPUs 30GiB memory) for at most $0.90 an hour. Less ($0.20ish an hour) if you use spot instances for the same box.",1,0,1
How do you unenroll from this course?,0,1,1
nevermind found it under the bashboard,1,0,1
"Oh, \strong relationship\"" is just the correlation, no one cares in this question about how accurately we determined this correlation. The wording of this question totally confused me in the context of the lecture...""",1,1,1
"<redacted> The courses uses \linear regression\"" and \""linear model\"" interchangeably. Also, re your point about not reading the course text, it *is* the course text and the video material can't possibly cover all of the things relevant to the course.",0,0,1
"In the lab for Chapter 3, we were first presented with this form for modeling a polynomial term:_x0007__x0007_    lm(medv~lstat+I(lstat^2), Boston)_x0007__x0007_This approach was described in the book as \cumbersome for higher order polynomials\"", and so it was suggested that we use the poly() function instead. Here's an example:_x0007__x0007_    lm(medv~poly(lstat, 3), Boston)_x0007__x0007_I assumed that this example was equivalent to:_x0007__x0007_    lm(medv~lstat+I(lstat^2)+I(lstat^3), Boston)_x0007__x0007_However, based upon my exploration in R, they don't appear to be the same. The RSE, R-squared, and F-statistic are identical, but the coefficients are wildly different._x0007__x0007_Here are my questions:_x0007__x0007_ 1. What is the difference between these two lines of code?_x0007_ 2. Which is the line of code that I \""should use\""? Meaning, under what scenarios is the poly() version useful, and under what scenarios is the other version useful?_x0007__x0007_Thanks! I read the poly() help file and played around with the function in R, but it only got me more confused...._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"Okay, I did some more research and found something! Here's my source:_x0007_https://www.inkling.com/read/r-cookbook-paul-teetor-1st/chapter-11/recipe-11-10_x0007__x0007_That led me to change my poly() line of code to this:_x0007__x0007_    lm(medv~poly(lstat, 3, raw=TRUE), Boston)_x0007__x0007_According to that source: \The raw=TRUE is necessary. Without it, the poly function computes orthogonal polynomials instead of simple polynomials.\""_x0007__x0007_When using raw=TRUE, it does indeed produce identical results._x0007__x0007_In summary, these two lines of code are identical:_x0007__x0007_    lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)_x0007_    lm(medv ~ poly(lstat, 3, raw = TRUE), data = Boston)_x0007__x0007_Now I'm wondering: why didn't the video or the book use raw=TRUE? Was that a mistake, or is that intentional?_x0007__x0007_Thanks!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"The `raw = TRUE` isn't necessary, unless you are trying to reproduce the coefficients shown in the slides/book. You've found that the fits are the same, however, the `poly()` version ()as shown in book) is numerically more stable as the polynomial terms are uncorrelated. The variables produced via `I(x^2)` and `I(x^3)` will be correlated with one another and with `x`. That can cause problems when fitting (though not in this case). You could also centre `x` (subtract `mean(x)` from all elements of `x`) to reduce the correlation in the polynomials terms._x0007__x0007_The `poly()` coefficients are more difficult to interpret, but then again, the correlations in the polynomials produced by in the `I()` version are difficult to interpret.",1,0,0
I really thought I understood what is going on but I am still puzzled. I thought the effect would be the gain of the slope from the fixed TV and radio values. This must not be the case as my answer is marked wrong for the effect of 1.0389 .,1,0,1
"With the theoretical model_x0007__x0007__x0007__x0007__x0007_$ Y = _x0008_eta_0 + _x0008_eta_1 X + epsilon $_x0007__x0007__x0007__x0007__x0007_the error term, $epsilon$, has variance $sigma^2$, _x0007_or equivalently standard deviation $sigma$._x0007__x0007__x0007__x0007__x0007_If you use regression to fit the model you get:_x0007__x0007__x0007__x0007__x0007_$ hat{Y} = hat_x0008_eta_0 + hat_x0008_eta_1 X $_x0007__x0007__x0007__x0007__x0007_If the fit is good, you should have _x0007_$hat{_x0008_eta_0}approx_x0008_eta_0$ and _x0007_$hat{_x0008_eta_1}approx_x0008_eta_1$. _x0007_That is, the fitted values of the parameters should be_x0007_close to the true values._x0007__x0007__x0007__x0007__x0007_So, although you don't know $epsilon$ exactly, you can_x0007_get an estimate by calculating the residual._x0007__x0007__x0007__x0007__x0007_$ mathrm{residual} = Y - hat{Y} $_x0007__x0007__x0007__x0007__x0007_Or, plugging in $hat{Y}$ from above_x0007__x0007__x0007__x0007__x0007_$ mathrm{residual} = Y - (hat_x0008_eta_0 + hat_x0008_eta_1 X )$_x0007__x0007__x0007__x0007__x0007_Note the similarity to the expression for the true error_x0007_(from rearranging the model equation at the top):_x0007__x0007__x0007__x0007__x0007_$ epsilon = Y - (_x0008_eta_0 + _x0008_eta_1 X) $_x0007__x0007__x0007__x0007__x0007_The residual standard error (RSE) on page 12/48 is the_x0007_standard deviation of the residuals, so it gives an estimate_x0007_of the true, but unknown, error standard deviation, $sigma$._x0007__x0007__x0007__x0007__x0007_You could use $mathrm{RSE}^2$ as an estimate of the_x0007_true, but unknown, error variance, $sigma^2$.",1,0,1
I can't find the advertising data. Does anyone know where it is or is it not provided?_x0007__x0007_Jim,1,1,1
The data are available the [course text book website](http://www-bcf.usc.edu/~gareth/ISL/data.html).,1,0,0
"????, ???????????? www.linkedin.com/in/mshokin/ - ????? ??? ?????? ?? ??????, ? R ?????? ?????? ???????, ??? ??? ???? ?????? ?????? ? ??????????... ???? ????? :)",1,0,1
I think the questions are defined a bit ambiguously. This question asks us to find a range of acceptable values for the slope. It does not actually specify if we use a one-sided interval or two-sided one. You would have to know that parameters of linear model follow a normal distribution.,1,0,1
"just to say that I had exactly the same problem. We are talking about linear regression, and this multiple choice test should have been phrased more carefully. But I see there is a lot of nitpicking in the questions, e.g. R3.2.2.",1,1,1
Thank you very much Sally!,1,0,1
"Slide 36 has the pic below and says that when TV or Radio expenditure is low that true sales are less than predicted.  However, assuming the 'true' sales are the red dots, then the dots are bunched ABOVE the prediction plane when those expenditures are low ... thus they are higher than predicted._x0007__x0007_Did I misunderstand?_x0007__x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>1<phoneRedac>8.png",1,1,1
"This regression is only considering one categorical variable, Ethnicity, with three values. \African American\"" is the reference value; \""Asian\"" and \""Caucasian\"" are shown in the table. The regression is computed with_x0007__x0007_`summary(lm(Balance ~ Ethnicity, data=Credit))`""",1,0,1
"I was concerned about this one too. I don't agree with the \correct\"" answer as I don't think that the fact that TV budget is measured in thousands of dollars has been taken into consideration. Anyone else wonder about this?""",1,1,1
"When either TV is low or Radio is low (and the other high), then the model over predicts. When the budget is split, then it under predicts.",1,0,1
"I just realized I should post the R code for the plot_x0007_above.  It's only a few lines. You should be able to cut-and-paste_x0007_the whole slab into R to get the plot._x0007__x0007_    advertising_file = \http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv\""_x0007__x0007_    Advertising = read.csv(advertising_file)_x0007_    attach(Advertising)_x0007_    _x0007_    mod1 = lm(Sales ~ Radio + TV , data=Advertising)_x0007_    resid1 = mod1$residuals_x0007__x0007_    colour = ifelse(resid1>=0, \""blue\"", \""red\"")_x0007_    size = abs(resid1)_x0007_    _x0007_    plot(Radio ~ TV, col = colour, cex=size)_x0007__x0007_Notes:_x0007__x0007_- If you have a copy of \""Advertising.csv\"" on you hard drive, adjust the first line to point to that._x0007_- If you're cutting and pasting, the first line should be just one line --- no line breaks.""",1,0,0
Awesome plot! would you mind posting the code to recreate something like this?,1,0,1
Thank You!,1,0,1
"By random chance (pun intended) I found an open text-book (on machine learning) here:_x0007_https://www.otexts.org/book/sfml_x0007__x0007_Seems to cover basically the same topics as our good teachers' book. I find it helpful to get a 2nd perspective on things, for \deeper\"" understanding._x0007__x0007_Best of luck to all,_x0007_<nameRedac_<anon_screen_name_redacted>> G""",1,0,1
"ight))^2}_x0007_end{align*}_x0007_Thus the minimum contributing term to the sum (0) occurs when $x_i = overline{x}$ and $y_i = overline{y}$ and so by definition the least squares line must pass through that point.""",1,0,1
That's very helpful. Thank you!,1,0,1
"Thanks to <redacted> for the comments. We usually have one set of data and we set the training data to train our model. We can have sets for test and validation. We use these techniques with \big data\"" (more than 1000 units) or data that have special characteristics such as Netflix data, phone movement data, etc.""",1,0,1
Audio is audible only when video is played online._x0007_Videos downloaded from youtube seem to have no audio.,0,1,1
I really thought I understood what is going on but I am still puzzled. I thought the effect would be the value of the slope from the fixed TV and radio values without the intercept. This must not be the case as my answer is marked wrong for the effect of 1.0389 . I don't understand what I am missing.,0,1,1
It could be you're making a trivial calculation error. I have got this question wrong as well and in my case it was a stupid mistake with getting the right units to multiply into the coefficients.,0,1,1
Thank you!  Very clear now.,1,0,1
"Thanks ProfHastie ... though I'm afraid I can't see that in picture above.  Must be the angle ... can't tell if the pic shows the top or bottom side of the plane.  I think it's the top ... and the positioning of the residuals relative to the plane seems to me as the opposite of what you're saying._x0007__x0007_I don't know what's causing the perceptual difficulty.  However, alhf's response and alternate graph made the point very well ... so I now understand the point slide 36 is trying to communicate.",1,0,1
"The first step in understanding this question is to make sure you have a crystal clear understanding of what the coefficients indiciate._x0007__x0007_For example, let's take B2, which is 0.0289. That means that for every increase of 1 radio advertising \unit\"", there will be a 0.0289 increase in sales \""units\"" (when in the presence of TV and the TV*radio interaction). As you can see in the book on page 16, 1 radio advertising \""unit\"" is equal to 1000 dollars in spending, and 1 sales \""units\"" is equal to selling 1000 of whatever they are selling (let's call them \""widgets\"")._x0007__x0007_Thus, a B2 of 0.0289 means that if you increasing your spending on radio by 1000 dollars, your sales will go up by 28.9 widgets. (Note that I'm ignoring the interaction effect for the moment, and also note that 28.9 is incorrect for a model that does not include the interaction effect.)_x0007__x0007_Now let's get to the sales formula, which can be rewritten as follows:_x0007__x0007_    sales = B0 + (B2 + B3*TV)*radio + B1*TV + e_x0007__x0007_We have rewritten it so that the coefficient of radio is modified to be a function of TV spending. In other words, the radio coefficient takes into account the interaction effect. Let's now focus on the radio coefficient:_x0007__x0007_    B2 + B3*TV_x0007__x0007_That means that if you increase your spending on radio by 1000 dollars, your sales will go up by 28.9 widgets, plus an additional 1.1 widgets for every unit spent on TV ads. And again, remember that 1 TV unit equals 1000 dollars in spending._x0007__x0007_Now let's get to the actual question: \""According to the model for sales vs TV interacted with radio, what is the effect of an additional 1 dollar of radio advertising if TV = <zipRedac>0 dollars?\""_x0007__x0007_If you understand what I have written above, this is a simple calculation. If you get confused about the units, I suggest that you pretend the question said \""what is the effect of an additional 1000 dollars of radio ads if TV = <zipRedac>0000 dollars\"" and calculate it that way, and then divide your answer by 1000._x0007__x0007_I'm sure there is a simpler way of explaining this, but this is how I walked through the problem in my head._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,0,0
"The equations to solve this are on slide 39 of the lecture slides PDF for this chapter/section. A couple of things to bear in mind;_x0007__x0007_ * You are asked to work on basis of $1 increase in radio spending; the equations on slide 39 are in terms of $1000 change. The coefficient values you need are in the table on slide 37, not the ones substituted into the equation on slide 39._x0007_ * Look closely at the equations; choose the right one for a change in *radio* advertising spend._x0007__x0007_Also, perhaps this is what you meant, but there are 3 \slopes\"" in the model (one each for the main effects of TV and Radio, and one for the interaction), plus the intercept. You can ignore the intercept for this question, but you will need one of the main effects \""slopes\"" and the interaction effect \""slope\"". Which of the main effects slopes you need is on slide 39.""",1,0,0
"Thanks so much for the response. Although I don't understand all of the concepts you've presented, I get the gist, and maybe I'll have a deeper understanding by the end of the course :)",1,0,1
gosh! b is the slope? I thought it was the intercept as in ax + b. _x0007_Very confusing question.,1,1,1
"How to create 3D hyperspace in R, which is like the ones in slides",1,1,1
Will we get Statement of Accomplishment after completing this course?,1,1,1
Read the Course Info page for that answer.,1,0,1
I understood the same like you. It was an error i think. Look at the PDF book. There is the right explanation.,1,0,1
"It is referring *in general* to multiple regression not just multiple regression models with interaction terms._x0007__x0007_Note, I presume you meant *multiple* when you wrote *multivariate*. A *multivariate* regression is a term usually reserved for a model with more than one response variable. *Multiple* regression is used for the case where there are multiple predictor variables.",1,0,1
"You can open them in any text editor, or download them and then open from within RStudio.",1,0,1
"I'm a student at a high school that allows access to the internet. However, YouTube is blocked. In the Stanford online course, Environment Physiology, the individual video lectures could be downloaded. Can you offer to download the different lectures as well as embedded code to YouTube? I'm sure there are also other people who are taking this course that have limited access to YouTube as well.",1,1,1
"Hello everyone._x0007__x0007_I am a newbie to R and RStudio._x0007__x0007_I have installed the package ISLR, but neither read.csv nor read.table are able to open the Auto data files._x0007__x0007_I get this:_x0007_> Auto=read.csv(\Auto.csv\"")_x0007__x0007_Error in file(file, \""rt\"") : cannot open the connection_x0007__x0007_In addition: Warning message:_x0007__x0007_In file(file, \""rt\"") :_x0007_  cannot open file 'Auto.csv': No such file or directory_x0007_> _x0007__x0007_What should I do to solve this?_x0007_Thank you""",1,1,1
"In the text and lecture the SE for the $_x0008_eta_1$ coefficient was given as $sigma^2/Sigma_{i=1}^n(x_i - _x0008_ar x)^2$. How does that formula change for $_x0008_eta_2 ... _x0008_eta_N$, given an N-dimensional model? Does it become $sigma^2/Sigma_{i=1}^n(x^j_i - _x0008_ar x^j)^2$ where $X^j = [x^j_1 x^j_2 ... x^j_n]$ is the data for the $j^{th}$ input variable?",1,1,1
"...QUESTION CONTINUED_x0007__x0007_I am using Ubuntu._x0007__x0007_I know how to set up my working directory._x0007__x0007_I have installed ISLR and executed command \library(ISLR)\"" without problems._x0007__x0007_I have searched all my hard drive and Auto.csv does not exist._x0007__x0007_Should the data files be downloaded from somewhere (apart from installing ISLR)?""",1,1,1
"If you typed_x0007__x0007_> library(ISLR)_x0007__x0007_and it worked OK (no error messages), then the \Auto\"" data set should be available._x0007__x0007_Try typing_x0007__x0007_> summary(Auto)_x0007__x0007_You don't need to type_x0007__x0007_> Auto=read.csv(\""Auto.csv\"")""",1,0,1
"I'm fascinated by R and its capabilities.  I am a newbie but I can follow along fairly well as I do the exercises or play with commands.  One challenge I am having is understanding how to organize my R activities so that I can create code and then retrieve it.  I can't figure out the basic organizing principles.  For example, I'd like to keep as separate \files\"" my work on each chapter and have draft \""files\"" and ultimately develop \""final\"" versions with comments that I can easily reference. _x0007__x0007_It appears that a \""Project\"" may be the R entity that I could use but I can't seem to create one that is unique and find myself back at some old version or work that I did a week ago. For example, why can't I click on the \""untitled\"" tab on my R script to name it.  I'm sure there are some simple concepts and approaches that I haven't found.  Any advice from R veterans?_x0007__x0007_Thanks""",1,1,1
"There have been multiple posts about this over the past week or so. And the Week 2 email update says that they are \working on it\"".""",1,0,1
"...just curious if there is a particular reason 'why' R uses A[1:5] instead of zero-based array indexing (i.e. A[0:5]) as seen in some other languages such as Python?_x0007__x0007_It took me a while to wrap my head around the latter, especially the 'up to but not including' aspect... but I thought it was considered more prevalent and/or more correct from a CS perspective?_x0007__x0007_Thanks,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Tried summary(Auto) and it works. Thanks_x0007__x0007_Still confused about where the data is, but i guess i can solve that part later.",1,1,1
That does not work for me either.,0,0,1
@JDLytle Thanks!,1,0,1
+1. I guess the question assumes more maths knowledge than I was expecting I needed to feel completely comfortable with this course... :),1,0,1
Perhaps the true Y is highly non-linear and cannot be fitted with a linear model satisfactorily? In that case the confidence interval of a linear model is not very usefull?,1,1,1
you can download Auto.csv data from http://www-bcf.usc.edu/~gareth/ISL/data.html,1,0,1
It is feasible to use n dummy variables to represent a qualitative predictor where n is the number of levels?_x0007__x0007_Would this change the baseline to be comparison where that predictor is not involved?,1,1,1
Is there any technique for finding significant interactions other than just trying them all? The number of possible interactions for $p$ predictors is $p 	imes (p-1)$ which grows quickly with $p$. The `Auto` data has 7 predictors so there are 42 possible interaction terms. This doesn't even count the possibility of interactions such as `horsepower^2*origin`. Trying each possible interaction seems unwieldy.,1,1,1
"I need clarification on the second question 3.1 R2. We have to select the true statements._x0007__x0007_One of the choices in the list is_x0007__x0007_If I perform a linear regression and get confidence interval from 0.4 to 0.5, then there is a 95% probability that the true parameter is between 0.4 and 0.5._x0007__x0007_It appears true to me. The answer seems to indicate that it is false. What am I missing? Thanks",1,1,1
If you click on HD (lower right) it might help.,1,0,1
"> It is feasible to use n dummy variables to represent a_x0007_> qualitative predictor where n is the number of levels?_x0007__x0007__x0007__x0007__x0007_Generally not._x0007__x0007__x0007__x0007__x0007_> Would this change the baseline to be comparison where that_x0007_> predictor is not involved?_x0007__x0007__x0007__x0007__x0007_If you had a baseline *and* a set of n dummy variables, the_x0007_model would not be able to be (uniquely) determined._x0007__x0007__x0007__x0007__x0007_----_x0007__x0007__x0007__x0007__x0007_You're referring to slide 32/48:_x0007__x0007__x0007__x0007__x0007_> There will always be one fewer dummy variable than the_x0007_> number of levels. The level with no dummy variable —_x0007_> African American in this example — is known as the_x0007_> baseline._x0007__x0007__x0007__x0007__x0007_Look at the top of that slide_x0007__x0007__x0007__x0007__x0007_$ y = _x0008_eta_0 + _x0008_eta_1 x_1 + _x0008_eta_2 x_2 + epsilon $_x0007__x0007__x0007__x0007__x0007_That shows just 2 dummy variables, _x0007_but this **already** covers **all** 3 levels:_x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007_- Asian:  _x0007_  $ y = _x0008_eta_0 + _x0008_eta_1 + epsilon $_x0007__x0007__x0007__x0007__x0007_- Caucasian:  _x0007_  $ y = _x0008_eta_0 + _x0008_eta_2 + epsilon $_x0007__x0007__x0007__x0007__x0007_- AA:  _x0007_  $ y = _x0008_eta_0 + epsilon $_x0007__x0007__x0007__x0007__x0007_Here, the baseline, $_x0008_eta_0$, represents \AA\""_x0007_and $_x0008_eta_1$ and $_x0008_eta_2$ represent the_x0007_differences from that baseline for \""Asian\"" and \""Caucasian\""._x0007__x0007__x0007__x0007__x0007_----_x0007__x0007__x0007__x0007__x0007_Imagine you added an extra dummy for \""AA\"". ($x_3 = 1$ if \""AA\"", else $0$)_x0007__x0007__x0007__x0007__x0007_Now you have_x0007__x0007__x0007__x0007__x0007_$ y = _x0008_eta_0 + _x0008_eta_1 x_1 + _x0008_eta_2 x_2 + _x0008_eta_3 x_3 + epsilon $_x0007__x0007__x0007__x0007__x0007_Which means:_x0007__x0007__x0007__x0007__x0007_- Asian:  _x0007_  $ y = _x0008_eta_0 + _x0008_eta_1 + epsilon $_x0007__x0007__x0007__x0007__x0007_- Caucasian:  _x0007_  $ y = _x0008_eta_0 + _x0008_eta_2 + epsilon $_x0007__x0007__x0007__x0007__x0007_- AA:  _x0007_  $ y = _x0008_eta_0 + _x0008_eta_3 + epsilon $_x0007__x0007__x0007__x0007__x0007_To fit, you need to find a baseline, $_x0008_eta_0$, and then_x0007_three differences from that baseline $_x0008_eta_1$, $_x0008_eta_2$,_x0007_$_x0008_eta_3$._x0007__x0007__x0007__x0007__x0007_There is no unique solution._x0007__x0007__x0007__x0007__x0007_You could pick **any** baseline you want and then measure_x0007_the 3 differences relative to that._x0007__x0007__x0007__x0007__x0007_If you try to feed this into a linear regression program, it_x0007_won't work._x0007__x0007__x0007__x0007__x0007_In effect, you've trying to find **4** unknowns ($_x0008_eta_0,_x0007__x0008_eta_1, _x0008_eta_2, _x0008_eta_3$) with only 3 \""equations\""._x0007__x0007__x0007__x0007__x0007_----_x0007__x0007__x0007__x0007__x0007_If you **removed $_x0008_eta_0$** it would be feasible.  _x0007__x0007__x0007__x0007__x0007_In effect, this would fix the \""baseline\"" at 0.  The other_x0007_dummy variables would then represent the differences of each_x0007_group from 0._x0007__x0007__x0007__x0007__x0007_Although you'd get a fit, the t-tests on the $_x0008_eta,$s_x0007_would probably be less interesting (testing a difference_x0007_from 0 rather than a difference of each non-baseline group_x0007_relative to the baseline group)._x0007__x0007__x0007__x0007__x0007_It would also mess up your F-statistic.  (Normally the_x0007_F-statistic could be used to test a null that all groups_x0007_have the same mean.  Here the null would become that all_x0007_groups have zero mean, which is probably not of interest.)_x0007__x0007__x0007__x0007__x0007_So in general this pr""",1,1,1
"Great find, thanks!",1,0,1
"I share jkwatson's frustration. Like him/her I have advanced degrees in a quantitative area (EE), and I've taken my share of Statistics courses. A little rusty, but that's easily fixed. _x0007__x0007_However, I feel the prerequisites (first courses in statistics, linear algebra, and computing) understate the knowledge required (and the currency of that knowledge), and this statement \This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics\"" really hinges on one's definition of 'formulas and complex mathematics'. Again, all easily fix-able by me with some review of my prior studies._x0007__x0007_I think the guidance of 3 hours/week is not realistic in my case. Between the lectures and reading the book I'm easily at 5-6 hours, and that's before any quizzes. I think people who are at 3 hours/week or less are already very familiar with the material being covered, and/or flying at a higher level than me._x0007__x0007_My real interest is in learning enough to apply some of the techniques taught in this class to real-world business situations (I like JMichael's suggestion). I'll stick with the course for as long as I think I can get some of that though I have to say after lecture series 3, I'm feeling the direction is too academic; we'll see if that changes.""",0,1,1
I would be really surprised if a linear regression model is anywhere in top 10.  If I had to guess a Bayesian classification would be more of a fit here.,1,0,1
"I think you need to modify your slide no. 16 in  \linear_regression.pdf\""._x0007_As a teacher you can make a simpler statement at beginning, but it should be such that it allows for a later, stricter, Mathematical definition. The concept of \""Null Hypothesis\"" is tricky and it was not stated clearly enough.""",0,1,1
Googled it. Not on the Stanford site.  It's at http://cran.r-project.org/web/packages/ISLR/index.html,1,0,1
Thank you both - very much appreciated.,1,0,1
The question should have been self-contained. the reference to the 1000 units is as obscure as it possibly could have been.,0,1,1
I downloaded one of the Sessions - for Ch 2 but I cannot use it.  I'm told there is no application compatible with it.  Please help.  I tried to use it in Chrome and/or AOL.  I use WIN 7 XP. and XP,0,1,1
"In R you could examine all the two way interactions in a 4 predictor model with:_x0007__x0007_`lm( Y ~ (X1+X2+X3+X4)^2 )` : interp of the \^\"" operator is for interactions_x0007__x0007_(It's NOT going to do any icky stepwise testing as is made so easy with SAS. So \""testing\"" is not made so easy. There are packages that will attempt to do so, but I'm allergic to such practices.)""",1,0,1
"For data munging you can use any tool of your choice. However, if you want to do serious stats the number and variety of packages in R are unbeatable.",1,0,1
I was testing some of our new found R knowledge and am working with a dataset where I would really like to be able to plot more than X~Y in a single plot. What package or command was being used for the plane plots we saw used as examples?_x0007__x0007_Note: if I understand correctly the lm command has no issue making a fit with such data we just have no way to visualize it,1,1,1
"> I was testing some of our new found R knowledge and am_x0007_> working with a dataset where I would really like to be able_x0007_> to plot more than X~Y in a single plot. What package or_x0007_> command was being used for the plane plots we saw used as_x0007_> examples?_x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007__x0007_[GavinSimpson](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/users/<zipRedac><zipRedac>99<zipRedac><zipRedac>)_x0007_gives the _x0007_[following suggestions](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/i4x-HumanitiesScience-Stats<zipRedac>16-course-Winter<zipRedac>014-course-material-feedback/threads/5<zipRedac>ea953<zipRedac>e561<zipRedac>f<phoneRedac>01d)_x0007__x0007__x0007__x0007__x0007_> If you mean the 3D hyperplane plot (on slide 36 of the_x0007_> lecture notes for section/Chapter 3), see ?persp. There is_x0007_> also wireframe() in the package lattice that comes with R._x0007__x0007__x0007__x0007__x0007_Also, check out_x0007_ [this discussion](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/i4x-HumanitiesScience-Stats<zipRedac>16-course-Winter<zipRedac>014-general/threads/5<zipRedac>e9b49<zipRedac>9defd1f1<phoneRedac><zipRedac>)_x0007_to see that even interpreting the 3d plot can sometimes be_x0007_difficult.  It contains an example of how you might_x0007_visualize what's going on by using <zipRedac>d techniques (code is_x0007_provided)._x0007__x0007__x0007__x0007__x0007_> Note: if I understand correctly the lm command has no issue_x0007_> making a fit with such data we just have no way to visualize_x0007_> it_x0007__x0007__x0007__x0007__x0007_Yes, that's right.  You can fit the model and end up with a_x0007_bunch of parameter estimates.  But you can't just do a scatter plot_x0007_against all the variables to \see how it looks\""._x0007__x0007__x0007__x0007__x0007_Even in the _x0007_[example above](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/i4x-HumanitiesScience-Stats<zipRedac>16-course-Winter<zipRedac>014-general/threads/5<zipRedac>e9b49<zipRedac>9defd1f1<phoneRedac><zipRedac>), _x0007_with \""only\"" 3 dimensions, it can be difficult to see what's_x0007_going on._x0007__x0007__x0007__x0007__x0007_With more than 3-dimensions, trying to fully visualize the_x0007_data space in this way is probably beyond the bounds of our_x0007_intuition._x0007__x0007__x0007__x0007__x0007_Sometimes a series of simple <zipRedac>d scatterplots is informative._x0007__x0007__x0007__x0007__x0007_But, in the case of _x0007_[this example](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/i4x-HumanitiesScience-Stats<zipRedac>16-course-Winter<zipRedac>014-general/threads/5<zipRedac>e9b49<zipRedac>""",1,1,0
"There's no such thing as a \project\"" in R. That's an RStudio term. When you create and save a <default> project, RStudio will save a workspace image that preserves the objects you had created in a directory with the project name. It also saves a history file of the commands you gave. When you use Open Project from the Project dropdown you can choose the .proj file that has the needed information to resume a session. \""Workspace images\"" and \""history files\"" are ordinary R concepts. Try `?save` and `?history` for their help pages. You are really learning two systems at the same time if you are using RStudio. Some systems (Windows and Macs) will hide the .Rdata and .history files from teh default file browsers. I run my Mac with \""invisible\"" (or \""dot\"") files made visible.""",1,1,1
We have two in the Seattle-area study group. Are there other students in the area that are interested in getting together to discuss and review the material?_x0007__x0007_My email address is included in a previous post.,1,0,1
Thank you for the contribution!,1,0,1
"what 'p' stands for, in lecture notes L1.p7?",1,1,1
"p represents the independent variable, e.g. age and n represent the data points or values. So age, year, education etc. are ps (the p vector) and the data points for each of them are ns (ns for age, ns for year, ns for edu)",1,0,1
"The sample determines the confidence interval, so it may contain the true parameter (in which case the probability to include  the true parameter is 1, with absolute certainty) or it may not include it (hence the probability is 0). However, in the long run, if you repeatedly take a different sample many many times, 95% of the confidence intervals will include the true parameter, as explained in the video.",1,0,0
"Guys & Gal + R Newbie;_x0007_Look like this class needs a RRRRrrrrrrrrr prerequisite... Else you will sink or swimm... Here is my advice:_x0007_- Google and google on RRrrr stuff when you in trouble. Lot of info out there._x0007_- Use the link below and view+study the video about rRRR _x0007_Computing for Data Analysis_x0007_by Roger D. Peng.. Great stuff and it's free_x0007_https://class.coursera.org/compdata-004_x0007_- Save the link above, download everything, and digest it._x0007_- I hope your brain capacity has enough space for this class, learning rrRR, work, family, NFL football, and private life.._x0007_- You have 24 hr/day & so much you can do. Don't loose sleep over it._x0007_- Learning is a long & labor intensive task.. I 'm not sure cramping everything in short time is good technique._x0007__x0007_Cheers!",1,0,1
"I wonder the same thing.... I did the exercises, and I would like to check it._x0007__x0007_Ping me if you get any news.",1,0,1
"You don't need to google a package and do the download-install process manually any more now that there is RStudio. Get RStudio, and in the install packages option, once you provide the package name, it will automatically connect to the corresponding mirror and in a few minutes, voila! The package is ready to use._x0007__x0007_#Happy #R-ing",1,0,1
The dataset Carseats is in the ISLR package._x0007_#Happy #R-ing,1,0,1
"If you have only have \apples\"" and \""oranges\"", may be you can fit two separate models. But if you have 50 different types of fruits, you don't wanna fit 50 different models. A much efficient solution would be to fit one model with all the 50 variables and infer from that which of the variables are important and which are not. Also, it may be that although all the 50 variables have some effect on the response, after the first 10, the rest 40 really contribute very little to our model. In situations like these, it would be helpful to know which subset of the predictors are the most useful. Hope that answers your question._x0007_#Happy #R-ing""",1,0,0
"In multiple regression, if you fit a different model for each value of one quantitative predictor, each model will have at least slightly different values for the coefficients of the other predictors as well. This is like including an interaction term for each predictor. If you don't think the quantitative variable interacts with the other predictors it is better to fit a single model.",1,0,0
Thanks so much for the advice.,1,0,1
"Thanks! But this is what I get after installing the package and running the code._x0007__x0007_> fix(Carseats)_x0007_> names(Carseats)_x0007_NULL_x0007_> Summary(Carseats)_x0007_Error in (function (classes, fdef, mtable)  : _x0007_  unable to find an inherited method for function ‘Summary’ for signature ‘\function\""’_x0007_Any help will be appreciated""",0,1,1
a small correction:_x0007_formula provided is 1/(1-R2)_x0007_ _x0007_thank you for the explanation.,1,0,1
"Restarting RStudio and making sure the workspace was cleared also fixed it for me, thanks for the tip.  R version 3.0.2, RStudio 0.98.497, Win7 64bit.",1,0,1
No. Look at the explanation. Sometimes there are more goals than simply prediction of the response.,1,0,1
The main case of the question is whether the sole goal is prediction or if there could be other goals.,1,0,1
A hypercube is just a cube in more than 3 dimensions.,1,0,1
"Also generalize the concept of volume. Volume of a line == length, Volume of a square = area, volume of a cube == volume, and so on.",1,0,1
Perfect.  Thank you SergeiP!,1,0,1
"Most normal people don't think of the first element of anything as being the zeroth. Seriously, R was designed so you didn't need to be schooled in the dark arts of FORTRAN or C programming just to be able to analyse data._x0007__x0007_The [R Language Definition](http://cran.r-project.org/doc/manuals/r-release/R-lang.html#Indexing-by-vectors) has this to say:_x0007__x0007_> A special case is the zero index, which has null effects: `x[0]` is an empty vector and otherwise including zeros among positive or negative indices has the same effect as if they were omitted.",1,0,1
"You do need to be careful with creating and presenting a 3D plot and if you are tempted to go the Excel-like route of drawing 3d bar charts etc; then yes, stop, don't do it. But 3d hyperplane/`persp()`-esque plots do have their uses.",1,0,1
Or even only 2 dimensions....,1,0,1
"Thank for this coure! It's very valuable and instructive curses. I will try to learn everything in this course, because this is the core of machine learning.",1,0,1
"Yes,_x0007__x0007_Go back to Introduction Page describing the course.  At  the bottom their is a link to the text.  Follow that and \Get\"" the book.  Your will be given a choice of Amazon or Springer.  I bought the book through Springer a couple of days ago.  Hope I receive it next week._x0007__x0007_Overall, I think the book is excellent and the price is reasonable given that it is a textbook.""",1,0,1
"Another bug, I've also noticed you can upvote your own posts.    _x0007_edit -- I upvoted my own post to be sure, now I can't undo it. Please don't count my upvote because I did it myself.",1,0,1
I have tried to buy the book also. I bought it but they don't have it at stock. I have to wait :-) and they couldn't say to me how much!,1,0,1
"@Kate: The way I see it, if the sample is biased, then you get a biased *estimate* (i.e., the number you end up with) of what you're trying to estimate , but that does not mean that your tool (i.e. your estimator) is biased. regarding estimators, the term",1,0,1
"search on discussions with \hypercube\"". i saw a lot of good answers with examples.""",1,0,1
"I bought a copy direct from Springer with a discount.  It took a while to come in the post, but worth having if you don't have access to a good quality printer for the PDF version.",1,0,1
"It says to do Outstate VERSUS Private. So you will do side-by-side boxplots, with a plot for each category in Private._x0007__x0007_boxplot(Outstate~Private,data=College)",1,0,0
Check out the resource mentioned in the following article:_x0007__x0007_http://blog.revolutionanalytics.com/2014/01/princetons-guide-to-linear-modeling-and-logistic-regression-with-r.html_x0007__x0007_The book is available as PDF or HTML and has a good intro on R.,1,0,1
"great hint. it took me for a while to figure out what the question want. Like Asier said, getting the answer is easy but to understand what it want is hell._x0007__x0007_Thanks all",0,0,1
"The question actually says to use the `plot()` function to create the boxplot, not the `boxplot()` function. But I think this is a typo in the text; using `plot()` to create a boxplot would be very difficult.",1,0,1
Or even 1 then 2 dimensions,1,0,1
"Hi,_x0007__x0007_any clue about why I get this and the package does not install? I am sure you have..._x0007__x0007_**package ‘ISLR’ is available as a source package but not as a binary**_x0007__x0007_Thanks!_x0007_<nameRedac_<anon_screen_name_redacted>>",0,1,0
"They are essentially the same. _x0007__x0007_The . says to fit all main effects. The Income:Age says to fit the interaction._x0007__x0007_Income*Age says to fit Income+Age+IncomeAge. But as the main effects (income, age) are already included with the . they aren't included again. _x0007__x0007_So the : is more appropriate than the * in this situation.",1,0,1
Thank you.,1,0,1
"Care to elaborate on your simulation? It does sound interesting for people trying to learn R, like myself. Thanks!",1,0,1
It's p(p-1)/2 .... still gets large,1,0,1
Thank you!,1,0,1
Thanks for the download links. The video files work fine.,1,0,1
Excellent! Thanks alhf :),1,0,1
"Thank you, got it. I think we are all understanding this the same way now, but coming from different vocabularies. For example, \biased\"" also has a technical meaning in statistical sampling: the difference in a given parameter between the true population value and the sample value that does not arise from chance but from a characteristic of the sampling method. Also just as a note, in public health we wouldn't use the phrase \""epidemiologically unbiased\"" because epidemiology has its own lexicon, and this meaning wouldn't fit. But I understand the usage in this context. Thanks again.""",1,0,1
"RE: \, I suggest that you pretend the question said \""what is the effect of an additional 1000 dollars of radio ads if TV = <zipRedac>0000 dollars\"" and calculate it that way\"" ... your B2 term would be underestimated by a factor of 1000, right? since it does not have TV term""",1,0,1
The question builds on the curse of dimensionality. This has been highlighted in the other thread.,1,0,1
"Hello!_x0007__x0007_I am a professor and evolutionary ecologist at Lund University (Sweden). I am mainly working with insects (dragonflies and damselflies) and my focus is sexual selection, speciation and morphological evolution, including the evolution of colour signals.",1,0,1
I was having the same issue. Thanks for the help!,1,0,1
This drawing is really helpful and I suggest to start just with this one and then focus on 3rd dimension. All you have to do then is to compare volume of your cube with the smaller one without that boundry ;),1,0,1
"FWIW, the `plot` function will easily make a box plot, too. It's a generic function that will do all sorts of things depending on the kind of inputs you give it.  Just type_x0007__x0007_    attach(College)_x0007_    plot(Outstate~Private)_x0007__x0007_or, alternately, you can use the `with` function:_x0007__x0007_    with(College,plot(Outstate~Private))_x0007__x0007_In either case, a box plot will be produced.",1,0,0
Thank you for providing the download links.,1,0,1
"Hello there, my name is <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>, <nameRedac_<anon_screen_name_redacted>>, from Caracas, Venezuela. I want to thank  all people behind this mooc too. Thanks you very much for your time and for all those resources you have made available for all us of  for free.",1,0,1
Small typo in 4.1 R2  It is page 3 of the notes.,1,0,1
"I would like to know whether it is possible to directly embed markdowns generated with RStudio to create/comment a Wiki page. I have already read the directions on the Wiki pages, but I havent found an explicit reference to it.",1,1,1
p stands for parameters,1,0,1
"Hej Erik!_x0007_You might have solved it already, but I'll still write :)_x0007_As <redacted> has written, one has to download the file from the book's website, and then follow instructions on the p.48: first File/Change dir. (choose the directory where the data file is saved) and then run the read.table command. Worked for me, although I also had problems at first, when I tried to load the data through \Open file\"" etc._x0007_Hälsningar från Moskva! :D""",1,0,0
"Haven't experimented with csv yet, so can't say anything about that",1,0,1
"In the example of what I was doing I was plotting a bunch of different variables vs the same Y and some of the plots turned out similar but I wanted a way to visualize that_x0007__x0007_IE_x0007_XP~Price, Generation, Level, #of Inputs_x0007__x0007_Maybe I could add multiple plots to the same plot in 2d using different colors?",1,1,1
"The title/description of the Figure 3.11 on p. 96 has this phrase at the end: \Right: The predictor has been log-transformed...\"".  According to the text, the right-hand plot is actually showing the result of log-transforming the response Y, not a predictor._x0007__x0007_Have I misunderstood?""",0,1,1
"Carseats with a capital C._x0007_> library(ISLR)_x0007__x0007_> names(carseats)_x0007__x0007_Error: object 'carseats' not found_x0007__x0007_> names(Carseats)_x0007_ [1] \Sales\""       \""CompPrice\""   \""Income\""      \""Advertising\"" \""Population\""  \""Price\""       \""ShelveLoc\""   \""Age\""         \""Education\""  _x0007_[10] \""Urban\""       \""US\""         _x0007_fix() is used for looking at the data. You can analyze the data without using fix().""",1,0,1
"I found that question reasonable and didactic, and the clue to solve it is contained in the videos. I think the easiest way to solve it is by growing in dimensions. 1 dimension is a line, so try to solve it for a line. Then, try to solve it for a plane, then for a cube... If you can solve it for these three, you'll find it can be generalised to n dimensions very easily._x0007__x0007_In my opinion, you need no more advanced mathematics than exponentiation, it's more a matter of logical reasoning, and maybe, a little bit of lateral thinking.",1,0,0
"You do need some mathematics skills for any stats course. It might not be \maths heavy\"" but that doesn't mean no maths. _x0007__x0007_The answer to this particular question does not involve any advanced technique. Think about a square and compare two areas, then a cube (volume), then go bigger. The process is the same._x0007__x0007_The prerequisites are \""First courses in statistics, linear algebra, and computing\"", so there is a bit of assumed knowledge - mathematical and statistical.""",1,0,0
"Well Ramesh, I'm with you. I hope someone can answer your question more fully as I had the same question.",1,0,1
"OK.. I Google about Hypercube.. Here what I found._x0007_![enter image description here][1]_x0007__x0007__x0007__x0007_A diagram showing how to create a tesseract from a point._x0007_0 – A point is a hypercube of dimension zero._x0007_1 – If one moves this point one unit length, it will sweep out a line segment, which is a unit hypercube of dimension one._x0007_2 – If one moves this line segment its length in a perpendicular direction from itself; it sweeps out a 2-dimensional square._x0007_3 – If one moves the square one unit length in the direction perpendicular to the plane it lies on, it will generate a 3-dimensional cube._x0007_4 – If one moves the cube one unit length into the fourth dimension, it generates a 4-dimensional unit hypercube (a unit tesseract)._x0007_This can be generalized to any number of dimensions. This process of sweeping out volumes can be formalized mathematically as a Minkowski sum: the d-dimensional hypercube is the Minkowski sum of d mutually perpendicular unit-length line segments, and is therefore an example of a zonotope._x0007_The 1-skeleton of a hypercube is a hypercube graph._x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>11.png",1,0,0
"The .R code is there for the examples, and pdfs containing the figures, but would it be possible to make available how exactly the figures were created? Would appreciate it!",1,1,1
"This is what I found...  Sirahman! Can you elaborate more about simulation_x0007__x0007_Thanks_x0007__x0007_6.3 Local Regression in IRp_x0007_Kernel smoothing and local regression generalize very naturally to two or_x0007_more dimensions. The Nadaraya–Watson kernel smoother fits a constant_x0007_locally with weights supplied by a p-dimensional kernel. Local linear regression_x0007_will fit a hyperplane locally in X, by weighted least squares, with_x0007_weights supplied by a p-dimensional kernel. It is simple to implement and_x0007_is generally preferred to the local constant fit for its superior performance_x0007_on the boundaries._x0007_Let b(X) be a vector of polynomial terms in X of maximum degree d._x0007_For example, with d = 1 and p = 2 we get b(X) = (1,X1,X2); with d = 2_x0007_we get b(X) = (1,X1,X2,X2_x0007_1 ,X2_x0007_2 ,X1X2); and trivially with d = 0 we get_x0007_b(X) = 1. At each x0 2 IRp solve_x0007_min_x0007__x000C_(x0)_x0007_XN_x0007_i=1_x0007_K_x0015_(x0, xi)(yi ? b(xi)T _x000C_(x0))2 (6.12)_x0007_to produce the fit ˆ f(x0) = b(x0)T ˆ _x000C_(x0). Typically the kernel will be a radial_x0007_function, such as the radial Epanechnikov or tri-cube kernel_x0007_K_x0015_(x0, x) = D_x0007__x0012__x0007_||x ? x0||_x0007__x0015__x0007__x0013__x0007_, (6.13)_x0007_where ||·|| is the Euclidean norm. Since the Euclidean norm depends on the_x0007_units in each coordinate, it makes most sense to standardize each predictor,_x0007_for example, to unit standard deviation, prior to smoothing._x0007_While boundary effects are a problem in one-dimensional smoothing,_x0007_they are a much bigger problem in two or higher dimensions, since the_x0007_fraction of points on the boundary is larger. In fact, one of the manifestations_x0007_of the curse of dimensionality is that the fraction of points close to the_x0007_boundary increases to one as the dimension grows. Directly modifying the_x0007_kernel to accommodate two-dimensional boundaries becomes very messy,_x0007_especially for irregular boundaries. Local polynomial regression seamlessly_x0007_performs boundary correction to the desired order in any dimensions. Figure_x0007_6.8 illustrates local linear regression on some measurements from an_x0007_astronomical study with an unusual predictor design (star-shaped). Here_x0007_the boundary is extremely irregular, and the fitted surface must also interpolate_x0007_over regions of increasing data sparsity as we approach the boundary._x0007_Local regression becomes less useful in dimensions much higher than two_x0007_or three. We have discussed in some detail the problems of dimensionality,_x0007_for example, in Chapter 2. It is impossible to simultaneously maintain_x0007_localness () low bias) and a sizable sample in the neighborhood () low variance) as the dimension increases, without the total samp",1,0,1
$y = f(x) + epsilon$_x0007__x0007_The first component of y is deterministic and the second component is random. True randomness cannot be predicted.,1,0,1
I did the same thing skipped it & moved onto next lecture.,1,0,1
"I do not understand how a linear method can caputer nonlinear effects. It is rather counterintuitive. I am sorry, I may have missed  it, but, it does not make sense to me.",1,1,1
"Convex optimization after a week included another tab \Video downloads\"", and now students can download videos to watch them off-line. It is pretty common practice among other MOOC to download the videos. Granted SU plaftorm is not that mature, but the other course proved it is possible._x0007__x0007_So please add video downloads feature (tab).""",1,0,1
"I am stagling with this as well. However, I think I know what that problems is here. For the 1st case, the uncertainty comes from the coin. In the 2nd case, the uncertainty comes from the observer's ingnorance. For the purposes of the study, the fact that the observer does not know is not randomness.",1,0,0
"No, it is the correct page._x0007_The value of 50% is a bit tricky!_x0007__x0007_Think about the features of the equation. _x0007_Good luck!",1,0,1
"I have a request for everyone who is able to grasp clearly the formulas we're arriving at in each section of a week. I can understand the concept but having little trouble getting the formulas. May be my math is rusty and needs some refresher. So..._x0007__x0007_Fellow students: What topics do I need to brush over again to get the formulas clearly. For example the formulas of slope and intercept at the end of 3.1. I know the slope intercept form itself, I'm talking about the sigma notation._x0007__x0007_TA: While I'm at it, what other math topics do I need to have a solid background in to understand forthcoming material and the ISLR book. I would greatly appreciate any help and pointers of possible (I'm thinking Khan Academy)._x0007__x0007_And please don't advice at a higher level like Calculus or Linear Algebra or Pre-Calculus as covering the whole branch would take months. I only need refreshers so the specific topic name would greatly help. Thanks much in advance.",1,1,1
"Same here..._x0007_Do we need to do Section Below in the ISLR pdf ?_x0007_2.4 Exercises_x0007_Conceptual_x0007_if we do, how can we chk our result?_x0007_Applied Section pg 69/441_x0007_if we do, how can we chk our result?_x0007__x0007_Thanks",1,1,1
"Its been made clear that for interactive terms to enter the model we must include the base variables even if they are not significant._x0007__x0007_However in the following 2 cases what must we do?_x0007__x0007_[A] Where a quadratic or cubic term is significant should we necessarily include all the lower order terms even if they are not statistically significant?_x0007__x0007_[B] Suppose we introduce Log, Reciprocal or square root transformation, do we include the raw variable as well?_x0007__x0007_Many thanks in advance.",1,1,1
I have purchased a physical copy of the Elements of Statistical Learning. Somebody told me that there is a website where all the R scripts from that book can be found. But I could not find them._x0007__x0007_Does anybody know where I can find those. Note - This is the advanced book (Yellow hard cover) not the current course text._x0007__x0007_Thanks,1,1,1
That's definitely correct page. Actually the value of 50% makes your equation very easy to solve;),1,0,1
"A couple of queries about the wording in the in lecture 5.2, with_x0007_slide 8/40 \Maximum likelihood\"" in view, at about time 05:52._x0007__x0007_----_x0007__x0007_Professor Hastie says:_x0007__x0007_> So this is 0.005 per dollar, but it would be 5 per thousand dollars._x0007_> So slopes --- you have to take the units into account._x0007_> And so the z statistic which is a kind of standardized slope does_x0007_> that. _x0007__x0007__x0007_That's a really clear explanation of a very good point --- that it is_x0007_important to take the units into account when interpreting the value_x0007_of a slope. And taking units into account would also be important in_x0007_linear regression (last topic) as well in the logit model (being_x0007_discussed here)._x0007__x0007_But it doesn't seem correct to suggest that the *z-statistic* helps to_x0007_\""take the units into account\"".  It only takes the standard error into_x0007_account._x0007__x0007_Taking the units into account gives some idea of the \""practical\"" size_x0007_of the effect.  But the z-statistic only really helps to get an idea_x0007_of statistical significance._x0007__x0007_To consider a regression example (which might be easier to think about_x0007_than a logistic regression model.)_x0007__x0007_Suppose there was a list of car journeys, from which was done a_x0007_regression of \""distance travelled\"" on \""amount of fuel used\"".  Then the_x0007_slope would be a measure of fuel economy._x0007__x0007_Suppose the slope came out to be \""4\"".  Someone might look at that and think,_x0007_*that's terrible fuel economy, what is it --- a tank?*  But as_x0007_Professor Hastie points out, the slope doesn't really mean anything_x0007_without knowing the units.  If distance was in miles, but for some_x0007_reason fuel was measured in pints (instead of gallons), then a slope_x0007_of \""4\"" is not as uneconomical as it might have first appeared._x0007_(4 miles per gallon is not good, but 4 miles per pint is OK.)_x0007__x0007_The z-statistic will depend on the variability and the sample size and_x0007_would not be a way of judging if the fuel economy was good. For_x0007_example, the z-statistic would probably be bigger if the sample were a_x0007_bigger.  But this is not to do with the underlying fuel economy of the_x0007_car._x0007__x0007_----_x0007__x0007_Professor Hastie continues:_x0007__x0007_> And then if we look at the p-value we see that the chance that_x0007_> actually this balance slope is zero is very small. (< 0.0001)._x0007__x0007_That seems a bit close to saying that the p-value is the probability_x0007_that the slope is zero.  But according to strict interpretation that's_x0007_not what the p-value is. It's the probability of getting an estimate_x0007_that far from zero if the true slope were zero, which is not quite the_x0007_same thing.  (The correct interpretatio""",1,1,1
Here you go_x0007_http://statweb.stanford.edu/~tibs/ElemStatLearn/,1,0,1
Can anybody say where I can find this dataset to load it to R. I was looking in ISLR library but found nothing.,1,1,1
"Confidence interval derived in lecture 3 refers to statistical inference drawn out from the training data. That is, in the context of a given training data, the coefficients are estimated and a given confidence interval is calculated._x0007__x0007_Which are the practical uses, in the context of learning, of confidence intervals drawn over training data sets?_x0007__x0007_Or in a different way: what does that confidence interval (calculated over the training set) tell us about how well the model will perform over other data set (real data, other test data)?. _x0007__x0007_I have similar question about p-values. For example, if the model is overfitted, the p-value will probably be very small but performance (prediction accuracy) over real data can be poor.",1,1,1
"Please help.  Can somebody please tell me how to use the R software or where I can go to learn how to use it?  I downloaded it after someone (I cannot find the discussion post) gave me the link.  I downloaded it; however, nothing is there except one-page.  I have no idea what to do with it.  I finally downloaded the r session for ch2 and opened it in Microsoft Word.  However, I do not know what to do with the information in that session because I don't know how to use the software.  Please help if there is an instructor on this.  I totally am lost about this software.  Thank you.",0,1,1
if you transform the variable first - say by taking its logarithm or by squaring it - the equation might still be linear with respect to the transformed variable.,1,0,1
"There is a \download video\"" link under each video now.""",1,0,1
There appears to be a small arithmetic error to the declared result of Quiz question Ch 4.2. R1 ?? Could this be checked? Thanks.  <nameRedac_<anon_screen_name_redacted>>,0,1,1
"hello guys/gals,_x0007__x0007_i found some q&a based on ISLR textbook on this site: http://www.alsharif.info/#!iom530/c21o7_x0007__x0007_enjoy!",1,0,1
"ight) = _x0008_eta_0+_x0008_eta_1 X_x0007_end{equation}_x0007_This is very confusing as you appear to have taken the log to base 10 of the function.  Assuming you meant to take the natural log then the function should be written either as $log_e$ or using the more conventional $ln$ i.e. if you code verbatim as log or press log button on a calculator you will return the wrong answer. _x0007__x0007_If we take the example for the lectures and plugin a probability of defult of 0.586 we see that by using $log$ a balance of 1964 is obtained whereas the correct answer of 2000 requires use of $ln$""",1,0,1
"from what i understood both Y and X have been log transformed.. look at the label on top of the right graph and look at the range of the x-axis and compare it to the left graph's x-axis.._x0007__x0007_still, this method of logging things somehow reminds me of how a lady tries to cover up her acne.. the problem is still there but put some make up on and it would achieve some desirable result for both the self and her significant other.. it would be nice if someone could provide more insight into this aesthetic treatment and its effectiveness / limitations",1,1,1
"Usually when you see \No such file or directory\"" error, the problem is that the file is not in your working directory or not in the specified path, or the file names don't match. For a .csv file it is easiest to use read.csv..._x0007__x0007_**data <- read.csv('prfmdata.csv')** reads prfmdata.csv that is in my working directory_x0007_if you want to use read.table_x0007__x0007_**data <- read.table('prfmdata.csv',sep =',' ,header = T)**""",1,1,0
"http://statweb.stanford.edu/~tibs/ElemStatLearn/data.html_x0007__x0007_THIS IS DOCUMENTED BUT FAILS BECAUSE R CAN'T TRACK THROUGH 2 302 status codes and \301 Moved Permanently\"":_x0007__x0007_ - read.table(\""http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data\"",_x0007_   sep=\"",\"",head=T,row.names=1) _x0007_ - Error in file(file, \""rt\"") : cannot open the connection_x0007__x0007_THIS WORKED:_x0007__x0007_ - read.table(\""http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data\"", sep=\"",\"",head=T,row.names=1)_x0007__x0007_EVEN BETTER:_x0007__x0007_http://cran.r-project.org/web/packages/ElemStatLearn/ElemStatLearn.pdf_x0007__x0007_ - install.packages(\""ElemStatLearn\"")_x0007_ - help(package=\""ElemStatLearn\"")_x0007_ - library( package = ElemStatLearn )_x0007_ - str(SAheart)_x0007_ - summary(SAheart)_x0007_ - data(SAheart)""",0,1,1
"- credit <- read.csv(\http://www-bcf.usc.edu/~gareth/ISL/Credit.csv\"")_x0007_ - str(credit)""",1,1,1
"Dear Trevor and Rob,_x0007__x0007_In Ch 4 of your book, I have not find any information on how to deal with / interpret interaction effects in logit models. _x0007__x0007_Due to non-linearity of logit model the interaction coefficients cannot be interpreted in a same way as in linear models. When I started to study this question, I came across Norton, Wang, Ai (2004) paper (http://www.unc.edu/~enorton/NortonWangAi.pdf) that stated that:  _x0007__x0007_1. The interaction effect could be nonzero even if the interaction coefficient equal 0._x0007__x0007_2. The statistical significance of the interaction effects cannot be tested with a simple t test on the coefficient of the interaction term. Instead, the statistical significance of the entire cross derivative must be calculated._x0007__x0007_3. The interaction effect is conditional on the independent variables, unlike the interaction effect in linear models._x0007__x0007_4. Because there are two or more additive terms, each of which can be positive or negative, the interaction effect may have different signs for different values of covariates. Therefore, the sign of interaction coefficient does not necessarily indicate the sign of the interaction effect._x0007__x0007_Given all these aforementioned issues how to properly calculate correct interaction effects and standard errors for logit models?_x0007_Do you know whether there is command in R that can do this? _x0007__x0007_Although Norton does introduce a STATA command that can do these calculations in his paper, this command works only if you interact 2 variables, and I need to interact 3 or more variables in my research. Further, Norton's command works only if there are less than 1 million observations in a data set.  _x0007__x0007_I would be very thankful if you could answer my questions.",1,1,1
"The task is the question: \According to the balance vs ethnicity model, what is the predicted balance for an Asian in the data set?\"" Explain me, please, what is a balance. I don't remember that this part of the lecture gives the definition. And what is the predicted balance? (the definition).""",1,1,1
"Russell, what error are you seeing? Like Chris, I got the correct answer. Did you use the formula on the top of page 6 in the slides?",1,1,1
You can also check the *blogify()* function from the **slidify** package (line 26 in https://github.com/ramnathv/slidify/blob/master/R/slidify.R)._x0007__x0007_That R package uses knitr() for processing the .Rmd files into .md and .html _x0007__x0007_Check the docs at http://slidify.org/,1,0,1
"One problem with file.choose()...each time you run your program, you have to locate your data again...and a few days or few months later you may not remember your data file name or where it was stored. Also if your program saves a another file or chart, it will be saved in your current working directory, so you'll end up with data in one folder and output in another.",1,1,0
"MaryHM._x0007_It should be more clear in the registration that you must have R experience to take this course.  R is a programming language. If you are not familiar with the basic of programming it will be very difficult to follow this courses.  Although there are many places you can find help for R, you have to set aside additional time for learning it before you could actually tackle any exercise problems in the course._x0007_I found this site very kind to an absolute beginner.  http://tryr.codeschool.com/levels/1/challenges/13_x0007_Hope this helps._x0007_Good luck !",1,0,0
"Hi,_x0007_I've noticed that some have posts have Math equations in them.  Can someone tell me how this is done please?",0,1,1
"Without giving any answers away - there is a typo in the explanation of the first quiz question at the end of Lecture 4.3 (ie, 4.3.R1).  It looks like the wrong digit was duplicated._x0007__x0007_Probably not a huge deal because the \Answer\"" is correct and the grading system correctly accepts the right answer, but I was confused for a minute about where the answer in the \""Explanation\"" box came from.""",1,1,1
"I've had problems with a number of quiz questions due to poor and ambiguous wording. Some involve things that are not covered in the videos, and at least one involves something not even covered in the book! Scanning the forums shows that other people are having similar issues. Were these questions tested before the course started, or are we effectively the testers?",0,1,1
Some info here;_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/undefined/threads/52e<phoneRedac>cae376ff<zipRedac><zipRedac><zipRedac><zipRedac>19,1,0,1
Great resource._x0007_Thank you.,1,0,1
Thank you! Great plot!,1,0,1
"I found the logit regression presentation interesting and tried to code my own problem into R to do such a regression. Here is the command: _x0007__x0007_    logfit<-glm.fit(data1[1],data1[2],family=binomial())_x0007__x0007_which produces the following error:_x0007__x0007_    Error in family$linkfun(mustart) : _x0007_  REAL() can only be applied to a 'numeric', not a 'list'_x0007__x0007_The data went from Excel to a CSV file which I read into R (I'm new to R). Then I massaged it.  I'm having trouble getting my arms around frames and such. But I really think my data is numeric (in the sense that they are numbers even if they are being stored differently). I say that because I've inspected them and I've plotted as shown below. If you are interested, here's what the data represent. data1[1] is a vector of the relative returns of stocks (SP500) and bonds (Barclay Agg) over the previous 6 months.  data1[2] is  0 (bonds outperform stocks) or 1 (stocks outperform bonds) over the subsequent month (assuming I got the data right - as I said, I'm new to R).  Thanks for any help. _x0007_![enter image description here][1]_x0007__x0007_    _x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>38.jpeg",0,1,1
"I admit that I got a few questions wrong. But I get 5 attempts, which are plenty.",0,0,1
joined.,0,0,0
Treat logistic regression like a linear model._x0007__x0007_$log(odds) = _x0008_eta_0 + _x0008_eta_1 X_1 + _x0008_eta_2 X_2 + _x0008_eta_3 X_1 X_2 + epsilon$_x0007__x0007_$log(odds) = _x0008_eta_0 + (_x0008_eta_1 + _x0008_eta_3 X_2) X_1 + _x0008_eta_3 X_2 + epsilon$_x0007__x0007_Interpretation of the coefficients gets hairy when you convert log(odds) into probability.,0,0,0
Seems like a lot of clicking going on when they are speaking._x0007_Especially noticable in week 4.,1,1,1
yes exactly. the figures in the book are novel in themselves and it would be very helpful to see how they were produced.,1,0,1
"Are the answers to the Applied portion of the chapter exercises available? This is my first effort to learn a programming language and it would be really helpful to have the answers so that I could check my work or easily reference the material when I'm stuck, which is happening more often than I'd like.",1,1,1
"The lecture and text clearly prefer Discriminant analysis to a logistic regression when there are more than 2 classes (groupings).  If there are two, particularly a True/False sort or condition, is logistic as good?  Did I misunderstand something?",0,1,1
"It sounds like R may have used the list type for data1[1] and/or data1[2]. What command did you use to read in the .CSV file? I'm guessing you used `read.table` or `read.csv`. Check the help function on the command you used (i.e. `?read.csv` or `help(read.csv)` ) to make sure everything is specified correctly._x0007__x0007_You can use `class()` to tell you the data type. Also, `is.numeric()`, `is.character()`, `is.vector(`), `is.matrix()`, `is.data.frame(`) queries on each of those data types, returning True/False._x0007__x0007_You can change type using `as.numeric(`), `as.character()`, `as.vector()`, `as.matrix()`, or `as.data.frame()`._x0007__x0007_Data frames can contain a mix of data types. If your data is all numeric an array may make more sense._x0007__x0007_Finally, check out the Quick-R web site at http://www.statmethods.net/index.html It has a good reference for the basic R commands.",1,0,1
"> I am from Dublin and had started this facebook group for_x0007_> this course,_x0007__x0007_Firstly, best wishes to you for your studies._x0007__x0007_> to help us study together and help each other in learning_x0007_> the concepts._x0007__x0007_Wouldn't it be nice, though, if that could take place_x0007_somehow on the course website?  That might make it more_x0007_accessible to everyone.  Some people may not have access to_x0007_facebook (because of network restrictions --- either on a_x0007_regional level, or on the local network where they access_x0007_the course materials.)_x0007__x0007_It would be a pity if interesting comments by people such as_x0007_<redacted>(https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/users/<zipRedac>17<zipRedac>87)_x0007_were on an \outside\"" site which not everyone may access._x0007__x0007_And you have an MSc in Data Science yourself, so you would_x0007_probably also have interesting comments to share._x0007__x0007_I know there have been a few criticisms of the forum, and_x0007_maybe in some ways it's not ideal.  If so, it might be_x0007_helpful to the organisers of the course to know \""what's_x0007_missing\"" to help them improve the course._x0007__x0007_Also note that, in addition to this forum, there is the subject's_x0007_[Wiki for StatLearning](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/course_wiki)._x0007__x0007_Being a wiki, it allows a lot of flexibility and control in_x0007_the structures you can set up (like wikipedia).  So maybe it_x0007_would have some advantages compared to the forum.  _x0007__x0007_You could set up a main study group page, which then has_x0007_links to pages for various topics, which have further links_x0007_and cross-links you want.  (Please ask if you'd like any_x0007_help using the wiki.)_x0007__x0007_And you have the ability to write those nice equations. _x0007__x0007_If I may be permitted to make a final, perhaps slightly_x0007_ideological, observation:_x0007__x0007_Isn't it nicer to contribute by building up content on the_x0007_site of the institution (Stanford) that offered the free_x0007_course (with free textbook), than to contribute the content_x0007_to a site (facebook) which, when it comes down to it, is set_x0007_up to make money advertising stuff to you?  [*]_x0007__x0007_In any case, best wishes for your study._x0007__x0007_And also for the facebook group if you think it's the best_x0007_way to go.  If it is, maybe you could leave a note about_x0007_why.  (I imagine the course organisers or site designers_x0007_would find such feedback useful.)_x0007__x0007_----_x0007__x0007_[*] Based, no doubt, on some machine learning analysis of_x0007_the content everyone has entered ;-)""",1,0,0
"RStudio is a graphical interface for running R and it is optional. I prefer the command line in R so I don't use RStudio._x0007__x0007_Good suggestion here to use Google, as there are lots of resources out there.",1,0,1
Thanks... It' help a lot now I can check my work.. Let me know if I can help .._x0007__x0007_Thanks again,1,0,0
How about captions? Did you see captions? I did not see it. Plz let me know._x0007__x0007_Thanks,1,0,1
"Thank You , Working...???",1,0,1
It'll be easier to help you debug with more of your code.  Show everything from reading in the data to your error.  Include a call of str(data1) after it is loaded.,1,0,1
Use getwd() if need be!,1,0,1
"Hi,_x0007_Unfortunately, I could not understand these paragraphs of the pages 132 and 133 of the book. Can anyone explain them by giving some examples?   _x0007_ _x0007_![enter image description here][1]_x0007_![enter image description here][2]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>82.jpg_x0007_  [2]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>1<phoneRedac>6.jpg",1,1,1
"Hello,_x0007__x0007_No it does not. If you change the baseline, the estimated coefficient will change and you will get the same result. For example, if you estimate that some effect is positive if you take AA as a baseline and Caucasian as the other group, it will become negative if you switch the baseline to caucasian._x0007__x0007_So yes, the output of the regression and the interpretation of the coefficient will change but no, the result will not be different.",1,0,0
"If `data1` is a dataframe, then `data1[1]` is also a dataframe and `data1[,1]` is a vector. The call `glm.fit(x,y,..)` needs `x` to be a matrix (or vector) and `y` a vector. So for a dataframe your call should be_x0007__x0007_    glm.fit(data1[,1],data1[,2],family=binomial())_x0007__x0007_To see for yourself, run:_x0007__x0007_    data1 <- as.data.frame(matrix(c(1, 2, 4, 3, 1, 0, 0, 1), 4, 2))_x0007_    p1 <- glm.fit(data1[, 1], data1[, 2], family = binomial())_x0007_    print(p1$fitted.values)_x0007_    ## [1] 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac>_x0007_    _x0007_    p2 <- glm.fit(data1[1], data1[2], family = binomial())_x0007_    Error in family$linkfun(mustart) : _x0007_      REAL() can only be applied to a 'numeric', not a 'list'",1,0,1
"\That is a convention used in those applications (and many coding lanaguages) but it is not formal mathematics.\""_x0007__x0007_In formal mathematics decimal logarithms are not used at all. They are an anachronism from the time when people used tables of logarithms to multiply numbers and such.""",1,0,1
"I need some simple help with linear algebra. What does a diagonal covariance matrix look like? For example, let us say that I have 4 explanatory variables and 1 dependent variable that takes on 2 classes (1=default, 0 =non-default).",1,1,1
Thank you very much;),1,0,1
"There is a link to a github site from the Wiki Section of the Course Site_x0007__x0007_[Answers to Conceptual and Applied Exercises][1]_x0007__x0007_This seems to be the effort of a single person and is not something official._x0007__x0007_I hope you know quite a bit on statistical learning methods already, mastering new concepts and a new programming language seems to be tough to me, especially if it is your first._x0007__x0007_  [1]: https://github.com/asadoughi/stat-learning",1,0,1
Logistic regression copes with many classes as well. I have an impression that Discriminant is just an alternative.,1,0,1
"but anway, I'd like to learn/know how to weigh variables or the complete dataset by using a computed weight - does anyone know how to do this in R?",1,1,1
"Thanks for the suggestion, Holger.",1,0,1
"Hmmm...the x-axis on the chart contains the fitted values, which are simply the predicted response Y's given the training set X's.  And of course the residuals are 'leftovers' in the Y signal given the prediction._x0007__x0007_So I still think the figure description is wrong, but hey, it's not my book.",1,1,1
"ight) = 0$.)""",1,0,1
You can find a free pdf of the book on Professor Tibshirani's website:_x0007__x0007_http://statweb.stanford.edu/~tibs/ElemStatLearn/,1,0,1
"Nico, are you getting different coefficients from those on the slide?",1,0,1
"Actually the probability goes on increasing and increasing, but more and more slowly.  It never gets all the way up to 1, but gets closer and closer.",1,0,1
"Hi Iman,_x0007__x0007_Here's a concrete example:  take William's example, where _x0007__x0007_log(odds) = -0.1 + 0.5*X_x0007__x0007_When X goes from 0 to 1, the log(odds) goes from -0.1 to 0.4, and the probability goes from 0.48 to 0.60._x0007__x0007_When X goes from 10 to 11, the log(odds) goes from 4.9 to 5.4, and the probability goes from 0.993 to 0.996.  In fact there is \no room\"" for the probability to increase by 0.5, but it <nameRedac_<anon_screen_name_redacted>> still increase.  Hope this clarifies things.""",1,0,0
"A diagonal matrix $M$ has $M_{ij}=0$ unless $i=j$.  The entries $M_{ii}$ are called the \diagonal entries.\""_x0007__x0007_So, for example, $_x0008_egin{pmatrix}1 & 0 \\ 0 & 2end{pmatrix}$ is diagonal, but the matrix $_x0008_egin{pmatrix}1 & 0 \\ 2 & 2end{pmatrix}$ is not._x0007__x0007_A covariance matrix for some random vector $(X_1,ldots,X_d)$ is just a matrix whose $(i,j)$ entry contains the covariance of $X_i$ and $X_j$.""",1,0,1
"FWIW, quiz questions--particularly in MOOCs like this--don't always have to be mere tests of whether you've learning something or not. They can be teaching tools in and of themselves._x0007__x0007_I did a MOOC not too long ago where it was actually fairly common to have questions that involved things that weren't covered in the lectures or the book. But you'd reason your way through it, get it wrong several times, and eventually figure it out. In the process, you'd learn something, which is really the whole point in the first place.",1,0,1
"I appreciate that it may well be a commonplace translation in some areas.  From my scientific backgorund it is not the approach I would take, or would it likely be acceptable.  _x0007__x0007_The fact that it is a widespread programmatic convention for $log$ to be used for the natural log should not deviate from the necessity of clearly presenting information._x0007__x0007_I merely raised the issue in case others are confused by it...or use a calculator to derive solutions to the mathematical puzzles offere (as it happens it is invariant in the situation when $p(X)=0.5$ so the correct answer is still obtained).",1,0,1
Only one of the possible answers is correct._x0007__x0007_(It might be a worthwhile endeavor to go over the ones that *don't* throw an error to confirm that you understand why they would still work.),1,0,1
@asadoughi I just found this discussion and just forked the repo.  _x0007__x0007_Thanks! Contributing back to the community in this way will motivate me to do more of the exercises.,1,0,1
"I noticed that in the video, around the 4:19 mark - as well as page 8 in the slides - the formula for Maximum Likelihood contains an error on the formulation of the ML function. It says_x0007__x0007_$ell(_x0008_eta_{0}, _x0008_eta)$_x0007__x0007_which should of course be _x0007__x0007_$ell(_x0008_eta_{0}, _x0008_eta_1)$_x0007__x0007_It is correct in the book (fortunately).",1,1,1
"In section 3.2.2 of the book, on page 82 (right before section 3.3 starts), the following paragraph has been written on confidence and prediction intervals._x0007__x0007_*For example, given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in each city, the 95 % confidence interval is [10,985, 11,528]._x0007_..._x0007_Given that $100,000 is spent on TV advertising and $20,000 is spent on radio advertising in that city, the 95% prediction interval is [7,930, 14,580].*_x0007__x0007_While I now understand the difference between confidence and prediction intervals (using different sources from the web), I still cannot trace out the calculations in the above examples. For example, given that there were two variables in the model, what was the standard error that was used to calculate the CI? How was this standard error calculated? Similarly, what was the standard error used to define PI and how was it calculated?_x0007__x0007_Would much appreciate some help on this.",1,1,1
"Are the diagonal elements of a diagonal covariance matrix just the variance of each explanatory variable? In your example, $var(X_1) = 1$ and $var(X_2)=2$.",1,1,1
Thanks Kevin. Your explanation really helped me understand too.,1,0,0
"Thank you, that was the issue!",1,0,1
"Coefficients are the same, but with different precision. Turns out that was the issue, just rounding error!",1,0,1
Perfect - thank you.,1,0,1
yes,1,0,1
"In logistic regression, we have a framework that predicts the probability of an outcome as:_x0007__x0007_$p(X) = frac{e^{_x0008_eta_0 + _x0008_eta_1X}}{1+e^{_x0008_eta_0 + _x0008_eta_1X}} $_x0007__x0007_We then maximize the joint probability of all events, which is a product of predicted probabilities:_x0007__x0007_$prod_{Y_i=1}{p(x_i)}prod_{Y_i=0}{(1-p(x_i))}$._x0007__x0007_What is the benefit of doing this instead of minimizing a squared loss function as we do in linear regression:_x0007__x0007_$sum_{i=1}^n(p(x_i)-Y_i)^2$_x0007__x0007_I realize the likelihood function has a nice interpretation as the joint probability of all events, but the regression model minimizes mean squared error. In practice should we expect the same results from these approaches, and if not, then why is the first one better?",1,1,1
"Trevor first says \ldl\"" is a bad one, than at the end of the video that it's the good one. I'm confused.""",1,1,1
"I agree that the questions touch a wide range of purposes and perspectives: 2.2R2 (the hupercube) may throw you off both with its purely mathematical formulation and by the fact that it calls for a purely quantitative approach. 2.3R1 on the contrary calls for a quasi-philosophical approach for the extreme conditions of a system (or that's the way i like to view it)._x0007_In either case what is clear from the questions is that one needs to handle problem solving from all these perspectives/ be a purist working on numbers and at the same time be an \extrapolator\"" working without any._x0007__x0007_amm, yes that's not always comfortable""",1,0,1
"You could assume that the irreducible error is normal, estimate their parameters from the training error and calculate how that distribution affects the model prediction's variance._x0007__x0007_But it is not clear to me why that irreducible error is not included in the first confidence interval in the paragraph above. That c.i. is about a prediction, not a parameter, so I think it should include all the possible errors and unknowns._x0007__x0007_Please comment, I think I am missing something.",1,1,1
"Dear All,_x0007__x0007_I have a question:  what happened to the features \age\"" and \""indus\""?_x0007_(Apologies in advance for the length of this post.)_x0007__x0007_Professor Hastie demonstrated that they are not needed in the final_x0007_model.  The explanation, if I undserstood the lecture, is that their_x0007_effect was accounted for by the other features, rendering them redundant._x0007_I wondered how this relationship with the other features_x0007_would show up, aside from their high p-values in the linear model_x0007_summary.  Maybe in their **correlation** with the other features?_x0007_But this does not appear to be so, as the table below (perhaps)_x0007_demonstrates.  So here are my 2 questions:_x0007__x0007_  If the effect of \""age\"" and \""indus\"" are accounted for by other_x0007_  features, should their correlations with other features_x0007_  somehow reflect this?  (If not, then is **\""Why not?\""** a meaningful_x0007_  question?)_x0007__x0007_  Are there other statistical tests that would demonstrate the_x0007_  nature of the relationship between \""age\""/\""industry\"" and the_x0007_  the other features?_x0007__x0007_Tables of correlations among features are attached to this post._x0007__x0007_Thanks for your help._x0007__x0007_=================================================_x0007__x0007_Below is a table of the correlation of each feature with all_x0007_the other features, made with \""cor\"" (using the default \""pearson\""_x0007_metric).  Table 1 is truncated on the right because it's just_x0007_too big, but what's there gives some idea of what's going on._x0007_Table 2 sums the absolute values of all the correlations for_x0007_each feature, in order to compare the values.  While \""indus\""_x0007_is larger than the others, \""age\"" is right in the middle._x0007__x0007_    Table 1 Correlations among features of the Boston dataset_x0007_              crim  zn    indus chas  nox   rm    age   dis   _x0007_        crim  1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  _x0007_        zn   -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 _x0007_        ind   0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  _x0007_        chas -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 _x0007_        nox   0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  _x0007_        rm   -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 _x0007_        age   0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  _x0007_        dis  -0.4   0.7  -0.7  -0.1  -0.8   0.2  -0.7   1.00 _x0007_        rad   0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.50  _x0007_        tax   0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  _x0007_        ptrat 0.3  -0.4   0.4  -0.1   0.2  -0.4   0.3  -0.2   _x0007_        blk  -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 _x0007_        ls    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.5""",1,1,1
You can have a confidence interval for the value of a regression function at a particular point.  This is difference from a prediction interval (which also takes the irreducible error into account).,1,0,1
"Remember that a linear regression is spitting out an *estimate* of $E(y|X=x)$. Your estimate may change if you fit the same regression on a different sample data set. The confidence interval captures the variability of your estimate when you repeatedly sample, fit, and estimate._x0007__x0007_Remember that $E(y|X=x)$ is the average of y given X=x. Just because you can exactly predict $E(y|X=x)$, doesn't mean you can exactly predict y. Because of irreducible error.",1,0,0
Sorry about that rparr.  Try to only click submit once and wait.,1,0,1
Thanks alhf for your explanation!_x0007_It was very helpful!,1,0,0
"How do you resolve the additional message:_x0007_> install.packages(\ISLR\"")_x0007_Installing package(s) into ‘/Library/Frameworks/R.framework/Versions/2.15/Resources/library’_x0007_(as ‘lib’ is unspecified)_x0007__x0007_   package ‘ISLR’ is available as a source package but not as a binary_x0007__x0007_Warning in install.packages :_x0007_  package ‘ISLR’ is not available (for R version 2.15.3)""",1,1,1
"Other respondents in these threads have cleared up how to interpret what a 95% confidence interval actually means, but I agree that the lecturer states the interpretation that matched this test option twice in the video explanation.  Only at the end was the language corrected, and even there I would have said \about 95 of the intervals are likely to contain the true value\"".""",1,1,1
"\predict\"" is what is known as a \""generic function\"" in R, which means that it dispatches different methods depending upon the class of the object that is passed to the function._x0007__x0007_For example, if you pass a linear model to the predict function, it dispatches the method predict.lm to actually perform the prediction. Or if you pass a logistic regression model to the predict function, it dispatches the method predict.glm._x0007__x0007_If you run the following line of code in R, it will show you all of the methods for predict:_x0007__x0007_    methods(predict)_x0007__x0007_And then once you know the method's name, you can get help on that method._x0007__x0007_If the line of code above does not work for a particular function, you can also try:_x0007__x0007_    showMethods(\""insert_function_name_here\"")_x0007__x0007_The methods() call works for what are called \""S3 functions\"", whereas the showMethods() call works for \""S4 functions\""._x0007__x0007_-<nameRedac_<anon_screen_name_redacted>>""",1,0,0
"Not clear how to access \Review\"" (I did see tab for accessing Chapter 2 Quiz which contains 4 MC problems but not anything that other students seem to be referring to in posts._x0007__x0007_I am guessing that something useful is in the .R files._x0007__x0007_Are there specific questions someplace that require one to do some R-coding?_x0007__x0007__x0007_Basically, I am wondering where would a student (like myself) who is expecting to have to do some work in the course actually know \""where to go\"" on the site to access Course \""exercises\""? I have simply been unable to quickly locate something that should be obviously accessible._x0007__x0007_And no specific mention was discussed in the course intro._x0007__x0007__x0007_---------------------_x0007__x0007_Thanks again for any useful guidance""",1,1,1
I viewed video 4.1 on Categories and then went to take the quiz associated with the video. The question was on Case/Control Samples. I don't recall that being discussed in the video. Am I missing something?,1,1,1
So far I'm using 3 decimal places and got all correct.,1,0,1
"Can you say more about \RSS no longer corresponds to a reasonable likelihood\"" ?_x0007__x0007_To be clear, I'm not talking about regression, but rather minimizing a regression style MSE loss function rather than a product of joint probabilities. If you use equations (1) and (3) in my post above, the $p(x_i)$ estimates will still be bounded by zero and one._x0007__x0007_In passing, while regression does predict values outside of 0 and 1, this doesn't seem substantive, even though it is the first thing mentioned in every treatment of logistic regression. We can just treat values greater than one to mean one and less than zero to mean zero._x0007__x0007_It also seems logit has higher costs than linear regression in terms of bias due to omitted variables, e.g. [see link][1]. _x0007__x0007_While econometricians will debate the theory of either measure, my take on Statistical Learning is that there will be a preference for what works better in practice. Can you say that logit works better in practice than linear regression, or is the preference of data scientists for logit and econometricians for linear regression just a matter of path dependence?_x0007__x0007_  [1]: http://www.mostlyharmlesseconometrics.com/2012/07/probit-better-than-lpm/""",1,1,1
"I believe the question on Case/Control Samples (4.4.R1) goes with video 4.4, which covers Case/Control Samples._x0007__x0007_There are two questions for video 4.1, one on Qualitative Variables (4.1.R1) and the other on the better predictor of Default based on the plots in the notes (4.1.R2).",1,0,1
"I misunderstood the question. Confidence intervals and standard errors make some strong assumptions about the data and about the true model. I would say low p-values (or small standard errors) are desirable, but the ultimate test of how well a model fits the data is to calculate the MSE of a test data set.",1,0,1
"No. R does not know the true prior probabilities. You can manually calculate the offset, or you can specify observation weights in R with the weights argument.",1,0,0
Why aren't there any programming assignments for the lectures? They will really help ground the concepts and expose challenges working with real data.,1,1,1
"Thanks for the link, <redacted>. But are you sure the answers provided particularly for the Conceptual exercises are correct? I was working on one of the conceptual questions from Chapter 3 (Question 3), and I don't think the supplied answer is correct...",1,0,1
"Hi Eiji, _x0007__x0007_thanks for your prompt response. I actually did that before posting this._x0007__x0007_Is it a version incompatibility?_x0007__x0007_Thanks_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Thanks much for the pointers man. I'm aware of the sigma notation for addition but wasn't sure show the end formula was derived. You mentioned Sal demonstrating the process, I'll sure check that out. When I posted the question (2 days ago), I thought I'd actually dropout of the class, brush up math a little and come back when I'm able. I'm kinda OK with conditional P but will brush up on Bayes's rule. I'm ok with basic matrix manipulations as well. It's just been a long time since I did math (years ago), I guess the symbols and notations are getting me. Even the SE formula at 9.56 minutes in section 3.1, it's not a hard formula at all but I wasn't happy with myself in understanding how we arrived there._x0007__x0007_I'm brush up on some basic stats & probability first (at Khan Academy,)come back in a week or two and see if I feel more comfortable taking this class. I hope they archive it even when it's over if I don't make it back soon enough :D_x0007__x0007_But I really appreciate your response man, especially pointing to Sal's videos. I'm sure that'll help.",1,0,0
Thank you. What I meant video for download the same that are presented online -- currently the ones for download are so low quality that it is impossible to use them (any text is barely visible).,0,1,1
"Thanks so much for this terosv, after your explanation I answered it in 2 mins, rather than the four hours I was thinking it was going to take to read the text and figure out what was being asked!",1,0,0
I recommend working through the questions at the end of each chapter of the textbook (both Conceptual and Applied) to provide that challenge,1,0,1
The other way to prove this is to look at the two extreme cases:_x0007__x0007_If exp(x) = 0:_x0007_   p(x) = 0 / 1+0   = 0_x0007__x0007_If exp(x) tends towards infinity_x0007_   p(x) = big number / (1+ big number) = almost 1_x0007__x0007_All other values are somewhere between these two limits..._x0007_So p(x) is always in the range 0 to 1...,1,0,1
I don't think you will be able to answer all questions based on just watching the videos. You also need to read the book,1,0,1
THANKS...dyslexia strikes!  R,1,0,1
"I am also very interested in exploring this ;-) _x0007_I have been using weights for non response adjustment for my datasets, but in SPSS. Have not tried it in R. Planning to study about how to do it in R. Anything to share for now, JulianeA ?",1,0,1
"I dont understand. At first, we have two boundary regions. One is  0<=x1<=0.05 and another one is 0.95<=x1<=1, they are separated. Why were we asked about \What proportion of the volume of a hypercube of dimension 50 is in the boundary region\"" in single number? So, would we provide an answer for two regions of volume 0.1 or for one (0.05)?_x0007__x0007_The second question is - that ratio does not depend on the total number of dimensions since only one dimension is required to fit into the boundary, we do not care about others. So, why provided answer \""it is the set of all points that have at least one dimension in the most extreme 10% of possible values\"" = 0.100 is not correct?_x0007_Thanks for explanations in advance!""",1,1,1
How can I tell by looking at the slide whether or not $ _x0008_eta_3 $ has a positive or negative slope?_x0007__x0007_At first glance it looks positive given the positive correlation between Balance and Income but I am not certain.,1,1,1
"I am confused about the case-control sampling. For example, if we have <zipRedac>00 spam emails and <zipRedac>0000000 ham emails. If we use all the samples, it will take too much time to run it. A good way is to use all the spam emalis, and sample 500 emails (ratio of 5 to <zipRedac> )from the <zipRedac>0000000 ham emails(on slide <zipRedac>7).Then we can use these sampled emails to get the parameters, and correct the intercept with  pi=<zipRedac>00/<zipRedac>0000000. Am I right? for any dataset, will the ratio of 5 to <zipRedac> always work? Any ideas will be appreciated.",1,1,1
"To adjust by weighting, add a variable to your data set that takes the value p1/r1 in event observations, and the value (1-p1)/(1-r1) in nonevent observations, where p1 is the probability of an event in the population and r1 is the proportion of events in your data set. Specify this variable in the weight argument in glm().",1,0,0
Thanks William. Very useful information.,1,0,0
"Here's my take:_x0007__x0007_From Eq. (3.35) on page 90 of the book we have_x0007__x0007_if no student, slope is $_x0008_eta_1$ <br>_x0007_if student, slope is $_x0008_eta_1 + _x0008_eta_3$_x0007__x0007_Now look at the plot on the same page, the right pane. Clearly, the slope of the student line is less than the slope of the non-student line. You can see that the non-student line goes up, while the student line is kind of tilted towards the right side, wanting to touch the non-student line. So, we can say_x0007__x0007_$_x0008_eta_1 + _x0008_eta_3 < _x0008_eta_1$ <br>_x0007_$1 + _x0008_eta_3 < 1$ <br>_x0007_$_x0008_eta_3 < 1-1$ <br>_x0007_$_x0008_eta_3 < 0$_x0007__x0007_or, in other words, $_x0008_eta_3$ must is negative. _x0007__x0007_I hope it helps!",1,0,0
"In slide 16 of chapter 4, we get a very brief justification of using case/control sampling of ratios 5 and larger based on the variance of the estimator coefficients. _x0007__x0007_I tried to look for more details but couldnt find them in the book. Can someone point me where in the book or outside resources I can find more details?_x0007__x0007_Thanks!",1,1,1
"Here's my two cents:_x0007__x0007_From page 90 of the book we have <br>_x0007_if no student, slope is $_x0008_eta_1$ <br>_x0007_if student, slope is $_x0008_eta_1+_x0008_eta_3$ <br>_x0007__x0007_From the plot in the same page, on the right pane, we see that the slope of the non-student is less than the slope of the student. The non-student line wants to touch the student line as income increases. So we can say, <br>_x0007_$_x0008_eta_1+_x0008_eta_3<_x0008_eta_1$ <br>_x0007_$1+_x0008_eta_3<1$ <br>_x0007_$_x0008_eta_3<1-1$ <br>_x0007_$_x0008_eta_3<0$ _x0007__x0007_I hope it helps!",1,0,0
"Thanks, William, for the follow up._x0007__x0007_I totally agree with your 1st comment above, hence my original doubt. I'll rephrase it here for the benefit of other readers: how do training set confidence intervals help us estimate what it is really important, namely the model's MSE over a test data set?_x0007__x0007_My current understanding (in progress): bootstrap will allow us to estimate the MSE over a test data set (or a similar metric), with minimal assumptions._x0007__x0007_I'll review chapter 5 in the book and other sources with your hints in mind, and probably come back when bootstrap is discussed in the lectures._x0007__x0007_Thanks again, course content is good but having you online is better.",1,0,0
You can also open the data set up in R and fit the data to determine the coefficients,1,0,1
Hello: My name is <nameRedac_<anon_screen_name_redacted>> and I am a Systems Analyst with 'The Library of Congress' in Washington DC. I look at this course as an opportunity to learn from a great institution. I hope we all have a very rewarding experience over the next eight weeks.  Thank you.  <nameRedac_<anon_screen_name_redacted>>,1,0,1
The math in LDA and QDA confuse me too. I'm going to stick with logistic regression.,1,0,1
"The thing I'm confused about is using a sampled value for b (b=0.5), but then talking about \other values of b\""?  What is a better way to word the question to avoid this duplicate use of the variable b?""",1,1,1
"I don't know the equations, but you can calculate them using the predict() function in R._x0007__x0007_predict(..., interval=\confidence\"")_x0007_predict(..., interval=\""prediction\"")""",1,0,1
"Ohh, that's what you meant when you wrote above that you don't know the equation. I thought you were referring to the regression equation :p_x0007__x0007_Thanks for your inputs though :)_x0007_Will look into the R function.",1,0,1
How can we get some feedback on our efforts with the textbook's exercises?_x0007__x0007_Are there any solutions available?_x0007__x0007_Could we perhaps have dedicated discussion threads for end-of-chapter exercises?,1,1,1
"With my limited understanding, you are right. But, the function might not be 100% confident that X belongs to a particular category. So we might be interested in finding out the confidence levels with which the categorization was made.",1,0,1
Thank you William. At the point in the video this was mentioned i didnt make the connection between KNN and this statement but later on it became evident. You confirmed it for me. Appreciated.,1,0,0
"yep, I did not get a complete grasp of LDA through the videos alone. Today will try my luck by reading through the text book.",1,0,1
"Any advice on how to develop intuition regarding t-statistic, p-value, r-squared, and F-statistic? Yes, I understand that GYIF the polite form of STFW. No, I'm not looking for the equations. I took an undergrad stats course too many years ago and my recollection is mushy. In Section 3.2, we see that R^2 is 0.612 and F-statistic is 312.1. We are told, \this is quite impressive.\"" However, I lean more towards dumbfounded rather than impressed. There are so many Google hits that I'm uncertain about the best starting material. Advice?""",1,1,1
I am from Mumbai.. Currently in Santa Clara.,1,0,1
"$R^2$ measures the strength of your model's fit. The metric is bounded between 0 and 1. A $R^2 = 0$ means your model is no better than using the mean of y to make predictions. A $R^2 = 1$ means your model perfectly predicts y. Obviously, you want your model to have a high $R^2$. However, the $R^2$ in your training data set will often be overstated due to overfitting. So if you fit a model and see a $R^2 = 1$ be extremely skeptical. You should apply your model to an independent test data set to see if you get a similar $R^2$._x0007__x0007_$R^2 = frac{MSE_0 - MSE_f}{MSE_0}$_x0007__x0007_where $MSE_0$ is the mean squared error where your prediction for every observation is just the mean of y; $MSE_f$ is the mean squared error of using your model to predict every observation. Using this formula, you can interpret $R^2$ as the relative improvement in prediction by using a model rather than using just the mean of y.",1,0,0
"Have you tried a different browser?_x0007__x0007_I've had trouble with Firefox and vari-speed videos, for example, but Chrome seems to work fine._x0007__x0007_As far as downloading goes, you can try downloading the videos from YouTube (using any of a variety of third-party methods... I use MacTubes) rather than using the download link provided here. The resolution is noticeably higher, and it works fine in VLC at different speeds.",1,1,1
"It's bad. The second mention must have just been a slip on his part._x0007__x0007_A good way to remember:_x0007__x0007_ - LDL - \L\"" for \""Lethal\""_x0007_ - HDL - \""H\"" for \""Healthy\""_x0007__x0007_:-)""",0,0,1
"When the question only lets you select one answer, that means that (we think) there is only one right answer.  Some questions let you select multiple ones, in which case you should select all that apply.",1,0,1
Thanks for telling us!,1,0,1
"I wouldn't sum up the correlations: one correlation of 0.9 and another of -0.9 are both bad in ways that don't cancel out._x0007__x0007_In general, there's no simple rule about how much correlation is \too much.\""  It's just generally true that if X1 and X2 are highly correlated, it's hard to distinguish their effects from each other.""",1,0,1
"I understand the reasoning now - thank you._x0007_My answer to the question was along the lines given by Amir: $(x_0,y_0)$ passes thru the line $y = mx + c$ if and only if $y_0 = mx_0 + c$.  From (3.4), $(_x0008_ar{x},_x0008_ar{y})$ therefore lies on the regression line.  _x0007__x0007_I can now see why Chris's answer works - thank you again.",1,0,0
"Actually you'll get slightly different predictions if you use weights vs. offsets, but both can be considered \correct.\""  R definitely does not know by default that you used a case-control sample, so you need to manually make the correction.""",1,0,1
"I think you are thinking of K-means clustering._x0007__x0007_For K-nearest neighbors as described in the course, we are assuming all of the training data already come with known labels, so we don't need to classify them.",1,0,1
"Thanks,I have changed it",1,0,1
"In the text (pg. 69), RSE is roughly defined as \the average amount that the response will deviate from the true regression line.\"" But isn't the \""true regression\"" function unknown in non-generated data?_x0007__x0007_If \""the RSE is considered a measure of the lack of fit of the model to the data,\"" does this mean that the \""lack of fit\"" we observe in the RSE calculated for a regression model doesn't have a known relationship to the actual lack of fit we would expect to see if we knew the \""true\"" regression line? [e.g. if the data were generated from a known function] _x0007__x0007_If that's the case, why bring up the \""true\"" regression line in the first place?""",1,1,1
Open source solutions:_x0007_https://github.com/asadoughi/stat-learning_x0007__x0007_They are being updated/improved throughout the course._x0007__x0007_-<nameRedac_<anon_screen_name_redacted>>,1,0,1
"So far the questions seem to be of the style I remember from school.  Those \gotcha\"" questions.  They have little \""tricky\"" things that many people trip over the first time they are exposed to this stuff.  They don't seem to be focused on checking if you got the key material from the section.  I also don't find enough material for folks to really \""know\"" the correct answers from the lectures.  Again I appreciate the work of the professors  - just some suggestions for improvement.  Quiz on the stuff you really want students to take away from the class.""",0,1,1
It would be great if you could tell the length of the videos from the courseware page.  I can only tell how long they are by starting them.,1,1,1
"Is there a help file of some sort explaining how the quiz software works?  Sorry to be dense, but I can't figure out what it's doing.  Can I go to the end of a chapter and do all the quiz questions?  If I get an answer wrong, what should I do?  What does the given number of \submits\"" mean?  Do I have to \""Save\""?  What if I don't?""",1,1,1
I came across this description on a page from George Washington University's School of Medicine & Health Sciences: http://www.gwumc.edu/library/tutorials/studydesign101/casecontrols.html_x0007__x0007_This Wikipedia article also looks pretty good: http://en.wikipedia.org/wiki/Case-control_study,1,0,1
"Ok, I am just not getting this. I used the same approach as -nico-, rearranging terms to get the coefficient of `Radio` expressed in terms of `TV`:_x0007__x0007_B2 + B3*TV_x0007__x0007_= .0289 + .0011*50_x0007__x0007_So increasing `Radio` by 1 would increase `Sales` by this amount. But I submitted that answer and got it wrong. I also tried doing it in units of $1000 (as it's done on slide 39), but that was wrong, too. I gave up and burned all of my submissions so I could see the explanation, but \Show Answer\"" just showed the raw number with no explanation!_x0007__x0007_Could someone help explain what I am missing?""",1,1,1
"BAH, nevermind... typo in my calculation, I did .011 instead of .0011",1,0,1
"Hello_x0007__x0007_My name is <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>, I am from Costa Rica. I am interested in this course in order to apply stat learning concepts to data generated by automatic weather systems._x0007__x0007_Nice to interact on-line with you all!!!_x0007__x0007_Cheers!!!",1,0,1
"Hi Constanzo,_x0007_So in your scenario 1, how do you know that a particular behavior makes a person more likely to click an ad? Without the training set, you have no evidence that based on previous behaviors, the person will end up clicking your ad. Plus, you need that information in order to assign a label to your clusters.",1,0,1
sorry - solved. Was just a typo. - rather than ~ in plot,1,0,1
"Thank you , Chris!.  I wasn't aware that there is dataset for this model... will certainly check it out.",1,0,1
"Hi sallysue, _x0007__x0007_Thank you for your answer, it helps._x0007_May I ask this matrix has any connections with the covariance matrix in probability analysis?",1,1,1
"William, I installed the add-on as you suggested, and then downloaded the video again, no change, still blurry and unreadable. Here is how I download the video ... I right-click on the Download button just underneath the video in the lecture page where the video is embedded on the class website. I also just clicked on the Download button and use the Save option. Is there another method you're using to download the videos? I assume from what you said, they're in a different format that my Windows computer doesn't understand. Like I said, I'm using Real Player and Windows Media Player. I would have thought either program would be able to play the videos from the class. Any other thoughts?",1,1,1
"it seems like there is a few minutes of 'what is LDA and why is this important and why do i use LDA rather than other things, etc., that could usefully be added to the beginning of the LDA section???",1,1,1
"Hi all,_x0007_I have a concern about the significance when using a categorial variable._x0007_First, for example if we have a categorical variable with three levels.. then I understood that pvalues (except the intercept)are the result of testing this level against the first level (i.e the base line)... so then, what does the pvalue in the intercept mean?_x0007__x0007_Second, in the quizz is proposed the following:_x0007_According to the balance vs ethnicity model, what is the predicted balance for an Asian in the data set?_x0007__x0007_If the categorial variable is not significant, why do we have to sum beta0+betaAsian?_x0007_I thought that due to its non significant effect all etnias have the same value, i.e., beta 0_x0007__x0007_Thank you very much for your ideas and help",1,1,1
"Section 4.3 of the book (Logistic Regression) does not give guidance on criteria for selecting among logistic regression models.  If I have several models to choose from, which is best?  Is there something analogous to $R^2$ or $F$ from linear regression?_x0007__x0007_A related challenge is when what seems like my best logistic reg model has a mix of significant and insignificant terms.  Does the presence of insignificant terms render the whole model insignificant?  The dilemma is that removing the insignificant sometimes reduces the significance of the remaining terms._x0007__x0007_Any guidance for these issues?  Thanks",1,1,1
"Hola a todos._x0007__x0007__x0007_Ayer nos juntamos por primera vez para hablar de el curso. Aquí os pongo un resumen de las cosas de las que hablamos:_x0007__x0007_**Ejercicios de programación del curso.**_x0007__x0007_Los ejercicios que hay que hacer para pasar el curso son, sólo los cuestionarios que hay en cada tema. Por otro lado, al final de cada capítulo del libro del curso, hay una serie de ejercicios que es recomendable hacer._x0007__x0007_**Foro(s)**_x0007__x0007_Por el momento vamos a publicar la informacion del grupo de estudio de Madrid en este foro y en el foro del curso. Los temas dentro del foro del curso los titularemos \Madrid Study Group......\"" para que se puedan buscar facimente._x0007_Si vemos que alguno de los foros no se usa, pues dejaremos de emplearlo, aunque por ahora publicaremos lo mismo en los dos._x0007__x0007_**Materiales del curso**_x0007__x0007_Simplemente mencionar que los organizadores del curso han anunciado que los videos ys se pueden descargar para ver despues._x0007__x0007_**Cursos paralelos**_x0007__x0007_Hay un curso complementario al MOOC Statlearning de Stanford impartido por el profesor Al sharif, que sigue el mismo libro y que puede ser un buen complemento para el curso._x0007__x0007__x0007_**Concursos**_x0007__x0007_Finalmente, se habló de la posibilidad de hacer un grupo de trabajo en alguna de las siguientes plataformas, en las que se hacen concursos de analisis de datos. La idea sería aplicar las técnicas que se vean en el curso a casos reales._x0007__x0007_[Kaggle][1]_x0007__x0007_[Challenge Telecom Italia][2]_x0007__x0007_[Dream 8][3]_x0007__x0007__x0007_**Organización siguiente reunion**_x0007__x0007_La próxima reunion la celbraremos en 15 dias aproximadamente. Puedes votar el dia en [este doodle][4]_x0007__x0007__x0007_Saludos a todos_x0007__x0007__x0007_  [1]: http://www.kaggle.com/_x0007_  [2]: http://%20http://www.telecomitalia.com/tit/en/bigdatachallenge.html_x0007_  [3]: http://www.the-dream-project.org/category/challengesdream/dream8_x0007_  [4]: http://doodle.com/fp8vxwt3f2bwtc73""",1,0,1
The logit function is sigmoid. The unit change in Y is not constant over all values of x. Based on this how can the logit function be linear (see question 4.6)?,1,1,1
"Thank you Terosv, your explanation made me solve it too in no time.",1,0,0
log odds (or logit) is a linear combination of the explanatory variables. when you convert logit into probability you get a sigmoidal function.,1,0,1
"Acturally this is not a difficult problem. I found it tricky when I first saw it and did not think about it carefully. Just follow Arbiturka and other people's suggestion, you will solve it.",1,0,1
"I thought of a way to explain it to myself that I'll post in case it helps someone else.  Suppose person A is transmitting letters of the alphabet to person B from far away.  The equipment at each end is effected by extreme temperatures and/or extreme humidity causing voltage drifts.  Therefore sometimes the letter received by B differs from the true one transmitted by A.  Similar to morse code only in my example errors are normally distributed around the true letter.  In other words, if the true letter is \O\"", the most likely errors states are \""N\"" and \""P\"".  If B constructs 95% confidence intervals showing that the letter is between \""M\"" and \""Q\"", he CAN say that based on past experience he is 95% percent certain that he put his intervals on either side of the the true letter.  He CAN NOT say that the unknown letter has a 95% chance of being between \""M\"" and \""Q\"".  If the true letter, unknown to B, is \""O\"", \""O\"" is either between \""M\"" and \""Q\"" or it isn't.  \""O\"" does not change its position in the alphabet randomly.""",1,0,0
... and I'm not sure the answers are correct. I superficially checked with my answers for the Conceptual exercises and they seemed to be pretty similar._x0007__x0007_Whether this is an indication of correctness or falseness is unclear at the moment ;-).,1,0,1
Kevin -- many thanks for all the work.,1,0,1
"Case control sampling doesn't seem to be covered in the text, and I'm not sure I followed the discussion of how to adjust the logistic regression parameter estimates with case control sampling. Can anyone recommend a good place to learn a bit more about it than the discussion of lecture 4.4? It seems like a very powerful technique, but I'm not sure how to implement it.",1,1,1
I think that the criteria for selecting between models is how the model performs on out of sample data. The insignificant terms are likely to overfit the training data. So that will render the model less valuable against the out of sample.,1,0,1
"Are there other G+ users here? Anybody want to put together a G+ circle? _x0007__x0007_Last time I did so for a MOOC, I didn't get much help with the coursework, but found a lot of new interesting people in general!!",1,0,1
"In the videos, it is stated that the likelihood function can be expressed as follows:_x0007__x0007_L(B0,B1) = Prod_i:y=1(P(xi))*Prod_i:y=0(1-P(xi))_x0007__x0007_That represents the probability of seing a string of 0's & 1's IN THE PARTICULAR ORDER OF THE OBSERVATIONS IN THE DATASET. However, we should take into account every possible string containing exactly the same number of 0's & 1's. For example: If we have 2 successes in 4 observations, there are 4_C_2 = 6 combinations ({1,1,0,0};{1,0,1,0};{1,0,0,1};{0,1,1,0};{0,1,0,1};{0,0,1,1}). So, the likelihood function should be:_x0007__x0007_L(B0,B1) = obs_C_success*Prod_i:y=1(P(xi))*Prod_i:y=0(1-P(xi))",1,0,1
"This is **the** best explanation I've seen, by far! Thank you very much.",1,0,0
"Performance against test data is always a great option.  Since it applies to any model, why bother with $R^2$ or $F$ in linear regression?  Why not go straight to testing?_x0007__x0007_When I have many models to choose from, it would be nice to have a set of diagnostic criteria to identify which may be the best candidates to evaluate against test data._x0007__x0007_Also, if one model performs slightly better on one set of test data than another, how do I know how much of that difference is due to chance? If test data was plentiful, I could do multiple evaluations and see which model has the best long-run test performance._x0007__x0007_Since test data is sometimes difficult to obtain ... hence my motivation selection criteria of fitted models.",1,1,1
"ok, thank you.",1,0,1
"Ch. 5 will cover techniques to overcome \test data is not available\"". Specifically, cross-validation. However, this still does not answer the question \""how do I judge which of two logistic regression models is better\""? In linear regression, you just calculate the test set MSE and choose the model with the lowest test set MSE. There is no equivalent concept to MSE in logistic regression.""",1,1,1
What I dont understand is he sys to use a ratio of 5 to 1 but then the exampke is 30w to 116 which is ot 5 to 1. Whats the deal?,1,1,1
"To add: R by itself is a correlation(How good it fits the points/data) and R squared is how well it explains it_x0007__x0007_I would say R deals with the line itself(the shape, what it looks assuming linear etc), and R squared deals more with the predictive side of it(how well X predicts Y)",1,0,1
"Yes. _x0007__x0007_Practically, though, since that normalization is not dependent on the parameters of the probability p(x;Beta_0,Beta_1). Thus when you maximize the likelihood function as presented in the lectures/notes,  that factor would not discriminate.",1,0,1
It always is...,1,0,1
"Hi all,_x0007__x0007_It actually doesn't make sense to assess the significance of a baseline.  Think the model `height ~ gender`.  Let's say we have males coded as 0 and females as 1.  If gender is a significant predictor, that means that men and women have significantly different heights.  What would it even mean to ask if men is significant (hint: it doesn't mean anything)?_x0007__x0007_As for the bit about excluding variables that are not significant at some cutoff, that really depends on what the goal of your model is.  CV will often pick up variables that are NOT significant at the .05 level because they still help with prediction error.  Remember that p-values are a measure of $n$ in addition to measuring some intrinsic connection.  So, the reason you were still supposed to include the Asian coefficient is because it was part of the model in the question!  _x0007__x0007_p-values can also be tricky for categorical variables with multiple levels because you can have some of the dummy variables be significant and others not significant.  What might be a more appropriate way to asses whether a categorical variable should be included than the standard t-test (what is reported by R)?",1,1,1
"Great questions (and conversation)!_x0007__x0007_First, the deviance of a logistic regression model is very similar to RSS for many purposes.  The are analogues of the F-test using Chi-Sq distributions on deviance._x0007__x0007_The only sense in which an entire model can be tested for significance is by the Chi-Sq test for dropping all of the predictors.  Also, be careful about how you label those \insignificant\"" (I would say not significant) coefficients!  Just because a t-test says p<.05 does NOT mean that a variable is hurting your predictions._x0007__x0007_CV and resampling techniques to address model selection/hypothesis testing are popular because they require very few assumptions.  Logistic regression already has some peculiarities under misspecification (see our very own Will Fithian's research), so it is probably best to avoid doubling down too hard on assumptions that you don't need.""",1,0,1
For very simple linear models (one predictor) there is a direct correspondence._x0007__x0007_http://en.wikipedia.org/wiki/Simple_linear_regression has a nice summary.,1,0,1
"Under the Null hypothesis t-statistics are student-t distributed.  F-statistics are F distributed.  You can look up the distributions in a table (I imagine this isn't what you were looking for, but it is the answer).  The p-value is the probability of seeing that extreme a value of the statistic under the null (this is always what p-value means, it is applied to MANY types of statistics).",1,0,0
i had trouble with this too but then i figured out what the syntax of lm():_x0007__x0007_lm( response ~ term1 + term2 + term1:term2) _x0007__x0007_from http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.html_x0007__x0007_RTFM FTW!,1,0,1
"the binary (0/1) defines the dummy variable -- it's way of logically denoting inclusion or exclusion in a categorical group. for k groups, we create k-1 dummy variables, because kth group is indicated by the intercept and the error term, which results when k-1 dummy variables all have their values go to zero.",1,0,0
"I got the following error when try to install the ISLR package:_x0007_> install.packages(\ISLR\"")_x0007_Warning in install.packages :_x0007_  downloaded length 5696 != reported length 200_x0007_Warning in install.packages :_x0007_  downloaded length 5696 != reported length 200_x0007_Error in install.packages : Line starting '<html> ...' is malformed!_x0007__x0007_Has anyone seen this? From RStudio I try to use the \""Install Packages\"" menu to load the .zip file I downloaded from \""http://cran.r-project.org/web/packages/ISLR/index.html\"" using with source or binary option for windows, but got the same error._x0007__x0007_Appreciate any help. I use a 32-bit Win7.""",1,1,1
"I'm not sure what you're doing here, but if you unzip the ISLR.zip file into your working directory you will get a sub-directory called ISLR.  _x0007__x0007_Load the library in RStudio by typing \library(ISLR)\"" or \""require(ISLR)\"" and you should be good to go.""",1,0,1
"Hi Jack,_x0007__x0007_Thanks so much for this, I was struggling with the concept that a null hypothesis could be something other than B=0!  However, I did have to quickly switch out of your screen when I realised you were going to complete the entire problem....",1,0,0
"This question almost got me too. Page one in our textbook says, _x0007__x0007_*\Broadly speaking, supervised statistical learning involves building a statistical model for **predicting**, or **estimating**, an output based on one or more inputs.\""*_x0007__x0007_Notice that the keywords are predicting or estimating. I hope this helps.""",1,0,1
"Yes, there will likely be many predictors with large p-values. These_x0007_will be removed until the remaining predictors all  have small p-values _x0007_(< .10 or .05, say)",1,0,1
"Ravil, I think you are right and the Prof. made a slight error.  In the portion of the video you mentioned, he said it right the first time when talking about the TSS, identifying the \true\"" values (maybe \""measured\"" would have been better, who knows if they are correct?) and their mean, but then he points to the same area of the screen and appears to talk about the error of the predicted values.  I never even noticed that the first time through -- you have a keen eye.  Aren't videos great:  maybe we will learn to forgive, but we will never be able to forget...""",1,1,1
"1) Perhaps I was not able to recognize that professors talked about \middle\"" part. I've seen the question \""What proportion of the volume of a hypercube of dimension 50 is in the boundary region?\"" and we have three boundary regions inside the hypercube. How did you understand that authors talked explicitely about \""middle\"" region?_x0007__x0007_2) Well, you are talking about another case. Authors stated \""... that there exists a j for which...\"" not \""... for EACH j...\"" as you explained. The difference is studied on the first semester of mathematical analysis during introduction into multi dimension functions. If we will make a drawn analogue, we'll have:_x0007_![At least one j, as described by authors][1]_x0007_(At least one j, as described by authors)_x0007_and _x0007_![Each j, your case.][2]_x0007_(Each j, your case.)_x0007__x0007_So, i can agree with you in all your reasoning but this is another case. Since authors provided condition \""at least one\"", the ratio does not depend on the total dimensions. There is one dimension fit into the requirement on the first draw (x1), but as you can see three is no dependency for the volume ratio. Drawn examples could easily extrapolate onto 50d case._x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>4.png_x0007_  [2]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>1<phoneRedac>6.png""",1,1,1
"So far, pretty much all the exercises have been doable in either Python (w/ pandas and statsmodels) or pencil and paper. I only fired up R to answer the \what does R say when you type this\"" kind of question._x0007__x0007_(Nice to see you here too, Peter :)""",1,0,1
"The results of a logistic regression gave me some results I found surprising and I'm hoping someone can explain what happened so I can understand the technique better. The dependent variable was categorical representing whether stocks outperformed bonds over the next <zipRedac> months.  I threw in <zipRedac><zipRedac> independent variables. The results show <zipRedac> of those variables were significant. I then ran the regression again using only those five significant variables.    I was surprised that only <zipRedac> of the <zipRedac> were significant.  I would have thought all five would have remained significant (and become more so)._x0007_Below I show some of the code and the results from the regressions including one with the <zipRedac> significant variable. _x0007_     _x0007__x0007_>     glm.fit=glm(x$gblStkBnd<zipRedac>m~.,data=x,family=binomial) _x0007_      summary(glm.fit)_x0007_> _x0007_>     glm.fit=glm(x$gblStkBnd<zipRedac>m~SvB<zipRedac>+SvB<zipRedac>b+SvB<zipRedac>b+SvB<zipRedac><zipRedac>+SvB<zipRedac><zipRedac>,data=x,family=binomial)_x0007_>     summary(glm.fit)_x0007_> _x0007_>     glm.fit=glm(x$gblStkBnd<zipRedac>m~SvB<zipRedac>+SvB<zipRedac><zipRedac>+SvB<zipRedac><zipRedac>,data=x,family=binomial)_x0007_>     summary(glm.fit)_x0007__x0007_Results from Logistic Regression with <zipRedac><zipRedac> independent variables:_x0007_Call:_x0007_glm(formula = x$gblStkBnd<zipRedac>m ~ ., family = binomial, data = x)_x0007__x0007_Deviance Residuals: _x0007_    Min       <zipRedac>Q   Median       <zipRedac>Q      Max  _x0007_-<zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>  -<zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>  _x0007__x0007_Coefficients:_x0007_              Estimate Std. Error z value Pr(>|z|)    _x0007_(Intercept)  -<zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>  -<zipRedac>.<zipRedac><zipRedac><zipRedac>  <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>    _x0007_SvB<zipRedac>         -<zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>  -<zipRedac>.<zipRedac><zipRedac><zipRedac>  <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>    _x0007_SvB<zipRedac>          <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>   <zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>",1,1,1
"z-statistic measures whether a particular explanatory variable (X1), **given that the other variables (X2,X3,...,Xp) are in the model**, has an effect on the dependent variable. Your example demonstrates that variable selection is difficult.",1,0,1
Thank you for a very good explanation of concepts I have difficulty understanding.,1,0,0
This question is indeed a little bit tricky. _x0007__x0007_Think about what is random in each of the three statements & the definition and what is not.,1,0,1
Thanks a lot!! Was frustrated with the question. Your explanation helped me get the logic. Got it right finally!!:-),1,0,0
"I understand I need to set up the t-statistic with \beta 1 hat\"", \""b\"", and the SE for \""beta 1 hat\"" and solve for the 5% significance level as +/- 2.SE?""",1,1,1
"I'm assuming that the \save\"" feature is most useful for situations where a quiz has many questions (or a few very time-consuming questions)._x0007__x0007_You could answer a few questions, **save** your answers so far, then come back later to continue working on it. You'd then **submit** when you were satisfied with your answers._x0007__x0007_For the quizzes that are typical of this particular class, where you usually only have a question or two, the \""save\"" feature probably isn't necessary.""",1,0,1
Yup._x0007__x0007_Although you're looking for the *largest* value. So you really just have to deal with your slope estimate **plus** two standard errors.,1,0,1
Can D(y) be calculated on a test data set knowing only the model parameters from the training data set?,1,1,1
Thanks Terosv a great a simple explanation really helped me to visualize the problem,1,0,0
"In this case then, no answers should be market as incorrect. In order to have only one correct answer, line 17 should read: **Smarket$train = Year<2005**",1,1,1
Sounds like a great idea.  Add me and I'll add you back.  We go from there?_x0007__x0007_http://gplus.to/<nameRedac_<anon_screen_name_redacted>>,1,0,1
"I followed the Lab in the ISLR manual, and got the output below.  Is there a way to force correct row order so true predictions are on the main diagonal?_x0007__x0007_> table(glm.pred ,Direction )_x0007_        Direction_x0007_glm.pred Down  Up_x0007_     Up   457 507_x0007_    Down  145 141",1,1,1
An extension of the linear model is a linear model with non-linear and interaction terms._x0007__x0007_Linear Model_x0007__x0007_$y = _x0008_eta_0 + _x0008_eta_1X_1 + ... + _x0008_eta_pX_p + epsilon$_x0007__x0007_Extension of the Linear Model_x0007__x0007_$y = _x0008_eta_0 + _x0008_eta_1X_1 + _x0008_eta_2X_1^2 + _x0008_eta_3X_1^3+... + _x0008_eta_pX_jX_k + epsilon$,1,0,1
"One can use measure called the Gini coefficient to compare logistic regression models. Gini is a summary of ROC curve which says how your model separates \good\"" and \""bad\"" cases. Some statisticians use also the K-S statistics (maximal difference between CDF's of \""good\"" and \""bad\"" cases)._x0007_If you want to test weather a model is valid I recommend the Hosmer-Lemeshow test.""",1,0,1
"Susi & Anja_x0007__x0007_You might want to go through the discussion on this link, especially replies from <redacted> <redacted>. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/undefined/threads/52e3f583df<zipRedac>9ce4adf<zipRedac><zipRedac><zipRedac><zipRedac>2f _x0007__x0007_Ravi",1,0,1
Just a sanity check as I get the wrong answer : the formula should be X = (log(p(x)/(1-p(x))-ß0-ß2)/ß1 Would you concur ?,1,1,1
"Thanks for the advice. Actually after switching to another computer, this time the install.packages command works correctly. I'm new to R. Still not sure what happened earlier, anyhow glad it works now.",1,0,0
Thanks. I did exactly that earlier only from a different computer and using company VPN account. Not sure if that might be the problem.,1,0,1
"Right on. I'm +jake <nameRedac_<anon_screen_name_redacted>>. _x0007__x0007_For whatever reason, that link is 404'ing for me.",1,0,1
Add me all:_x0007__x0007_google.com/<redacted>,1,0,1
Is anyone else unable to plot lda objects? All I am getting is a blank device and a stopwatch._x0007__x0007__x0007_I went ahead and grabbed Rstudio to see if the plot would work in that environment. (I usually just call R from my terminal and work there)._x0007_plot(lda.fit) worked perfectly in the IDE._x0007__x0007_I don't know why it doesn't work from the command line.,1,1,1
Question _x0007_*Using the model on page 8 of the notes*_x0007_Where can I find these notes? What is it?,1,1,1
"It's on Page 8 of Ch4 classification pdf. It corresponds to \14 of 70\"" page of the pdf._x0007_If you cannot find it, I can tell you that the model has the intercept = -10.6513 and the slope = 0.0055.""",1,0,1
Very good suggestion thank you!,1,0,1
- k is a particular class in the K class of Y. Note difference between capital K an lower k_x0007_ - In multiclass logistic regression we have K-1 logistic regression. The expected result is probability for each k,1,0,1
"Hi,_x0007__x0007_I got the correct answer for question 4.2.R1 using the model on page 6 of the notes (that's $logit(p(X))=_x0008_eta_0+_x0008_eta_1 X$, not the formula on page 8 as the question states (the formula on page 8 is about Maximum Likelihood)._x0007__x0007_Is it just a typo in the question, or is there more than one way to get the result?_x0007__x0007_PS: when I say page 6, I mean the page labelled as \6/40\"" in the footer.""",1,1,1
"Any idea , is there any solution manual /answers to the conceptual questions in exercises of the book?",1,1,1
"It was very hard to understand for me that the required is the value of the volume of the (hyper)cube without the edge, I was thinking about the edge volume.",1,0,1
"Hi All_x0007__x0007_Please guide me, how to approach question R1 and R2._x0007__x0007_                     Coe_x000E_cient Std. Error   t-statistic  p-value_x0007_    Intercept             531.00        46.32   11.464     < 0:0001_x0007__x0007_    ethnicity[Asian]      -18.69        65.02   -0.287       0.7740_x0007__x0007_    ethnicity[Caucasian]  -12.50        56.68   -0.221       0.8260_x0007__x0007_AA is the baseline, Shouldn't there be the direct conclusion of R2 - _x0007_yi =  _x000C_B0+_x000C_1xi1+_x000C_2xi2+_x000F_ei _x0007__x0007_Which will reduce to the form - B0+ei where ei is the error & B0 is the intercept in the table._x0007_After using the formula ,I could reach to the answer ._x0007__x0007_Secondly,_x0007_Should there be a R programming required to solve?",1,1,1
I've been reading the more advanced course book Elements of Statistical Learning and have a question. SVD decomposes X into UDV (pg. 64). The fitted values of vector y are then UU'y (pg. 66). My confusion is that Wikipedia states (http://en.wikipedia.org/wiki/Singular_value_decomposition)_x0007_that U (and V) is unitary such that UU' is the identity  matrix. This would imply that UU'y is just y?_x0007__x0007_I came to this confusion while trying to show X inv(X'X) X'y=UU'y._x0007__x0007_Any clarification you could provide would be appreciated. Thanks.,1,1,1
Does anybody know how to pull SQL server 2012 data from R? thank you,1,1,1
"I think you are on the right track. You do not need to know any R programming to get this answer. You just need to know what the given coefficients mean and how they relate the regression formula._x0007__x0007_The \intercept\"" coefficient given in the table (531.0) is your B0 value. The other two coefficients under that are your B1 and B2 values._x0007__x0007_You are correct that, in the baseline case of AA, you don't have to worry about B1 or B2 (because their associated x values will be zero)._x0007__x0007_You do not know the error term (that's why it's a error term!), so you can ignore that part of the formula when calculating your answer.""",1,0,1
"Any model which is linear in the fitting parameters is a linear model, thus also stuff like_x0007__x0007_$Y = _x0008_eta_0 + _x0008_eta_1log(X_1) + _x0008_eta_2exp(X_2^2) + _x0008_eta_3Gamma(X_1^3sin(X_3))+epsilon$_x0007__x0007_but not the apparently simple_x0007__x0007_$Y = _x0008_eta_0+_x0008_eta_1sin(_x0008_eta_2X_1+_x0008_eta_3)+epsilon$",1,0,1
"I use my wife's ipad to look at the discussion board. The left panel \Show All Discussions\"" gets really annoying when I scroll through the topics.""",0,1,1
The videos of a Coursera course on an introduction to programming with R are on Youtube:  _x0007_http://www.youtube.com/watch?v=EiKxy5IecUw&list=PL7Tw2kQ2edvpNEGrU0cGKwmdDRKc5A6C4,1,0,1
"There is none. Somebody started some page in the Wiki, but no answers have been inserted so far.",0,0,1
"Why does logistic regression give an estimate of the **probability** that the response variable belong to a certain category? The fact that it takes values between 0 and 1 doesn't mean that it's a probability. In any case, what is the assumed distribution of the predictors?",1,1,1
And the lectures?!,1,0,1
"*From the dept of \too much free time,\"" I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;_x0007_    _x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 4.6.1 The Stock Market Data *****;_x0007_title '4.6.1 The Stock Market Data';_x0007__x0007_* csv file created from Smarket dataset (> write.csv(Smarket,file=\""Smarket.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_proc import out=Smarket dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=100;_x0007_run;_x0007__x0007_* > names(Smarket);_x0007_proc contents data=Smarket;_x0007_run;_x0007__x0007_* > summary(Smarket);_x0007_proc univariate data=Smarket;_x0007_run;_x0007__x0007_* > cor(Smarket[,9]);_x0007_proc corr data=Smarket;_x0007_run;_x0007__x0007_* > plot(Volume);_x0007_data Smarket;_x0007_   set Smarket;_x0007_   Index + 1;_x0007_run;_x0007__x0007_proc sgplot data=Smarket;  * note: by default, places a png file in current folder;_x0007_   scatter x=Index y=Volume;_x0007_run;_x0007__x0007_quit;""",1,0,1
"proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_* csv file created from Smarket dataset (> write.csv(Smarket,file=\Smarket.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_proc import out=Smarket dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookSmarket.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=100;_x0007_run;_x0007__x0007_* create test & training datasets;_x0007_data TrainingData Smarket2005;_x0007_   set Smarket;_x0007_   if Year < 2005 then output TrainingData;_x0007_     else output Smarket2005;_x0007_run;_x0007__x0007_***** 4.6.4 Quadratic Discriminant Analysis *****;_x0007_title '4.6.4 Quadratic Discriminant Analysis';_x0007__x0007_* > qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)_x0007_  > qda.class=predict(qda.fit,Smarket.2005)$class;_x0007_proc discrim data=TrainingData testdata=Smarket2005 method=normal pool=no simple testout=Preds5;_x0007_   * 'pool=no' performs QDA: 'QDA assumes that each class has its own covariance matrix'; _x0007_   title2 'Reduced Model: From Training Data Applied to Test Data';_x0007_   class Direction;_x0007_   priors proportional; _x0007_   var Lag1 Lag2;_x0007_run;_x0007__x0007_* confusion matrix (http://support.sas.com/kb/22/603.html);_x0007_* > table(qda.class,Direction.2005);_x0007_proc freq data=Preds5;_x0007_   table _INTO_*Direction / out=CellCounts5 norow nocol nopercent;_x0007_run;_x0007__x0007_* calculate % correctly predicted;_x0007_* > mean(qda.class==Direction.2005);_x0007_data CellCounts5;_x0007_   set CellCounts5;_x0007_   Match = 0;_x0007_   if _INTO_ = Direction then Match = 1;_x0007_run;_x0007__x0007_proc means data=CellCounts5 mean;_x0007_   freq count;_x0007_   var Match;_x0007_run;_x0007__x0007_quit;""",1,0,1
"*From the dept of \too much free time,\"" I created SAS programs to duplicate the output from the R code in the Chapter 4 Lab.;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 4.6.6 An Application to Caravan Insurance Data *****;_x0007_title '4.6.6 An Application to Caravan Insurance Data';_x0007__x0007_* csv file created from Caravan dataset (> write.csv(Caravan,file=\""Caravan.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_proc import out=Caravan dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookCaravan.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_* > attach(Caravan)_x0007_  > summary(Purchase);_x0007_proc freq data=Caravan;_x0007_   table Purchase / nopercent nocum;_x0007_run;_x0007__x0007_* standardize the data;_x0007_* > standardized.X=scale(Caravan[,-86]);_x0007_proc standard data=Caravan mean=0 std=1 out=standardizedX;_x0007_run;_x0007__x0007_* check variances of original & standardized data;_x0007_* > var(Caravan[,1])_x0007_  > var(Caravan[,2]);_x0007_proc means data=Caravan var;_x0007_   title2 'Original Data Variances';_x0007_run;_x0007__x0007_* > var( standardized.X[,1])_x0007_  > var( standardized.X[,2]);_x0007_proc means data=standardizedX var;_x0007_   title2 'Standardized Data Variances';_x0007_run;_x0007__x0007_* create test & training datasets;_x0007_* > test=1:1000_x0007_  > train.X=standardized.X[-test,]_x0007_  > test.X=standardized.X[test,]_x0007_  > train.Y=Purchase[-test]_x0007_  > test.Y=Purchase[test];_x0007_data trainX testX;_x0007_   set standardizedX;_x0007_   if _n_ le 1000 then output testX;_x0007_   else output trainX;_x0007_run;_x0007__x0007_* model using k=1 (results may differ slightly from R b/c of ties/use of seed - i think);_x0007_* > set.seed (1)_x0007_  > knn.pred=knn (train .X,test.X,train .Y,k=1);_x0007_proc discrim data=trainX testdata=testX method=npar k=1 testout=Preds8 noprint;_x0007_   title2 'KNN: K=1';_x0007_   class Purchase;_x0007_   priors proportional;_x0007_run;_x0007__x0007_* confusion matrix;_x0007_proc freq data=Preds8;_x0007_   table _INTO_*Purchase / out=CellCounts8 norow nocol nopercent;_x0007_run;_x0007__x0007_* > mean(test.Y!= knn.pred)_x0007_  > mean(test.Y!=\"" No\"");_x0007_data CellCounts8;_x0007_   set CellCounts8;_x0007_   NoMatch = 0;_x0007_   if _INTO_ ne Purchase then NoMatch = 1;_x0007_   if Purchase = 'Yes' then Yes01 = 1;_x0007_     else Yes01 = 0;_x0007_run;_x0007__x0007_proc means data=CellCounts8 mean;_x0007_   freq count;_x0007_   var NoMatch Yes01;_x0007_run;_x0007__x0007_* model using k=3 (results may differ slightly from R b/c of ties/use of seed - i think);_x0007_* > knn.pred=knn(train.X,test.X,train.Y,k=3);_x0007_proc discrim data=trainX testdata=testX method=""",1,0,1
Sorry about the formatting. It appears to be automatic.,1,0,1
Sorry about the formatting. It appears to be automatic.,1,0,1
Sorry about the formatting. It appears to be automatic.,1,0,1
Sorry about the formatting. It appears to be automatic.,1,0,1
"Logistic function has a S-shape curve. So, it can be easily fit to a classification model with 2 classes. If you take look at slide 7 where linear function and curve obtained by logistic regression are compared, you can see that the line is not appropriate to DISCRIMINATE (classify) the observations. I hope to understand your question correctly.",1,0,1
I haven't watched lectures on the ipad.,1,0,1
"In your (top) equation, the right-hand side is a random variable, while the lhs is (apparently) a fixed number. You also say \where p is probability\"". Probability of what exactly? Or are you saying that the probability of a random variable (actually, of an event) is **itself** a random variable?!""",1,1,1
Check out the RODBC package.,1,0,1
"Perhaps someone can help me with this. This problem has a response with two classes (Y = {A; no A}). So (at least it´s what I think), we can exclude linear regression._x0007__x0007_The independent variables are tricky. Family income could be related to grades. But the height of the student does not seemed to be related. So, I thought the best approach would be to make a random guess. Turns out I was wrong._x0007__x0007_Help! Thanks.",1,1,1
What do you exactly mean by SPARSITY in machine learning? ppl want to have sparse models. What is a sparse model and what are the advantages/disadvantages over other models._x0007__x0007_Thanks,1,1,1
"If p is the probability of X, how is it that you have X1, X2, ... but you don't have p1, p2, ....? And if you do have p1, p2, ..., how do these numbers go into the equation in your previous post?",1,1,1
"From what I understand (see here: http://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&ved=0CG4QFjAI&url=ftp%3A%2F%2Fftp.sanger.ac.uk%2Fpub4%2Ftheses%2Fdown%2Fchapter2.pdf&ei=D5r1UtzOBIOQ7AaC_oDoAw&usg=AFQjCNFHDUZt5z-6Ow7_AqRXR6iRfXCnHw&sig2=H4k87eZDTSHW3H3YhZNYuw&bvm=bv.<phoneRedac>3,d.ZGU) sparse learning has to do with selecting a small number of highly predictive features. I think we'll learn about this in future weeks (but if we don't, there's always google)._x0007__x0007_(If I understand this correctly, perhaps we've already learned a bit about that when we talked about forward and backward selection of significant predictors in logistic regression).",1,0,1
I figured it out :).,1,0,1
William did not completely specify the model. (He omitted the response variable.) The complete specification is_x0007_log(P(Y=1|X)/(1-P(Y=1|X))) =  B_0 + B_1X_1 + ..._x0007_Now you can easily see which probability is being modeled. It's the conditional probability that Y = 1 given X.,1,0,1
"Hi,_x0007_Can anyone help me how we can reach from equation 4.12 to 4.13 and finally to 4.14?_x0007_I became confused with them.",1,1,1
"> Why does logistic regression give an estimate of the_x0007_> probability that the response variable belong to a certain_x0007_> category? _x0007__x0007_I'm not sure I understand, but what else would you want from_x0007_a model where the dependent is a categorical variable which_x0007_must be either in one category or the other ?_x0007__x0007_Maybe a model which will tell you *for sure* which of the two_x0007_categories it will be ?_x0007__x0007_That would be ideal, but such certainty may not be possible_x0007_in some cases._x0007__x0007_The *probability* prediction from the logit can be used to_x0007_make a forecast for a definite category, for example by_x0007_picking the category with the highest probability.  But this_x0007_would not be expected to be right all the time._x0007__x0007_> The fact that it takes values between 0 and 1_x0007_> doesn't mean that it's a probability. _x0007__x0007_The *observed* values will be 0 or 1, nothing in between._x0007_But logit model gives a prediction of the probability that_x0007_the \1\""-category will occur._x0007__x0007_For example, suppose the dependent variable is $B$, \""buy an_x0007_ice-cream sometime during the day\"", and, there is a single_x0007_explanatory variable, $T$, \""the daily maximum temperature\""._x0007__x0007_The logit model would give a prediction of the probability_x0007_like this:_x0007__x0007_$ _x0007_hat{mathrm{Prob}}(mathrm{Buy ice cream})_x0007_=_x0007_{_x0007_e^{(hat_x0008_eta_0 + hat_x0008_eta_1 imes T)} over 1 + e^{(hat eta_0 + hat eta_1 imes T)} } $  Fitting the model would give the fitted coefficients $ eta_0,  eta_1$.  You could then take a particular temperature, $T$, and plug it into the equation to get a prediction or the probability that someone would buy and ice-cream at that temperature. You'd expect $hat eta_1$ to be positive, meaning that as $T$ increased, it would increase the probability of buying an ice-cream.  In the end, at the given temperature, $T$, the person either buys an ice-cream ($B=1$) or they do not ($B=0$).  But this may not be completely predictable so the most we can say is that there is some probability, $hat{mathrm{Prob}}(B=1)$, that the person will buy ice-cream at that temperature.  If the model is working well, that probability will be close to the proportion of days of that temperature that the person buys ice-cream.  > in any case, what is the assumed distribution of the > predictors?  I don't think there is any assumed distribution of the predictors.  In the example above, that would mean no assumed distribution for temperature, $T$.  This is as opposed to linear discriminant analysis, which assumes that the \""buy ice cream\"" category has a normal distribution of temperatures (probably """,1,1,1
<redacted> :_x0007__x0007_> Logistic regression assumes the following true model:_x0007_> _x0007_> $ log(mathrm{odds}) = _x0008_eta_0 + _x0008_eta_1 X_1 + ldots + _x0008_eta_p X_p + epsilon $_x0007_> _x0007_> Log(odds) ranges between negative infinity and positive infinity. _x0007_> Your estimate of log(odds) ca,1,0,0
How to deal in R with missing data in regression and classification? Is there a compiled list of R functions which allow the use of data matrices with missing entries?,1,1,1
"Hi,_x0007__x0007_With regards to slide 20 of Lecture 4.5, I'm not convinced when prof. Hastie says that $f_k(x)=P(X=x|Y=k)$ is the pdf (probability distribution function) for $X$, given $Y$ in class $k$. If $X$ is a continuous random variable, then _x0007__x0007_$0=P(X=x)geq P(X=x|Y=k) geq 0 implies P(X=x|Y=k)=0$_x0007__x0007_By the way, that would also make the denominator of Bayes Theorem zero,which also is an issue. So, how do we solve that? I guess that when we write $X=x$, we are really thinking of $X in [x,x+dx]$. This way, $P(X in [x,x+dx])=f_X(x)dx$ and $P(X in [x,x+dx]|Y=k)=f(x|Y=k)dx$, where $f_X(x)$ denotes the marginal density of $X$, while $f(x|Y=k)$ is the conditional density of $X$ given $Y=k$. By eliminating $dx$ between numerator and denominator, and using the Total Probability theorem, we get the final formula in slide 20. Is this correct? Thanks, _x0007__x0007_Best Regards_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,1,1
joined._x0007_Hi friends i'm from Italy and I work whit data mining in a big company in Roma. I have a master degree in physics,1,0,1
"Hi all. I'm very glad to participate this course. I saw that we can download de full video of each class. Is it possible to release also the captions? If is, how?_x0007__x0007_Thanks a lot!",1,1,1
Thank you.,1,0,1
Packages vary on how they handle NA values.  You need to look at help page of the function.  In cases where the function does not allow NAs you can use the na.omit() function to create a data frame with non-missing cases.,1,0,0
"> \But the height of the student does not seemed to be related.\""_x0007__x0007_Says who? You can't actually know for sure if it's related until you run the model and check._x0007__x0007_For example, maybe height correlates with nutrition/diet, and malnourished students might perform worse academically? Or maybe taller students get called on in class more often and/or otherwise make more favorable impressions on the teacher? Or maybe taller students are more likely to be on the basketball team and thus have less time to study?_x0007__x0007_Or maybe you're right and there's no relation whatsoever. You would still need to choose a type of model to *check* that assumption, and some models would be more appropriate than others._x0007__x0007_Don't forget that you can choose more than one! :-)""",1,0,0
On the courseware tab I can only access Chapters 1 - 4. No Chapter 5 onwards?,1,1,1
I am not seeing chapter 5 either.,1,1,1
"Patience! :-)_x0007__x0007_The new chapters seem to be \opened\"" at *some* point Saturday morning (California time), but not necessarily at the stroke of midnight._x0007__x0007_I assume we'll see something within an hour or so, if the pattern holds.""",1,0,1
"\install.packages\"" wasn't working, so I downloaded the Zip file from CRAN and unzipped the contents under \""C:Users<nameRedac_<anon_screen_name_redacted>>DocumentsRwin-library2.13\"" of my laptop. It still gives a couple of warnings when I load it using \""library(ISLR)\"", but I am able to view the data sets contained in the package.""",1,1,1
"Since LDA/QDA require multivariate normality among the predictors, I am wondering how often this is useable in practice.  My work is in biostatistics where predictor variables are more often than not categorical.  So in my field, is this ever are viable option?",1,1,1
We want to to see for which $k$ $p_k(x)$ is largest. Take a closer look at the denominator of $(4.12)$. As this is just a sum over all possible classes (and does not depend on any particular $k$) we can ignore it. _x0007__x0007_The next step is to take the $log$ of the numerator of $p_k(x)$ (Since this is just a monotone transformation). When you do that and discard the unimportant terms (i.e. all that don't depend on $k$) you get $(4.13)$._x0007__x0007_To get from $(4.13)$ to $(4.14)$ set $delta_1(x)=delta_2(x)$ (why?) and solve for $x$.,1,0,0
Agree with malencv - took me 4/5 attempts with rounding before realising getting the result grading wanted.,1,0,1
"The most useful function, I think, if you want to have control over your data, is:_x0007_is.na()_x0007__x0007_Then you can use it to subset your data, find the incomplete cases (which() is useful) etc._x0007__x0007_na.omit(), na.exclude() and na.fail() are explained here:_x0007_http://stat.ethz.ch/R-manual/R-patched/library/stats/html/na.fail.html_x0007__x0007_You can use na.omit, na.exclude, na.fail as options in other functions, for example check option na.action here:_x0007_http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lm.html_x0007__x0007_Also useful is:_x0007_complete.cases()_x0007_http://stat.ethz.ch/R-manual/R-patched/library/stats/html/complete.cases.html_x0007__x0007_Disclaimer: I have never used R for real, all the above is courtesy to Coursera courses in Computing for Data Analysis and Data Analysis (both from John Hopkins) and google search.",1,0,1
"If I understand this right - you are citing an example - where we initially find more instances of lung cancer in the non-smoking group. If we do not have any other information - then the only conclusion we can draw would be - what you have said- that, \a smoker has lower risk of lung cancer\"" !!_x0007__x0007_It is only when we add age to the mix, that we find that the high incidence of cancer was due to the non-smokers' higher age and for the same age a smoker is more likely to have lung cancer. _x0007__x0007_We could argue that we should distrust the simple linear regression because of this, but, in my mind, this has got to do with how we pick samples for our study. We could have picked our samples of both smokers and non-smokers such that both included the same fraction of elderly people (\""everything else being common between the two groups except that one group smokes and the other does not\"")._x0007__x0007_I am not sure if this is the correct answer. Hope someone with more knowledge throws some light on this. Thanks for the interesting question though.""",1,1,1
"My stab at answering this (but please correct me if I am wrong): if a student asks for credit, the bank knows that he is more likely to pay than a non-student for the same level of balance, but also that he is likely to run larger balances, and the general probability of larger balances being paid is smaller. So, not knowing what balance that particular student would run, it makes sense to be cautious about giving him a loan (and this is captured by the single-variate logistic regression with 'student' as predictor)._x0007__x0007_Formally, I think we can write:_x0007__x0007_p(default|student)=sum_{available_balances}(p(default|balance)*p(balance|student))_x0007__x0007_compared to a non-student, the terms with larger default probabilities (ie those for larger balances) are more heavily weighted for students, which is why: p(default|student)>p(default|nonstudent)._x0007__x0007_But:_x0007_p(default|student, balance)lt p(default|non-student|balance)_x0007__x0007_Hope this makes sense.",1,0,1
lol... thanks,1,0,1
"Hi,_x0007_To see how good my prediction is I can do a cross-validation study._x0007_But I was wondering if the cross-validation was going to be OK because my sample size is very small (n=22 european countries)._x0007__x0007_If I split data into 2 parts, most of the time one of this part must have >60% of the sample size, that means one part with n=13 or 14 countries and a second part with 8 or 9 countries._x0007__x0007_Is it really enough data to do a cross-validation study ? Or there are not enough data for cross-validation study ? So what to do ?_x0007__x0007_Thanks for your response,",1,1,1
"I don't really understand how MSE would be used to predict categorical variables. For the binomial case for instance, all our outcomes are 0 and 1 - not the probability of a 0 or a 1 (unless we repeat the experiment with exactly the same inputs several times), but just 0 or 1, for each combination of inputs. It makes all the sense to try to predict the probability of the outcome, and to minimise the discrepancy from observations given by a particular model - which is exactly what the ML function is. This is what makes intuitive sense to me, at least.",1,0,1
"By 'covariance of Xi and Xj' do you mean:_x0007__x0007_sum_k(xik-mean(Xi))*sum_l(xjl-mean(Xj))_x0007__x0007_where xik are the components of Xi and xjl are the components of Xj_x0007_(assuming Xi and Xj are vectors)?_x0007__x0007_Sorry, I've always struggled to get an intuition of the concept of covariance, in relation with correlation and variance._x0007__x0007_Is it that for normalised vectors covariance is the same as correlation matrix?",1,1,1
Try leave-one-out?,1,0,1
"Like the fellow coursemate in the other thread, I took the class (first time round) and thoroughly enjoyed it - highly recommended if you wanted well structured practice.",1,0,1
"AlanX, I think you're right, I think it has to do with the contrasts you are interested in._x0007__x0007_If you code: 2 for Asian, 1 for Caucasian, 0 for African, what you will get is a comparison between the weighted average of Asian and Caucasian (with weight 2 for Asian and 1 for Caucasian) versus African._x0007__x0007_This contrast might make some sense if you're checking for differences in running speed, and you've previously established that an Asian runs twice as fast as a Caucasian (twice of half? please would someone who knows tell me what would be the right interpretation here? - thank you) - and in your current model you're trying to decide whether or not the running speed of an African is different from the average running speed of a non-African.",1,1,1
"Should you prefer a more intuitive way of handling R from a graphical user interface, the R Deducer limits or avoids programming - I regret it is not a program I know.",0,0,1
"You can save the data from RStudio to csv format. for example, to save the Carseats data:_x0007__x0007__x0007__x0007_     library(ISLR)_x0007_    _x0007_     data(Carseats)_x0007_    _x0007_     names(Carseats) # to look at variables_x0007_    _x0007_     write.csv(Carseats, file='Carseats.csv') _x0007__x0007__x0007__x0007_    _x0007__x0007_Now you have a csv file to use with any software package.",1,0,1
"I'm using a MacBook Pro and I prefer to watch the course videos in fullscreen (clicking the \overlapping rectangles\"" icon) with my browser in fullscreen mode, so that the video is as large as possible and to minimize distractions. When I do this, the video controls along the bottom of the window overlap the bottom of the video content, so that the bottom of the lecture slides are cut off._x0007__x0007_I've found that if I take my browser out of fullscreen and resize the window to roughly 4:3 ratio, then the controls no longer overlap the video content. So I have a workaround, but it would be nice if it just worked in a widescreen format._x0007__x0007__Widescreen example -- bottom of slide is cut off:__x0007_![widescreen][2]_x0007__x0007__Resized window to 4:3 ratio:__x0007_![not widescreen][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>58.png_x0007_  [2]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>31.png""",1,1,1
Do you have just one data point for each country?,1,1,1
"In the simplified LOOCV formula for linear and polynomial regression using leverages hi topic(Lec 5.2), is there a simplified formula for classification problems?",1,1,1
"Sparse models, i.e. with few predictors, have the advantage of being simpler and can avoid overfitting. _x0007__x0007_Another advantage to sparse models is that if it has a variable selection procedure, then it can help you decide which variables are important without trying all the subsets of predictors. For example, with number of predictors = 40, the number of models (subsets of predictors) is over 1 billion. _x0007__x0007_Disadvantage is that it may not fit as well if you had a richer model. But then again, it might fit better as overfitting could be an issue for the larger model.",1,0,0
50 samples is actually the number of observations and not the number of datasets.,1,0,1
"Sorry, I'm not really concerning myself with this \going to the client\"" business. That has to do with your business or particular job or something. We're talking here about models and how to use them. If you don't want to include parameters in your model that aren't significant when fitting your model to the dataset, then it sounds like you're interested in how to do model selection, which is a large topic. So a good place to start for you might be to check out some basic material on model selection.""",1,0,1
"\...but what else would you want from a model where t...\""_x0007__x0007_The issue is not what I want, but rather, what it is. Apparently you are saying that probabilities are meaningful even when the distribution of the random variables in play is not specified!! So, what definition of random variable do you use?""",0,1,1
"I think you should use the same model across every cv-split, which may be chosen when you fit the model use the whole train data.",1,0,1
"\If we have n data points, what is the probability that a given data point does not appear in a bootstrap sample?\""_x0007__x0007_What is the format of the answer accepted? just a real number?_x0007_I used fractionals and reals but it doesn't seems to get accepted... anyone got this question right? thanks""",1,1,1
"On page 14/44 of the slides, how is 'flexibility' quantified - what does it mean, in terms of model parameters (degree of polynomial, or...?)_x0007__x0007_Also, to calculate the 'true' error I suppose one could generate huge training sets and huge test sets, but wouldn't the resulting error still depend on which particular training and test sets were used? (perhaps for very large sets it wouldn't, but I am not sure I can 'picture' this)._x0007__x0007_Thanks for help.",1,1,1
"Your answer will involve \n\"".  Use the usual parentheses and +, -, *, /, ^ operators to express your answer._x0007__x0007_For example, your answer might look something like (2 - 3*n)/(n + 1)^5.""",1,0,0
"Your answer should include at least two significant digits.  For example, if your answer was <zipRedac>.<zipRedac><zipRedac>12345, you should at least enter <zipRedac>.<zipRedac><zipRedac>12.",1,0,0
"Liaosa, just pick the std error as comes out of the R model as shown with the \summary\"" function.""",1,0,1
......I am considering it talks about using data within 10% of some......_x0007__x0007_Thanks for clarifying this...,1,0,0
"I think that if you pretend to be \God\"" (knowing all the population, like if simulated), you should just pretend to use the whole population to train the model. The error rate will then just depend on the model complexity (bias).""",1,0,1
Does it mean : will include purely the interaction unless the main effects are already included in the . ?,1,1,1
"anonymous, I don't know how this is done - but how would you do it, if you had to invent your own method?_x0007__x0007_If I had to invent my own method, I might think of a few things:_x0007__x0007_- the first thing I would try would be to use the whole data and compare between the winning models coming from cross-validation using AIC or BIC_x0007__x0007_- the second thing I might try would be to do forward/backward step-wise selection of predictors, using only the set of predictors that had appeared at least in one of the winning models_x0007__x0007_Other things I might try:_x0007__x0007_- if I could, I might gather more data and do cross-validation on it using only the predictors that had been significant in at least one (or at least a fraction, or perhaps in all the models fitted for the first set of data)_x0007__x0007_- if I couldn't gather more data, I might be tempted to throw away the predictors that are clearly non-significant, and redo the cross-validation with a fewer number of predictors_x0007__x0007_- in fact, I would try to redo the cross-validation over a number of different random splits of the data, and hope the process converges onto a common model_x0007__x0007_Would such a 'hierarchical' cross-validation process be correct, statistically - what do people think?_x0007__x0007_I am curious, too, what is the 'official' answer - how this is done in practice.",1,1,1
I'm pleasantly surprised at how much the professors' joking banter makes the videos pass more quickly without distracting from or diluting the material.,1,0,1
"I am using Firefox (not going to go into what all add-ons I have.  The coefficient for model element beta_0 was displayed as \  6\"" instead of \""-6\"".  After getting it wrong with different numbers of significant digits specified, I reloaded.  One can see the codes briefly before they are converted to math symbology and it clearly says \""-6\"" in gray before changing to the math font._x0007__x0007_Just in case any others are having the same issue....""",1,0,1
"I think it also reduces to what is what you try to achieve when fitting the model, for example, if your goal is accuracy, then maximizing likelihood is what in some sense is closer to minimizing accuracy. If your goal is to minimize RSS of the probabilities, and that's your metric then what does make sense is to minimize least squares, and if you don't have any metric of your model, just assign random values to the parameters, there is no reason to do any other thing._x0007__x0007_Keeping in mind your metric is what should guide you in choosing the training. Of course, what I have just stated is that the problem is to decide what metric you should use when fitting a model. It turns out that decision theory suggests that accuracy should be used for classification tasks, and that leads naturally to maximum likelihood.",1,0,0
"Yes, the blocks can overlap.  _x0007__x0007_The bigger the blocks, the fewer you have to bootstrap with.  The smaller the blocks, the greater chance of blocks being correlated due to sequences of observations being related.  So I would think the correct block size is domain and problem dependent._x0007__x0007_For anyone having trouble with R for that question, check out the 'tsboot' function.  It's the time-series version of the regular 'boot' function with options for specifying sub-blocks.",1,0,0
The same happens on my Windows computer. Like you I discovered that I had to reduce the width of the screen. Still don't have a fix for keeping the closed captions from obscuring the video.,1,1,1
Cracked up when Prof Tibshirani added that having a funny accent was also a risk factor for heart disease.,1,0,1
But still a slightly better resolution video download link will be better.,1,1,1
Is LOOCV the exact same thing as PRESS? At least for regression models.,1,1,1
"I don't understand what the term \stepwise regression\"" and \""sequence of models\"" mean. Also, I don't quite get the answer explanation. Does it mean using the full dataset, we are less likely to overfit the data?""",1,1,1
"OK, got it. The real definition is on p. 119 of the other Statistical Learning textbook.",1,0,0
Only for linear regression models.,1,0,1
"I like it when Prof Tibshirani says \aboot\"" when he means \""about\"".""",1,0,1
"hello guys,_x0007_I m from Lucknow. I am a doctrate student.",1,0,1
"hello guys,_x0007_I am from Lucknow.",1,0,1
"If equations (1) and (2) confuse you, then this explanation may help:_x0007__x0007_Total Sum of Squares is the Error Sum of Squares associated with using the mean of y ($_x0008_ar y$) as your prediction for all observations._x0007__x0007_Residual Sum of Squares is the Error Sum of Squares associated with using a model ($hat y$) as your prediction for all observations._x0007__x0007_$R^2$ measures the net reduction in error sum of squares as a result of using a model rather than just ($_x0008_ar y$)",1,0,0
"\ ... net reduction in error sum of squares relative to the total sum of squares ... \""""",1,0,1
"<redacted>, I don't thing removing the mean would change the correlational structure of the variables. What you might think about is to orthogonalise X1 and X2 (eg enter in regression not X2 but the projection of X2 on a direction perpendicular to X1). I, too, would like to know what are the advantages and disadvantages of orthogonalising variables (I don't like orthogonalisation, but I know others do use it)._x0007__x0007_Also, the idea that X1*X2 is correlated with X1 is a surprising thought. Intuitively, if X1 and X2 are not correlated, the interaction term shouldn't be collinear with either of them - but, again, I would like to know if this is the case.",1,1,1
"@MASte:_x0007__x0007_> Download the file [5.R.RData](https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/5.R.RData) and load it into R using load(\5.R.RData\"")._x0007__x0007_> So where exactly is \""5.R.RData\""?_x0007__x0007_If you click on the link, [5.R.RData](https://class",1,0,1
"Unless otherwise stated, any sample data would be taken as observational data. And for observational data, you cannot make any \cause-and-effect\"" statement (in general), such as smoking leads to lung cancer. To be able to do so, you need to carry out (double-blinded) randomised experiment, the gold standard. That's why, the distinction between association (correlation) and causation is always so highly emphasised.""",1,0,1
"I tried tsboot() and got the wrong answer. Since this is incorrect (it gives the same result as in 5.R.R3), let me post the function call:_x0007__x0007_    tsboot(Xy, seBeta1.fn, R=1000, sim=\fixed\"", l=100)_x0007__x0007_where `seBeta1.fn()` was the function used to answer 5.R.R3_x0007__x0007_Not sure what's missing...""",0,1,1
"Hi <redacted>, my point in the example I gave is that sometimes collinearity can occur by accident, in this case X1 and X1*X2 are correlated regardless of the relationship between X1 and X2. I think the following discussion is quite straight to this point: http://stats.stackexchange.com/questions/<zipRedac>047<zipRedac>/collinearity-diagnostics-problematic-only-when-the-interaction-term-is-included_x0007__x0007_About the orthogonalisation, I am not an expert too. But I think one thing that we might lose is \interpretation\"" of the coefficients attached to the orthogonalized predictors._x0007__x0007_Cheers,_x0007__x0007_Sorry for any spelling mistakes, my PC has a German spell-checker even though I am not a native German speaker.""",1,0,1
"Yes sirahman, totally agree.",1,0,1
Hi !_x0007__x0007_I also would appreciate that the captions made available for download._x0007__x0007_Thanks in advance._x0007__x0007_Cheers !,1,1,1
"If you right-click any of the videos here in the course, you will find an option to copy the actual YouTube URL of the video.",1,0,1
"> The seed you set is only valid for the next command (I think)._x0007__x0007_That's pretty much correct. Each call to any function that uses the random number generator will use the seed and then essentially set it to a new value by the time it's done._x0007__x0007_But that new value will be the same every time for any given previous seed._x0007__x0007_Which is to say that you don't have to set the seed before every single call to a random function, if you're using a sequence of random function calls in your program. Just a single seed set at the beginning will do the trick._x0007__x0007_But if you change the order of your random function calls, or add/subtract a random function call, then you will get different overall results for the same initial seed.",1,0,0
"The problem is posed as, \If we have n data points, what is the probability that a given data point does not appear in *a* bootstrap sample?\"".  I took this to mean in a single bootstrap sample._x0007__x0007_However, I believe the correct answer as provided instead solves the question, \""If we have n data points, what is the probability that a given data point does not appear in *any* bootstrap sample?\""_x0007__x0007_Could be that it's too early in the day for me, or that it's been too long since I have thought much about probability.  Can someone please point out where I've gone wrong?_x0007_Much obliged""",1,1,1
Ah...thanks!,1,0,1
"Hi <redacted>, I've not fully figured it out yet and will post back when I do but I'm pretty sure page 132 of the book is a good place to help.  There we see the exponential e in 4.3 being transformed via logarithm into 4.4",1,0,1
"Use a different Firefox add-on, \Download YouTube Videos as MP4\"".""",1,0,1
"I'm enjoying this class and am interested in recommendations for other classes.  Please share suggestions.  I like the pace, level of difficulty, introductory nature, and subject of this course.  I'm trying to learn R for applications in finance (investing) but have a wide range of interests.  But I'd like to hear about any online courses you've enjoyed and why - others may find it helpful.",1,0,1
"The question is:  To carry out this implementation on a data set with n data points, how many bootstrap data sets would we need to average over?_x0007__x0007_It seems to me that the answer must include a factorial (!).  But when I try to enter \!\"" into the answer box I get the error message \""Sorry, couldn't parse formula.\""  Does anyone have any suggestions?""",1,1,1
"> Is it just me, or do the questions in a lot of these quizzes bear very little relationship with the course lectures and/or text?_x0007__x0007_I've felt the quiz questions follow the material and lectures quite closely. In fact, many of them literally ask you something about a particular slide from the lecture notes, which is just a record of what was presented in the videos...",0,1,1
[This answer][1] explains it really well. Especially the geometric explanation._x0007__x0007__x0007_  [1]: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/52e819e84c0c9bd8f<phoneRedac>,1,0,1
Shouldn't it be page 9 rather than 8?,1,1,1
The wording of the question has me confused. Is the professor asking how many data sets do we need to average over WITHOUT resampling?,1,1,1
"For all to connect to a CRAN mirror from the R command line type in setInternet2(TRUE) and select your location and moving forward you can then load packages and dependencies via install.packages(\nameofpackage\"") followed by library(\""nameofpackage\"") or      require(\""nameofpackage\"") .""",1,0,1
"A follow up question, my inexperience with R (I'm actually using RStudio) is showing through..._x0007__x0007_After I've loaded the \5.R.RData\"" file, what new variable(s) should I see?  ls() is just showing my my previous list and there are no new packages to load""",1,1,1
I also have a problem with the factorial.  A way to specify a product series would also work. _x0007__x0007_I have not tried Stirling's approximation or anything like that though.,1,0,1
"When the number of dimensions increases, observation points become farther away from each other.",1,0,0
Thank you for the feedback. Check the numerator and denominator of your R^2 formula.,1,0,1
"I got the right answer. 'amw5g' is correct in his/her restatement of the question. 'boethian' is right, but just went 'one step too far' since (based on the accepted answer) the author of the question isn't interested in the limit as n->infinity but rather a formula in terms of n.  Cheers!",1,0,0
"@Gavin: I think what you are asking (second part of your question) is: _x0007__x0007_*does the answer of 5.3.R1 somehow suggest that **more data is worse?*** _x0007__x0007_No, it does not! Here is what I think it says:_x0007__x0007_We want to choose the model size, call it \n\"". Put some li",1,0,1
"For 5.R.R3 I tried the following:_x0007__x0007_    myfun<-function(aframe, subset) {_x0007_     fit1<-lm(y~., data=aframe, subset) # lin. rgrsn on a bootstrap sample_x0007_     coefficients(fit1)[['X1']] # return the est. of Beta1_x0007_}_x0007__x0007_    # Now compute the bootstrap stat's on Beta1 from 100 samples:_x0007_boot(Xy, myfun, R=100)_x0007__x0007_but the S.E. returned by boot() is rejected as the answer to the question.  Where did I go wrong?",1,1,1
use objects() or ls(),1,0,1
Hi were can I download a copy of R and how so I install the packages for the assignments etc.,1,1,1
Paul is right. _x0007__x0007_$p == Pr(Y=1 | X)$_x0007__x0007_The true model is _x0007__x0007_$log(frac{p}{1-p}) = _x0008_eta_0 + _x0008_eta_1X_1+...+_x0008_eta_pXp+epsilon$,1,0,1
"Ian Witten gave a course on machine learning using Weka program. (He is one of the main developers.)  Don't know when he will offer it again, however.  Here is the link:_x0007__x0007_[Weka Course][1]_x0007__x0007__x0007_  [1]: http://%5Bhttps://weka.waikato.ac.nz/dataminingwithweka/course%5D",1,0,1
"We have a normal mixture whose trial probabilities (pi) are different that 1/2, the probabilities in the previous slide. As far as I can see the two densities, with their new weights, cannot possibly cross each other at the same point they crossed when their weights were 1/2. To me, they should cross to the left of zero, the initial crossing point.",1,0,1
"Why do you get incorrect results when you bootstrap the S.E. (and compute the mean of this dist.), compared to bootstrapping the parameter estimate of from that infer the standard error?",1,1,1
"Note that in bootstrap you are allowed to sample the same point multiple times. For instance one of the possible bootstrap data sets is obtained by a single point from the original data, but used n times.",1,0,1
Boethian had one more step after the 1-1/n and before the 1/e,1,0,1
Could you elaborate more about $epsilon$? Is there no irreducible error in logit models?,1,1,1
"In the normal bootstrap, yes.  But the way I understand this question is that this is *not* the normal bootstrap.  Note the statement in the question, \Unlike the usual implementation of the bootstrap, this method has the advantage of not introducing extra noise due to resampling randomly.\""  So the bootstrap samples produced here must all have the same data points in them but in different order.  If this were not the case than the number of bootstrap samples of an original data set of n size could theoretically be infinite.""",1,0,1
edit: removed $epsilon$ based on feedback from alhf.,1,0,1
"In your function, what role does the index statement do?  I understand that when we use a data[something,]  that we are picking a subset of certain rows and all columns of the data.  So I am not sure as to what index is doing here in this context?_x0007__x0007_Thanks for everyone's help!",1,1,1
"Are you referring to the two panels, left panel $pi_1 = pi_2 = 0.5$ and right panel $pi_1 = 0.3, pi_2 = 0.7$? The right panel does cross to the left of zero.",1,1,1
"I think this is a good explanation, and I also found this question rather tricky._x0007__x0007_My way to describe what the effect is would be:_x0007__x0007_The more complex the models are, the higher the likelihood of overfitting._x0007__x0007_How do we recognize overfitting? By seeing that the test-data has an error rate a lot worse than the training data._x0007__x0007_But by using all data (that is, the training data as well) you \prepare\"" your model for the test data that will come._x0007__x0007_This preparation will lower the error rate on the test-data, and you will not recognize overfitting.""",1,0,1
"Dear All,_x0007__x0007_Could anyone please advise me where I could access the lecture videos? Have my settings changed - as I remember having seen the first one?_x0007__x0007_Thank you in advance._x0007__x0007_Best wishes,_x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,1,1
"After the function definition alpha=function(x,y){} there is a line (#55 on my version) _x0007_alpha.fn(Portfolio,1:100)_x0007__x0007_What does this do? Does it just run the alpha function once with the first 100 observations?  It is just for testing to see it doesn't produce an error as it appears to do nothing with the results?",1,1,1
we are after the se of the parameter (b1). tsboot computes this.  not the se of the se.,1,0,1
Yes it uses the first hundred observations from the dataset Portfolio to calculate one alpha value,1,0,1
"Mario:_x0007__x0007_down <- x%*%solve(si)%*%means[,1] - as.numeric(0.5*(t(means[,1]%*%si%*%means[,1])) + as.numeric(log(prior[1]))_x0007__x0007_In this line, solve(si) would give you the original covariance matrix. Shouldn't it be, in fact si? (without solve?)_x0007__x0007_Much admiration for doing this.",1,0,1
"Loading the 5.R.RData file will put a new dataframe \Xy\"" in your workspace.  Xy has fields y, X1, and X2.""",1,0,1
"Also:_x0007__x0007_means <- t(means) #is this needed?_x0007__x0007_I am not sure it's needed. What I would do, I would call down_means<-means[1,] and up_means<-means[2,]_x0007__x0007_then I would use 4.19 as it is (eg t(down_means)%*%si%*%down_means) - I am not sure how it would be dimensional otherwise (but perhaps my version would turn out to not be dimensional)_x0007__x0007_Please check these two things - if they don't help, I'll try to look more at this tomorrow evening (or perhaps someone else will find a better answer).",1,0,1
This topic is on the lecture slides (pages 38-44) but I couldn't find the corresponding video lecture.,1,1,1
"'boot(Xy, myfun, R=1000)' takes the dataframe Xy, the function myfun, and the number R, and estimates the standard error for the value of myfun.  The function myfun must be a function of a dataframe and an index vector.  Behind the scenes, 'boot' will randomly create R index vectors.  For each of the R index vectors (which we never see), 'boot' takes the dataframe, subsets it using that index vector, and computes the value of myfun.  At the end of this process there will be R different values for myfun.  'boot' then returns the standard deviation of these R numbers, and this standard deviation will be a good estimate of the standard error for the value of myfun if we could apply myfun to many different samples (which we can't because we have only the one).",1,0,0
I found a useful discussion by Dr. Singh (Texas A&M Univ) here. http://www.researchgate.net/post/How_many_bootstraps_can_I_do_from_a_sample_of_N_elements_x0007_where he discusses the law of counting.  I hope this helps.,1,0,1
"In each bootstrap sample, you randomly choose n data points with replacement.  What is the probability that a given data point misses all those n choices?",1,1,1
"Hi,_x0007_Please tell me if this statement is right or wrong._x0007__x0007_I always have to use the whole dataset to first calculate the training error that will give me an idea of the relationship between the predictors and the class, and depending on that result find a subset of predictors that best fit the model. _x0007__x0007_If it's wrong please tell me why._x0007__x0007_Thank you in advance",1,1,1
"First can you run `str(Auto)` to determine for sure that `horsepower` is numeric? Mine isn't after running only `library('ISLR')`. Second, one can convert to numeric as follows: _x0007__x0007_    Auto$horsepower <- as.numeric(Auto$horsepower)_x0007__x0007_Try this for the correlations:_x0007__x0007_    cor(Auto[ , c(1:8)])  # exculed 9th column \name\""""",1,0,1
"I got this right in the second attempt after reading this post. Hint is that not \resampling randomly\"" is not the same as \""not resampling\"". If n=2, then the permutations (1,1), (1, 2), (1, 1), (2, 2) are still valid. It does resampling but what you have here is all possible sets you can generate of size n and you use each of them only once to average over, as opposed to resampling randomly where you can take the same sample multiple times. Hope this helps and I am not completely giving up the answer.""",1,0,0
Look at my post above for hint.,1,0,1
Look at my post above for hint.,1,0,1
at BRM ... Thanks for clarifying. Now the question makes sense. :),1,0,0
"Hi, _x0007__x0007_When I looked at the question to answer it, I wasn't exactly sure what was meant. At one point, $k$ is used to denote the number of variables to be selected out of $p$ predictors, and later on it seems to be used as an index in selecting individual \best\"" predictors from the original $p$. Could someone explain what is being asked for, and if others agree about a lack of clarity with the question, could it be re-worded?_x0007__x0007_Thanks""",0,1,1
I guess that depends on how you setup the bootstrap function as input argument to the tsboot(). Then compute the mean of the distribution of s.e. ?,1,0,1
"Hi!_x0007__x0007_I have a problem with loading data into R._x0007_After executing subsequent code_x0007__x0007_    dat<-load(\5.R.RData\"")_x0007_    str(dat)_x0007__x0007_I got just chr \""Xy\""._x0007_What is wrong with my code?""",1,1,1
Thanks!,1,0,1
"Okey._x0007_After loading data with command in exercise description, one can access dataframe typing Xy.",1,0,1
Yeah you're right didn't see it mentioned in any of the videos.,1,0,1
Can someone help to derive this magic formula (5.2) (listed below) of Chapter 5 in the text?_x0007__x0007_$$mathrm{CV}_{n} = frac{1}{n}sum_{i = 1}^{n}(frac{y_i - hat{y}_i}{1 - h_i})^2$$,1,1,1
"Thanks acflippo and BRM! I was misinterpreting the question to allow only permutations of the original data set, because of the statement in the question \where two bootstrap data sets are different if they have the same data points but in different order.\""  Thanks to your posts I now see that samples with repeated instances of the same value *are* allowed (as one should expect from the fact that the procedure is bootstrapping, which by definition means with replacement).  _x0007__x0007_Apologies to anyone I may have mislead with my previous posts.""",1,0,0
"To compute the block bootstrap, you should consider the data as a time-series and use the function tsboot that is part of the boot package._x0007__x0007_The function tsboot requires two extra parameters, the size of the block and I used the parameter sim (simulation type as fixed)",1,0,0
"@timothy235 - yes, thanks for the great explanation.",1,0,1
"That's exactly what I arrived at independently. Thanks, Works!",1,0,1
"Did you have to modify the user defined statistic function?  I've tried using the same function that I used for 5.R.R3 (as per ISL on pg 195), but tsboot does not pass an index variable to the statistic function.  Been hacking at it all day with no success.",1,1,1
"ight._x0007_$_x0007__x0007_It's a model where there is definitely uncertainty --- we're_x0007_not sure if $Y$ will be 1 or 0.  We only know the_x0007_probabilities._x0007__x0007_If we had no explanatory variables at all that would be all_x0007_we could say._x0007__x0007_But if we have an explanatory variable, temperature, we can_x0007_have a model where the probability depends on temperature, _x0007_as in the following logit model:_x0007__x0007_$ _x0007_p_x0007_=_x0007_{_x0007_e^{(_x0008_eta_0 + _x0008_eta_1  T)}_x0007_over_x0007_1 + e^{(_x0008_eta_0 + _x0008_eta_1  T)}_x0007_}_x0007_$_x0007__x0007_Regarding your question on irreducible error:_x0007__x0007_Even if:_x0007__x0007_- the form of the model is perfectly correct_x0007_- and we have the true values of the parameters $_x0008_eta_0, _x0008_eta_1$_x0007__x0007_then we *still* have only a *probability* of buying_x0007_ice-cream at a given temperature.  Even if we predict the_x0007_category with the highest probability, our predictions will_x0007_not be perfect --- we will still have errors._x0007__x0007_I guess this is like the irreducible error._x0007__x0007_In fact, the true data is not a mathematical function of_x0007_temperature.  (Strictly for a function, a given input should_x0007_have a unique output.)_x0007__x0007_For a given temperature sometimes the person will buy_x0007_ice-cream and other times (at that same temperature) they_x0007_won't._x0007__x0007_----_x0007_**Linear Regression**_x0007__x0007_Consider a model for the *amount* of ice-cream as _x0007_function of temperature._x0007__x0007_The *population model* is:_x0007__x0007_$ _x0007_mathrm{Amount of ice cream}_x0007_=_x0007_{_x0007__x0008_eta_0 + _x0008_eta_1  T + epsilon_x0007_}_x0007_$_x0007__x0007_Here, the dependent variable, $Y$, is a quantitative_x0007_variable. _x0007__x0007_Regarding irreducible error:_x0007__x0007_Even if:_x0007__x0007_- the form of the model is perfectly correct_x0007_- and we have the true values of the parameters $_x0008_eta_0, _x0008_eta_1$_x0007__x0007_then the best prediction we could get, for the amount of_x0007_ice-cream at a given temperature, would be the (true_x0007_population) *mean* amount:_x0007__x0007_$_x0008_eta_0 + _x0008_eta_1  T$_x0007__x0007_We can't know the exact amount because of the random error_x0007_$epsilon$._x0007__x0007_Here also, the true data is not a mathematical function_x0007_(given input -> unique output) of temperature._x0007__x0007_For a given temperature the person will buy dif""",1,0,0
"The solution to Elements for Statistical Learning textbook are not \official\"".""",1,0,1
"use tsboot with l=100 and sim=\fixed\""""",1,0,1
Consider the following procedure. _x0007__x0007_For k = 1 to p {_x0007__x0007_Step 1: Find the k variables most correlated with y_x0007__x0007_Step 2: Fit a linear regression using those variables as predictors_x0007__x0007_}_x0007__x0007_True or false: a correct cross-validation procedure for Step 1 will possibly choose a different set of k variables for every fold.,1,0,0
"I'd say it in a different words using t-statistics (bottom of the 67 page of ISLR). _x0007_t = (bhat1 - b) / SE(bhat1). For 5% significance level t must be from -2 to 2. And out of these t and [-2;2] interval we get out b in [0.1 or 0.9]._x0007__x0007_P.S. Sorry, don't know how to write formulas here.",1,0,0
"See Equation 7.9 in Element of Statistical Learning (2nd ed., 10th printing). _x0007__x0007_http://statweb.stanford.edu/~tibs/ElemStatLearn/",1,0,1
"I think this has to do with equation 5.27 in Elements of Statistical Learning, 2nd ed., 10th printing. (Though good luck following that!) _x0007__x0007_In general, if you want some more mathematical derivation in ISL, a good place to check is ESL. _x0007__x0007_http://statweb.stanford.edu/~tibs/ElemStatLearn/",1,0,1
"Hadn't actually heard of $R^2$ of any kind for classification. It looks to be, like regression $R^2$, a kind of compared-to-the-null-model metric. Other popular classification metrics are F1-score, AUC, and depending on your application, precision, recall, etc. I personally like the arithmetic mean of the true positive rate and true negative rate since that works for imbalanced data sets quite well, where accuracy is not a good metric.",1,0,1
"The model is presented on page 8, and used on page 9.",1,0,1
@Timothy235 - I saw the Data Science Specialization also.  I am planning on taking those.,1,0,1
"Hello folks, I am a Web Analyst from Ahmedabad. Very excited to be a part of this course. :)",1,0,1
"I think this has been discussed quite a few times in this forum. For example: _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/52e548ce9b16c754bc<zipRedac><zipRedac><zipRedac><zipRedac>43_x0007__x0007_In general when dealing with dummy variables coding you should think in advance what you would like the estimated coefficients to represent (interpretation). For a rather extensive discussion on this topic (involving R), please follow:_x0007__x0007_http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm",1,0,0
"For every bootstrap data set, we can imagine it as a sequence consisting of n boxes. For every box, there are n possibilities because as we discussed above repetition like (1,1) and (2,2?are allowed. [SOLUTION REDACTED BY STAFF]_x0007__x0007_n=2: 11 12 21 22_x0007__x0007_n=3: 111 112 113 121 122 123 131 132 133 211 212 ... ...",1,0,0
"Thank you.  I missed the addition of the \Xy\"" variable""",1,0,1
"I'm not getting the data to appear either._x0007__x0007_> load(\F:/Statistical Learning (Stanford University)/Datasets/5.R.RData\"")_x0007_> summary(\""F:/Statistical Learning (Stanford University)/Datasets/5.R.RData\"")_x0007_   Length     Class      Mode _x0007_        1 character character _x0007_> names(\""F:/Statistical Learning (Stanford University)/Datasets/5.R.RData\"")_x0007_NULL_x0007__x0007_You get the same result using single quotes too:_x0007__x0007_> load('F:/Statistical Learning (Stanford University)/Datasets/5.R.RData')_x0007_> summary('F:/Statistical Learning (Stanford University)/Datasets/5.R.RData')_x0007_   Length     Class      Mode _x0007_        1 character character _x0007_> names('F:/Statistical Learning (Stanford University)/Datasets/5.R.RData')_x0007_NULL_x0007__x0007_How am I supposed to do this quiz when I can't get to the data???""",0,1,1
"I can't load the data too. Besides, how can we check whether loading is successful or not?",1,1,1
"when you code color as 1:yellow, 2:blue, 3:red, you're saying that blue has **twice** as much color as yellow.",1,0,1
Deriving that formula looks like a pain.,0,0,1
"With time series data, you also should determine the number of lags needed before the dynamics are no longer important._x0007__x0007_However, it would be nice if the staff (or profs) would respond on their thoughts.  The following seems to be one way:_x0007_http://robjhyndman.com/hyndsight/crossvalidation/",1,1,1
I wanted to thank the professors for EXPLICITLY demonstrating the right and wrong ways of using these important methods. This chapter alone should be **required reading** for all students interested in modeling. _x0007__x0007_I've seen many people violate and misunderstand these methods from prestigious engineering papers to internal business units.,1,0,1
"I would second that thought.  A dataset for homework would be great!  LASSO, support vector machine, and GAM will be better understood through data analysis.",1,1,1
I understand the hassle with grading.  But a homework without grading would be fine too.  We can discuss them here and have better understanding that way.,1,1,1
"Dear Prof Hastie and Tibshirani,_x0007_I think this is a great course.  I am curious if we could get some data analysis homework to apply LASSO, support vector machine, and so on.  It's very hard to understand these methods without actual practice on the data._x0007_Thanks!",1,1,1
the textbook has R labs.,1,0,1
"I ran a multiple linear regression analysis in R with two predictor variables. One predictor variable's distribution had a high skew and kurtosis so I did a transformation and took the log of those values to make the distribution appear normal. Then I ran the regression model and the response variable has positive and negative values. However, in reality the response variable cannot be negative. (for instance, predicting how many apples someone will eat can only be <zipRedac> or ><zipRedac>) Did I run this model incorrectly or is it fine to have these negative prediction values? _x0007__x0007_Here is the regression code: (residuals are normal with no corr to predicted values)_x0007_model1<- lm(response ~ predictor1 + predictor2)_x0007__x0007_Here is the result:_x0007__x0007_Coefficients:_x0007_                    Estimate Std. Error t value Pr(>|t|)    _x0007_(Intercept)          24.3465     4.133<zipRedac>   5.891 3.94e-<zipRedac>7 ***_x0007__x0007_predictor1      -5.1336     <zipRedac>.9434  -5.441 1.86e-<zipRedac>6 ***_x0007__x0007_predictor2   <zipRedac>.9955     <zipRedac>.2769   3.596 <zipRedac>.<zipRedac><zipRedac><zipRedac>774 ***_x0007__x0007_Residual standard error: 5.5<zipRedac>1 on 47 degrees of freedom_x0007_Multiple R-squared:  <zipRedac>.5<zipRedac>79,	Adjusted R-squared:  <zipRedac>.487 _x0007_F-statistic: 24.26 on 2 and 47 DF,  p-value: 5.784e-<zipRedac>8",1,1,1
"Within each \section\"" of a week there can actually be multiple videos and quizzes._x0007__x0007_Above each video, there's a sort of horizontal bar that you can use to move to the next \""thing\"" in that section. There are also arrows underneath the video that you can use._x0007__x0007_Typically, there's a small quiz as the second item in each section. Some sections (such as the R example sections) have multiple videos too.""",1,0,1
"did not like the wording of this exercise, my answer was 0.1. It clearly says: \...the Null hypothesis that B = b\"" which I read to mean How large can b be before we reject the Null hypothesis (B = b), so the answer is 0.1. Where did I go wrong?""",0,1,1
"After each lecture, there are usually questions.",1,0,1
Thank You!  Before this hint I squandered way too much time trying to create my own implementation.,1,0,0
Ordinary regression and LDA are equivalent in this case because they fit the same decision boundary.  The proof is just doing out the math to verify.,1,0,0
Classification has a fair amount of implementation details to keep track of.  It can make it seem daunting to learn at first.  I encourage you to keep on and hopefully things will become clearer as you get more exposure.  Make sure to post any questions you have and/or come by office hours!,1,0,1
"Hi,_x0007__x0007_when you use '=' you assign an argument to a variable._x0007_When you use '!=' or '==' you are comparing two variables or comparing the value of a variable to an argument. Here you need to use '==' to make a logical expression the function 'mean()' can work on.",1,0,0
thanks william! so to determine the actual predicted values i just take inverse log and subtract 1? thanks again!,1,0,0
"This isn't meant to be an analytic answer and could be just a coincidence. However in answering the question about the probability of a particular datapoint not being included in a boostrap sample, I calculated values for large samples (say size 100,000 or larger) and found that it converges to a value. This value is convenienly related to the one you're asking about since probabilities sum to one; all you have to do is take the formula you found to answer question 5.5, evaluate it with a large n and take its complement to 1 in order to find the probability of a data point being included in a boostrap sample. As I said, it might just be a coincidence but if it were it would be quite a remarkable one!",1,0,0
"I took the course \Introduction to Computational Finance and Financial Econometrics\"" from University of Washington on Coursera and highly recommend it if you are interested in investing and finance.The next course starts Feb 19. The first few lectures are pretty basic so you could take them concurrently with this course._x0007__x0007_https://www.coursera.org/course/compfinance_x0007__x0007_Learn mathematical and statistical tools and techniques used in quantitative and computational finance. Use the open source R statistical programming language to analyze financial data, estimate statistical models, and construct optimized portfolios. Analyze real world data and solve real world problems.""",1,0,1
when and where are the office hours?,1,1,1
I found chapter 4 on classification quite difficult but the slides were somewhat easier to follow as they did not have the math. I have fallen behind and need to catch up as some in the class seem far ahead of where I am now.,0,0,1
"A bit off topic, but something from Chapter 5 of ESL.  They write_x0007__x0007_More generally, an order-M spline with knots _x0018_j , j = 1, . . . ,K is a_x0007_piecewise-polynomial of order M, and has continuous derivatives up to_x0007_order M ? 2. A cubic spline has M = 4_x0007__x0007_So, this definition baffles me.  They say a cubic polynomial is a 4-spline but has order 3 and not 4.  So is it a piecewise polynomial of order at most M?",1,1,1
"The question for section \5.4 The Bootstrap\"", 5.4.R1, asks (paraphrased) how many bootstrap samples there are if the bootstrap consisted of averaging over all possible permutations of the original n observations. (At least that's how I am [mis?]interpreting the question.)_x0007__x0007_The problem is that when I try to type in \""n!\"" or variants such as \""n * (n-1) * ldots * 2 * 1\"", or \""Pi_i=1^n i\"" I get the error message \""Sorry, couldn't parse formula.\""_x0007__x0007_Has anybody found a way around this? Naturally, if I am barking up the wrong tree, I would appreciate some guidance.""",1,1,1
http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf,1,0,1
"in ddply because Smarket is a global object it does not always eval_x0007_uate to 1, but if you are not careful it can (and I been guilty of that)",1,0,1
"thanks a lot for the insight, I guess I was inverting the covariance matrix, then I decide to do it only once and was not careful enough - since it was not working I wondered if i needed to invert the mean matrix (and honestly I do not know if I should, the formula says mu of class k, so it should be the row of the means as outputed by lda, which may mean I do need to - I will try both and report back)",1,0,1
"I'm new to R.  It doesn't feel right to ask R questions in this community.  I'd appreciate recommendations of a forum where it would be appropriate.  _x0007_For example,_x0007_I have a data frame with historical values of the P/E ratio for the S&P 500 courtesy of Yale's Shiller.  I want to plot the P/E's conditional on interest rates and inflation rates to see if the P/E varies on these economic conditions.  One plot I'd like to make is a set of box plots with the P/E on the vertical axis and one of the variables along the horizontal with the data divided into bins.  Also, I'd like a 3D plot.",1,0,1
"I fixed the double inversion of s, but regardless of whether I take the transpose of x or not I can't match the output from lda (which I am 90% sure it is due to some scaling that I do not understand) - I should be able to match all the predictions but I do not - I must be missing something_x0007__x0007_I wish a TA would help",0,1,1
"My last idea was to look at the lda code, sometimes if a fucntion is written in R if you just type the name (lda in this case you see the underlying code_x0007__x0007_But sometimes if it is in C or Fortran you just see _x0007__x0007_function (x, ...) _x0007_UseMethod(\lda\"")_x0007_<bytecode: 0x7fdeca6adf80>_x0007_<environment: namespace:MASS>_x0007__x0007_I will keep trying, but i also have work and want to watch the bootstrap and CV lectures""",1,0,1
"On both the book an the vide the k-fold CV states the fold are equal, however the dagrams shows them to be clearly unequal_x0007__x0007_I believe equal is the answer, but want to validate which is true",1,1,1
Try a reproducible post on Stackoverflow:_x0007__x0007_http://stackoverflow.com/questions/<phoneRedac>/how-to-make-a-great-r-reproducible-example,1,0,1
"In the simulated samples that the bootstrap uses to estimate standard error, you don't take ever more data points (with replacement) from the original sample, you only take n.  So the answer should involve an n-th power where n is the number of data points.",1,0,0
"32K students signed up, with between 8K and 12K actively involved",1,0,1
But why is that the case? Does anyone know?,1,0,1
"For the first question, you need to write a function that returns the values you are looking for in your bootstrap - here, the coefficients from the linear regression.  You did the regression in part 1, so you can pull them with the coef() command - see page 195 in the textbook._x0007__x0007_When you call boot, you need to call the function you created as the second input.",1,0,0
"After the videos in ch4, i still don't understand why logistic regression is not stable with features well-separated while LDA is much suitable for this situation. Could someone give me an intuition for it?",1,1,1
Thanks! Helped me!,1,0,1
Hi everyone!_x0007__x0007_I'm a software engineer based in New York. I'm in love with all things data and would like to eventually take a degree in statistics or data science._x0007__x0007_Feel free to connect with me on LinkedIn:_x0007_http://www.linkedin.com/profile/view?id=<phoneRedac>50_x0007__x0007__x0007_-Jake,1,0,1
"Hopefully the following pictures will help in getting a_x0007_feel for this._x0007__x0007_----_x0007__x0007_First, here's a case where there is *not* perfect separation._x0007_At $x=3.5$ the $mathrm{Prob(Y=1)}=0.5$ (which makes_x0007_sense given the symmetry).  And the probability_x0007_decreases at lower values of $x$ and increases at_x0007_higher values of $x$.  There's a well defined fit which_x0007_gives the maximum likelihood.  _x0007__x0007_The circles are the 6 data points and the blue line is_x0007_the logit model predicted probabality:_x0007__x0007_$p = {_x0007_e^{(hat_x0008_eta_0 + hat_x0008_eta_1 x)}_x0007_over_x0007_{1+e^{(hat_x0008_eta_0 + hat_x0008_eta_1 x)}}_x0007_}_x0007_$_x0007__x0007_![not perfectly separated - logit works OK][1]_x0007__x0007_----_x0007__x0007_Now here's a case where there *is* perfect separation._x0007__x0007_The glm fit wasn't happy about this, and gave the_x0007_following warning message:_x0007__x0007_    Warning message:_x0007_    glm.fit: fitted probabilities numerically 0 or 1 occurred_x0007__x0007_Anyway, here's where it ended up with:_x0007__x0007_![perfectly separated - logit struggles][2]_x0007__x0007_----_x0007__x0007_In fitting, the *steepness* of the predicted_x0007_probability can be adjusted:_x0007__x0007_![logit model curves][3]_x0007__x0007_So in the second case, anything with an *infinite*_x0007_steepness and a transition somewhere between $x=3$ and_x0007_$x=4$ would fit the data *perfectly*.  This means the_x0007_the fit isn't well defined._x0007__x0007_The infinite slope would mean that the left 3 points_x0007_$mathrm{Prob}(y=1)=0$_x0007_while the right 3 points have_x0007_$mathrm{Prob}(y=1)=1$_x0007__x0007_With these probabilities, the observed sample data_x0007_would be *certain*, so that would be the maximum_x0007_likelihood._x0007__x0007_But the infinite slope parameter and lack of unique_x0007_solution create problems in the actual fitting._x0007__x0007_----_x0007__x0007_ **Note about LDA**_x0007__x0007_In the second case, with perfect separation, LDA should be fine._x0007__x0007_It will see two groups with equal prior_x0007_probabilities for each group. (3 points in each)_x0007__x0007_It will see the groups centred about different means:_x0007__x0007_- $y=0$ group with mean 2_x0007_- $y=1$ group with mean 5_x0007__x0007_and assume they both have the same standard deviation_x0007_(of 1)._x0007__x0007_So that if $x>3.5$ the probability will be higher for_x0007_the $y=1$ group and if $x<3.5$ the probability will be_x0007_higher for the $y=0$ group._x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>84.png_x0007_  [2]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>83.png_x0007_  [3]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>17.png",0,0,0
"For those interested in trying this out for themselves, _x0007_here's some R code for the plots above._x0007__x0007_(Hopefully it's self contained, so you can just_x0007_cut-and-paste to get started)_x0007__x0007_First plot (not perfect separation)_x0007__x0007_    data1 <- data.frame(x=c(1,2,3,4,5,6), y=c(0,0,1,0,1,1))_x0007_    plot(y ~ x, data=data1)_x0007_    glm.fit <- glm(y ~ x , family=\binomial\"", data=data1)_x0007_    glm.fit_x0007_    b0 <- glm.fit$coefficients[1]_x0007_    b1 <- glm.fit$coefficients[2]_x0007_    prob.curve.data <- data.frame(x=seq(1,6,by=0.01))_x0007_    prob.curve.data <- transform(prob.curve.data, prob=exp(b0+b1*x)/(1+exp(b0+b1*x)))_x0007_    plot(y ~ x, data=data1)_x0007_    lines(prob ~ x, data=prob.curve.data, col=\""blue\"")_x0007_    _x0007_Second plot (perfect separation)_x0007_    _x0007_    data2 <- data.frame(x=c(1,2,3,4,5,6), y=c(0,0,0,1,1,1))_x0007_    plot(y ~ x, data=data2)_x0007_    glm.fit <- glm(y ~ x , family=\""binomial\"", data=data2)_x0007_    glm.fit_x0007_    b0 <- glm.fit$coefficients[1]_x0007_    b1 <- glm.fit$coefficients[2]_x0007_    prob.curve.data <- data.frame(x=seq(1,6,by=0.01))_x0007_    prob.curve.data <- transform(prob.curve.data, prob=exp(b0+b1*x)/(1+exp(b0+b1*x)))_x0007_    plot(y ~ x, data=data2)_x0007_    lines(prob ~ x, data=prob.curve.data, col=\""red\"")""",1,0,1
"One of the things I thought about as I was going through the R exercises for using LDA to predict Smarkets direction is that one does not have to trade every day.  If you could understand which days represent situations that are really set up for up days (or down days) then you could keep your powder dry until one of those days happened to come by and then pounce on the opportunity._x0007__x0007_As I thought about how it might make sense to use the LDA Up and Down predictions to do so, I came up with the idea of looking for days where the absolute difference between probabilities of being in either k(given x) was > the .95 percentile and the prob of being in the predicted k (given x) was > .51.  This improved the proportion of correct guesses to .61-ish.  This rate may be represent a bad sample given I've only now got 13 yhat/y pairs on which to base the statistic AND in practical terms this means one only has 13 trading days (out of the original 252 for Smarket.2005), but if there is truly some \there\"" there, I would say most folks would take that trade off?_x0007__x0007_What I'm not sure about is the way in which I intuited this method and whether the absolute difference in probabilities of being in either k (given x) truly has the meaning which I'm inferring? Anyhow, the more I think about it, the more I find I'm talking myself out of thinking this is a good idea.  I think it falls more into the realm of total voodoo, but anyhow it was a fun exercise, and I offer up the code below..._x0007__x0007_    require(ISLR)_x0007_    require(MASS)_x0007_    lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005) _x0007_    Smarket.2005=subset(Smarket,Year==2005) # subset data where year = 2005_x0007_    lda.pred=predict(lda.fit,Smarket.2005) # use it as data to make predictions_x0007_    mean(lda.pred$class==Smarket.2005$Direction)_x0007_    df1 <- data.frame(lda.pred)_x0007_    df1$diff <- abs(df1$posterior.Down-df1$posterior.Up)_x0007_    quantile(df1$diff, .95) # the 95th percentile of the absoluet difference_x0007_    df1$max <- pmax(df1$posterior.Down, df1$posterior.Up)_x0007_    View(df1[ df1$diff > quantile(df1$diff, .95) & df1$max > .51 ,])_x0007_    df1p_gt.95 <- df1[df1$diff > quantile(df1$diff, .95)& df1$max > .51 ,]_x0007_    df2.test <- Smarket.2005[ df1$diff > quantile(df1$diff, .95) & df1$max > .51 ,]_x0007_    mean(df1p_gt.95$class==df2.test$Direction)_x0007_    table(df1p_gt.95$class, df2.test$Direction)_x0007_    data.frame(df1p_gt.95$class, df2.test$Direction)""",1,1,1
"I'm a machine learning player in Kaggle platforms. As i do practical problems and use cross validation(CV), my procedure is as follows:_x0007__x0007_(1) Split training dataset to sub-training dataset and validation dataset (about 90%-10% or 95%-5%)_x0007_(2) Use CV to select model parameters such as flexibility or feature selection_x0007_(3) Instead of using sub-training dataset, i train the model with the entire training dataset with the parameters selected by CV._x0007__x0007_With this procedure, the entire training dataset are used and we seems to have a reasonable model parameters. I want to know whether the procedure i use is reasonable or it is so dangerous that we should use sub-training dataset only.",1,1,1
"The tolerances in the scorer should be increased a bit. If you use lm to create a fit, then write an expression into R involving coef(fit)[3] and coef(fit)[4], the grader will mark you wrong. You need to use the rounded off coefficients from slide 37.",1,0,1
"I passed the original dataset to tsboot().  Since we are using the entire dataset, there doesn't seem to be a need to subset it.",1,0,1
"Use CV to do model selection, then, of course, use the entire data set to estimate the parameters of the model; i.e., once you've selected a model based on CV risk, use the entire data set to fit the final model.",1,0,0
"No,no, don't worry, it's not a convention :) it's just that for some examples they don't use field data, but simulated data from known distributions. I.e., they know exactly which are $pi_k$ and $f_k(x)$ for each $k$ , so they know which are all the terms in the Bayes theorem and they can exactly compute $P(Y=k|X=x)$ . Once you have this conditional probability law, then it's trivial to construct the Bayes classifier. But in real life you only get a sample of data from a population! You don't know which are the distributions that generated it. Sometimes, that's exactly one of the things you would like to find out! In the case of LDA, for example, you *assume* that the $f_k(x)$ are normal and share a common standard deviation, but you don't know for sure. Also, you have no idea which are the means $mu_k$ and standard deviation $sigma$ of these (assumed) normal distributions, and you don't know which are the $pi_k$. Of course you can *estimate* them from data, but you don't know for sure._x0007__x0007_By the way, one of the examples where LDA got so close to the Bayes Classifier was an example with two classes,where $f_1(x)$ and $f_2(x)$ were *chosen* to be normal with the same standard deviation. That's also a reason why LDA did so well :)",1,0,0
"Wow brilliant!!! Did you already know this stuff about logistic regression, or did you get this mastery of the topic just by following the course? I had an idea that fitting an exponential ratio may get into trouble, because all these \activation functions\"" can model very sharp transitions with relatively small variations in the parameters. However, I couldn't devise such a clear picture. Thanks a lot!""",1,0,1
"Thank you, that makes sense, I had a rough guess but your explanation helps make it more clear.     _x0007_I was wondering in LDA, if this ever happens, what if one distribution is normal, but another is something non-bell, like geometric (the one that starts high at zero and decreases)? But I suppose that would be for a higher level course, the math probably gets really tough. I suppose in that case, the boundary between the bell and the non-bell could be curved.",1,0,0
"I, too, feel that some of the questions are not directly related to the lecture material (perhaps some are \tricky\"" and so on), and I enjoy it for the following reasons:_x0007__x0007_1. Its a good opportunity to think out of the box and extend what you have (presumably) learned._x0007__x0007_2. The answers provide some food for thought, even if you have selected the right answer e.g. alternative explanation/ point of view._x0007__x0007_3. Independently of being right or wrong, there may be considerable effort in \""unpacking\""/ rationalising the provided correct answer._x0007__x0007_4. The ones you get wrong (perhaps just the first time around or all five times) will stick with you! _x0007__x0007_Hope this makes for a more positive/ optimistic view of the learning process...""",1,0,1
"ight)x+frac{hat{mu}_2^2-hat{mu}_1^2}{2hat{sigma}^2}+logfrac{pi_1}{pi_2}$_x0007__x0007_So $frac{hat{mu}_1-hat{mu}_2}{hat{sigma}^2}$ takes the place of $hat{_x0008_eta}_1$, but unlike $hat{_x0008_eta}_1$ the LDA version is restricted to have the form of $frac{hat{mu}_1-hat{mu}_2}{hat{sigma}^2}$ whose parameters are estimated with further restrictions. _x0007__x0007_So loosely speaking these restrictions help to restrict the possibilities for the estimated slope to only one in the case of perfect separation.""",1,0,1
df1 not found,1,0,1
I couldn't get the right answer for this. If you want blocks don't you need to use [tsboot][1]? However even using tsboot I can't get the right answer?_x0007__x0007_  [1]: http://astrostatistics.psu.edu/su07/R/library/boot/html/tsboot.html,1,1,1
"Hi,_x0007_I think that the legend and comment of the picture are backwards. Maybe I am wrong._x0007_[Here is the way I found it.][1]_x0007__x0007__x0007_  [1]: https://dl.dropboxusercontent.com/u/1<phoneRedac>8/code.R",1,0,1
"Actually, in the book on top of page 68 it is explicitly stated: \we declare a **relationship to exist between X and Y** — if the p-value is small enough\"". Therefore, this question should be either removed or asked differently. Or the book should be fixed.""",0,1,1
"Hello,_x0007__x0007_Question 5.R.R2 tells us to use matplot(Xy,type=\l\"") and asks if the s.e. estimate of B1 is low/high or ok. I understand that X1 and X2 are correlated (seem to roughly oscillate in unison with a slight offset as seem by the plot). What I cannot understand is how this correlation explains a low s.e. estimate for B1 in the case of linear regression. _x0007__x0007_Can anyone explain this?""",1,1,1
"In k-fold CV we fit the model k times to estimate the test error. As I understood, the test error not only depends on the flexibility of the model but also on the coefficients. However, here we have k sets of coefficients!!",1,1,1
I miss also a structured and step by step cases of using discriminant analysis._x0007_Are the assumptions (normal/Gaussian) important? _x0007_How close to Normal should be?_x0007_What if some variables are Normal and others not? _x0007_Can i go further with this kind of analysis?_x0007_How should i know that the results are not what we expect? _x0007_Do i need to do some corrections?_x0007_Or if i am far away from normal use another method?_x0007_Or maybe i should use another method from the beginning and discriminant analysis is just for our information...,1,1,1
"@greendust Yes, `tsboot()` passes the data already resampled on to the function supplied to the `statistic` argument. Hence you need to modify your function so that it just works on the supplied data object.",1,0,1
"Thanks, Daniel and Kevin. After re-reading the question and knowing from Kevin's reply that the answer was simple, I figured it out. I just wonder why the question simply have been stated as \How many bootstrap samples are there, if sample order matters?\""_x0007__x0007_It also helped to think about submitting an answer in R syntax, rather than TeX/LaTeX. (Still, there are limits: neither `cumprod(1:n)` nor `gamma(n+1)` worked for $n!$, back when I thought the question asked for how many ordered bootstrap samples there are without replacement...)_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,0,0
"Just one little question: why do you use [[\X1]], instead of [\""X1]? Is there any difference?""",1,1,1
"Because there are $k$ models. Each of the models is fitted to a smaller subset ($k-1$ chunks) of the data. Further, as each of the $k$ models is fitted to a different set of data, the estimates of the coefficients of the model under test will vary with different estimates being obtained for each of the $k$ models._x0007__x0007_However, the set of coefficients for the $k$th model **is** the $k$th model, so it might be easier to think of this as there being $k$ models, each an estimate of the true data generating process. Each of the $k$ models supplies a fitted/predicted value for each observation in the test set, which are then averaged to give the $k$-fold CV estimate for each test observation.",1,0,0
"I don't see why you think that. The Quiz question for video 3 pertains to the issue of doing CV the *wrong* way, which is discussed in Video 3. The Quiz for Video 2 is a general question about CV, which is the topic of Video 2._x0007__x0007_If you still think these are associated with the wrong video could you expand upon why you believe this is so?",0,1,1
"BRM explanation is right, and, yes,(1,1) should be (2,1) (obviously is a mistype). He almost gives the answer (the key is permutation with repetition)",1,0,1
Thank you very much timothy235. I got it,1,0,0
"I edited the code int he original post and just checked ti to make sure it ran, and it looks like it works now, thanks for the catch Jim!",1,0,1
"those files are the ones used by the prof in the video. if you have R and RStudio installed, you can load those file in the RStudio environment and play with it.  if you just click on the file, i think your OS just picks the application than understands file with a .R extension. i have Xcode installed on my Mac so Xcode brings up the .R file.  i have no issue installing R and RStudio on my Mac running Snow Leopard.",1,0,1
"I'm going to think about your question in terms of a social scientist...Sure, I don't see any problem with it. What if, for example you had an observations zip code and not their income, you could find a census file online and merge the relevant data by zip.  I can see other instances where an observations behavior may be influenced by macro environmental factors, like employment rate, consumer sentiment and such (esp. in a time series/dynamic regression setting).",1,0,1
"I am curious if the prof could give us, say, a one page homework assignment like a grad class.  I think it's fine if no answer is given.",1,1,1
"It probably does, but I find those to be easy and straightforward.  I don't find that stimulating my brain.  What does the profs give to his student at Stanford?",1,1,1
"what is  default refer to, in logistic regression example",1,1,1
"Hey Ben, I live on the east side (redmond area).  Nice to see a fellow PNW-er.",1,0,1
"When I've looked at things like this myself, for the point of building a trading system, the problem is it's too simplistic to classify as 'up' vs 'down'. A lto of times, the 'up's that are correctly caught are very small, like random noise, and the few incorrect 'up's are quite a bit larger. It might be better to set the categoricals as 'down3%', 'down2%', 'down1%', with ups, grouping the returns into intervals. This might help with above problem.    _x0007_Another R function to look into is autocor (or autocorr) for autocorrelation.     _x0007_(Note -- I haven't done anything successful with this, so I certainly am no expert to give advice or guidance !! )",1,0,1
"I removed the \subset=index\"" of the original function. And it seems to be working. The problem is that I don´t understand why... ja. _x0007__x0007_Besides, the result is not the same of the answer, but I got it right anyway.""",1,0,0
"Agreed... simply allowing posters to organize responses by selecting a Lecture or some other subjects \general interest\"", etc., would be huge.  This is a good problem to have (I must say); I'm thinking that no one expected the kind of participation that you have here.""",1,1,1
"I don't really understand that: _x0007__x0007_\True or false: a correct cross-validation procedure will possibly choose a different set of k variables for every fold.\""_x0007__x0007_If we vary the set of k variables from 1 to p, does that mean we have $2^p$ folds? And how does the cross-validation error tell us which set of k variables to choose?""",1,1,1
"I am not able to load the 5.R.RData file into RStudio for the quiz.  I am using the load command given in the question.  It seems to accept the command, but I can't see the data matrix.  When I type y or x it does not find the object._x0007__x0007_Can anyone tell me what I am doing wrong?",1,1,1
"Hi,_x0007__x0007_one possible cause for getting 0s for std might be the way your function f (input to tsboot) is written. Be sure to use the same variable names for the function's input and what is in the function itself. _x0007__x0007_For instance the following function gives all 0s:_x0007__x0007_bootFunc <- function(myData, myIndex){_x0007_  return(coef(lm(y~X1 + X2, data=Xy[myIndex, ])))_x0007_}_x0007__x0007_tsboot(Xy, bootFunc, R=1000, l=100, sim=\fixed\"")_x0007__x0007__x0007_while the following works_x0007__x0007_bootFunc <- function(myData, myIndex){_x0007_  return(coef(lm(y~X1 + X2, data=myData[myIndex, ])))_x0007_}_x0007__x0007_tsboot(Xy, bootFunc, R=1000, l=100, sim=\""fixed\"")""",1,0,0
You can resample as many times as you want regardless the number of blocks in your data. So R can be any integer theoretically.,1,0,1
"In ISLR, 5.3.1, the predict() is done on the whole dataset, and then the values to do with train thrown away ([-train]), then take the mean of the square to get the MSE._x0007__x0007_Why couldn't/wouldn't you take the predict() of the test dataset only, and then mean the square?  [That would give you ...mpg-predict(lm.fit,Auto[-train]))^2)  , i.e. [-train] inside the bracket  ]_x0007__x0007_The second way gives a lower MSE, but I can't fathom out why._x0007__x0007_If it's obvious, would appreciate being put out of my misery :-)",1,1,1
"For those of you who (like me) aren't that familiar with R, i will point out a feature of the language that may be helpful with this exercise (though sadly discovered too late to do me any good)._x0007__x0007_Consider the linear model fit in answer to question 1; call it `lm.fit`.  Now take a look at `coef(lm.fit)`, in particular take a look at `coef(lm.fit[2])`.  It might be that **coef**, giving you the coefficients of the fit, will be of use to you in parts 3 and 4.",1,0,1
"Not an answer but I believe I have followed all of the steps in the 5.3.1 tutorial but I get the following results from what seems to be a correct typing of the mean command:_x0007__x0007__x0007_----------_x0007__x0007__x0007_> mean((mpg~predict(lm.fit,Auto))[-train]^2)_x0007__x0007_Error in (mpg ~ predict(lm.fit, Auto))[-train]^2 : _x0007_  non-numeric argument to binary operator_x0007__x0007__x0007_----------_x0007__x0007__x0007_Can anyone help?_x0007_Thanks in advance",1,1,1
"cool, I did not know this was possible - I am not sure if I will get to the bottom of the lda thing, but this tip will come in handy :)_x0007_Cool Blog by the way",1,0,1
"The command load(\5.R.RData\"") makes a data frame_x0007_called \""Xy\"" available._x0007__x0007_The individual variables, like \""y\"" are *inside* the_x0007_\""Xy\"" data frame._x0007__x0007_    > load(\""5.R.RData\"")_x0007_    > ls()_x0007_    [<zipRedac>] \""Xy\""_x0007__x0007_    > head(Xy)_x0007_            X<zipRedac>        X<zipRedac>         y_x0007_    <zipRedac> <zipRedac>.<zipRedac>977<zipRedac>0 0.<phoneRedac> 0.<phoneRedac>_x0007_    <zipRedac> <zipRedac>.<zipRedac>673<zipRedac>3 0.<phoneRedac> 0.<phoneRedac>_x0007_    3 <zipRedac>.<zipRedac>3688<zipRedac> 0.<phoneRedac> 0.<phoneRedac>_x0007_    4 <zipRedac>.<zipRedac>063<zipRedac>7 0.<phoneRedac> 0.<phoneRedac>_x0007_    5 <zipRedac>.<zipRedac>75553 0.<phoneRedac> 0.<phoneRedac>_x0007_    6 <zipRedac>.<zipRedac>445<zipRedac>3 0.<phoneRedac> 0.<phoneRedac>_x0007__x0007_If you really want to \""get at\"" one of the individual_x0007_variables, you can use \""$\"" to get that column._x0007_    _x0007_    > head(Xy$y)_x0007_    [<zipRedac>] 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac>_x0007__x0007_If you want to refer to the variables without the data_x0007_frame, you could use the \""attach\"" command:_x0007_    _x0007_    > attach(Xy)_x0007_    > head(y)_x0007_    [<zipRedac>] 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac> 0.<phoneRedac>_x0007__x0007_The \""attach\"" command will cause problems if you already have a_x0007_variable called \""y\"" (or any of the other column names), or_x0007_if you later define a variable called \""y\""._x0007__x0007_So it might be safest to avoid it if you can._x0007__x0007_----_x0007__x0007_Note: in all the above, the \""head\"" command is used to_x0007_\""peek\"" at the first few rows. (Otherwise all <zipRedac>000 rows_x0007_would be printed out, which is fine, but it would take_x0007_up a lot of space)""",1,0,0
"This is great, many thanks!",1,0,1
"It should be minus \-\"" instead of tilde \""~\""""",1,0,1
"I have solved 5.R.R3, I kind of understand the concept of block bootstrap,but I do not know how to write the R code for this question.",1,1,1
I'm afraid I do not think so.,1,0,1
"For 3d plots, take a look at the rgl package and the functions plot3d or rgl.surface for instance",1,0,1
<redacted>_x0007__x0007_Maybe a bit of both._x0007__x0007_I knew about logistic regression.  But I hadn't really_x0007_thought about this issue of logistic classification with_x0007_perfectly separated groups.  So when it was mentioned in the_x0007_course it was interesting.  And trying to c,1,0,1
Do you know about the `tsboot()` function? (part of the boot package),1,0,1
"Hi Danie, could you explain more about that function? Shoud I modify tseries and statistic in parameters? Thanks.",1,0,1
Thanks for the answer and does it include the videos ?,1,0,1
Try '?tsboot' to get help on its function._x0007_You will see that it works much like 'boot'.,1,0,1
I do not have R installed in my machine. Can you please post questions that can be solved/resolved analytically.,1,1,1
"What's the difference between tsboot and boot with strata parameter like _x0007__x0007_    Xy$estrato <- rep(1:10,each=100)_x0007_boot(Xy,lm.boot,R=999, strata=Xy$estrato)_x0007__x0007_I thought what strata parameters in boot function take a sample in each strata and put together before estimate coefficients in lm model.",1,1,1
You can guide me on how to make block bootstrap? I don't find any information about this topic in the recommended book._x0007_Thanks,1,1,1
"Or you can set the default:_x0007__x0007_    options(digits=7)_x0007__x0007_Although this affects the total number of digits displayed--not necessarily the number of digits after the decimal point. (It's also just a \suggestion\"" according to the docs. I suppose some R outputs might decide to override it.)""",1,0,1
You can download the videos now and watch them whenever you want later.,1,0,1
Thank you so much.  I now have the data loaded.,1,0,1
Thank you alhf and jiza.  It now works.,1,0,0
"Maybe it's interesting for those who aren't familiar with R, make intensive use of str() function. If you apply it over any object, it provides very useful information... In your example, try:_x0007__x0007_str(lm.fit)_x0007__x0007_str(summary(lm.fit))",1,0,1
It works for me too. They're a really likeable pair and get it just right.,1,0,1
"I found an answer that works, after looking at the thread \Quiz 5.4.R1 Question\"" I also realized the question is asking for a count of samples of size n only - at first I thought it was looking for samples of size 1 plus samples of size 2 etc. up to samples of size n.""",1,0,0
"Check out the other threads on this question, which helped me out. There should be no ellipsis (...), since only samples of size n are considered (NOT samples of size 1 plus samples of size 2 ... plus samples of size n). Also, the counting scheme is \permutations\"" and not \""combinations\"", as order of selection matters in the counting. Based on these assumptions, it accepted my answer as right (after getting some frustrating \""can't parse the formula\"" errors.)""",1,0,1
"FWIW, my view is as follows:_x0007_1. The quizzes are \light\"" in the sense they don't require a lot of time. For anyone who is very busy and for whom a high learning-to-time-spent ratio is important, this is good. I'm definitely learning a lot._x0007__x0007_2. Some of us with PhDs in theoretical fields (mine is pure math) bring a mindset of \""define the problem with total precision and work out the logic of every detail\"", whereas this course is APPLIED statistics, so it's necessary to adapt. I see a trade off (you learn more in less time) if you're willing to let go of some details for now.""",1,0,1
"I joined the course a week late, but was able to catch up fast thanks to the excellent material posted online.",1,0,0
"I'm not a journalist, but I've seem some of the quality work coming out of ProPublica. It seems journalists need to be data scientists now. How the world has changed!",1,0,1
"Hi,_x0007_I'm having trouble with R code and the tsboot function.  Specifically, passing arguments to my statistic function nested within the tsboot function. Here's what I have **first:**_x0007__x0007_**se=function(data,index){_x0007_  coef(summary(lm(y~.,data=data,subset=index)))[,\Std. Error\""]_x0007_}**_x0007__x0007__x0007_I get into trouble because 1) I do not know how to pass the data into this function from the tsboot function and 2) I think the index argument is (will be) a problem since tsboot samples the data (if that makes sense).  The function returns Standard Errors for the coefficients of a linear model. _x0007__x0007_**Second:**_x0007__x0007_**tsboot(Xy[,3],se,R=10,l=100,sim=\""fixed\"")**_x0007__x0007_I get the following: *Error in terms.formula(formula, data = data) : _x0007_  '.' in formula and no 'data' argument*_x0007__x0007_I interpret that to mean the se function is not getting the data (and next it won't get the index, but it shouldn't need the index because tsboot is sampling the data - *stream of consciousness*).  I thought about wrapping tsboot inside another function in order to pass the arguments to the se function, but that seems too complicated._x0007__x0007_So I'm stuck.  I must be missing something simple, but what?""",1,1,1
"Thank you indeed for sharing these resources._x0007_Only a little note...on \notes\"": _x0007_often in the quizzes, references are made to certain pages in the notes, but it is unclear to me where these can be found. I looked into the Ppoint presentations with no avail. _x0007_Perhaps it would be better to indicate the missing info (required to answer the question) explicitly within the text of the quiz._x0007_Thank you again""",1,1,1
"Forgot, for example, \page 30 of the notes\"" in question 2.4.R1_x0007_Thanks""",1,0,1
"It's simpler than you think_x0007__x0007_se=function(data){coef(lm(y ~ ., data))}_x0007__x0007_tsboot(Xy,se,R=1000,l=100,sim=\fixed\"")_x0007__x0007_will show the standard errors for Beta0, Beta1 and Beta2.""",1,0,0
"There are many kinds of data in \Datasets\"" Package._x0007__x0007_Try: _x0007__x0007_library(help = \""datasets\"")""",1,0,1
"Thank you so much for letting us know professor Hastie!._x0007_I learned through the forums of a coursera course that the average number of students that finish a MOOC is now about 5% of the number that registered. I really hope we all complete this one to turn it into \an outlier\""!""",1,0,1
"DOH! Thank you. You're right, it was MUCH simpler than I thought. (I knew I didn't need that index variable.)",1,0,1
"They are looking for \largest value of b for which we would NOT reject the null hypothesis\"".  What you have is the value at the lower end, not the larger end.  Again, they are looking for the \""largest value\"".""",1,0,1
"Apart from the datasets package, you can also find some data sets in the book website (http://www-bcf.usc.edu/~gareth/ISL/data.html).",1,0,1
"> install.packages(\ISLR\"")_x0007__x0007_> library(ISLR)_x0007__x0007_Then read the help file:_x0007__x0007_http://cran.r-project.org/web/packages/ISLR/ISLR.pdf""",1,0,1
"jiza, Thanks for going through this.  I hadn't noticed that X1 and X2 were uppercase.  duh!",1,0,0
data(package = .packages(all.available = TRUE)),1,0,1
"* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;_x0007_* No warranty.  :-);_x0007_* Sorry about the formatting. It appears to be automatic.;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 5.3.1 The Validation Set Approach *****;_x0007_title '5.3.1 The Validation Set Approach';_x0007__x0007_* csv file created from Auto dataset (> write.csv(Auto,file=\Auto.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_* note: sas imports horsepower as character;_x0007_proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_data Auto;_x0007_   set Auto;_x0007_   horsepower = horsepowerchar + 0;_x0007_   drop horsepowerchar;_x0007_   horsepowersq = horsepower**2;_x0007_   horsepowercb = horsepower**3;_x0007_run;_x0007__x0007_* randomly sample 196 obs for the training and test datasets & fit first order model to training data;_x0007_* > set.seed(1)_x0007_  > train=sample(392,196)_x0007_  > lm.fit=lm(mpg~horsepower,data=Auto,subset=train);_x0007_ods listing close;  * supress printed output;_x0007_proc glmselect data=Auto seed=1;_x0007_   title2 'Seed = 1, 1st Order Model';_x0007_   model mpg = horsepower / selection=none;  * keep all variables in the model;_x0007_   partition fraction(test=0.5);  * randomly split the data in half between training & test;_x0007_   ods output fitstatistics=FitStats1;_x0007_run;_x0007_ods listing;_x0007__x0007_* estimate MSE;_x0007_* > mean((mpg-predict(lm.fit,Auto))[-train]^2);_x0007_proc print data=FitStats1 noobs;_x0007_   where substr(Label1,1,3) in ('ASE');_x0007_run;_x0007__x0007_* repeat for quadratic model;_x0007_* > lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train);_x0007_ods listing close;_x0007_proc glmselect data=Auto seed=1;_x0007_   title2 'Seed = 1, 2nd Order Model';_x0007_   model mpg = horsepower horsepowersq / selection=none;_x0007_   partition fraction(test=0.5);_x0007_   ods output fitstatistics=FitStats2;_x0007_run;_x0007_ods listing;_x0007__x0007_* > mean((mpg-predict(lm.fit2,Auto))[-train]^2);_x0007_proc print data=FitStats2 noobs;_x0007_   where substr(Label1,1,3) in ('ASE');_x0007_run;_x0007__x0007_* repeat for cubic model;_x0007_* > lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train);_x0007_ods listing close;_x0007_proc glmselect data=Auto seed=1;_x0007_   title2 'Seed = 1, 3rd Order Model';_x0007_   model mpg = horsepower horsepowersq horsepowercb / selection=none;_x0007_   partition fraction(test=0.5);_x0007_   ods output fitstatistics=FitStats3;_x0007_run;_x0007_ods listin""",1,0,0
"* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;_x0007_* No warranty.  :-);_x0007_* Sorry about the formatting. It appears to be automatic.;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 5.3.1 The Validation Set Approach *****;_x0007_title '5.3.1 The Validation Set Approach';_x0007__x0007_* csv file created from Auto dataset (> write.csv(Auto,file=\Auto.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_* note: sas imports horsepower as character;_x0007_proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_* randomly sample 196 obs for the training and test datasets;_x0007_* > set.seed(1)_x0007_  > train=sample(392,196);_x0007_proc surveyselect data=Auto method=srs sampsize=196 rep=1 seed=1 out=AutoSelect1 outall noprint;_x0007_   * 'method=srs' specifies without replacement;_x0007_   * 'outall' option keeps all data in output dataset and adds 'selected' variable (0,1);_x0007_run;_x0007__x0007_data AutoTraining1 AutoTest1;_x0007_   set AutoSelect1;_x0007_   horsepower = horsepowerchar + 0;_x0007_   drop horsepowerchar;_x0007_   horsepowersq = horsepower**2;_x0007_   horsepowercb = horsepower**3;_x0007_   if Selected = 1 then output AutoTraining1;_x0007_     else output AutoTest1;_x0007_run;_x0007__x0007_* fit first order model to training data;_x0007_* > lm.fit=lm(mpg~horsepower,data=Auto,subset=train);_x0007_proc reg data=AutoTraining1 outest=RegEst1 noprint;_x0007_   title2 'Seed = 1, 1st Order Model';_x0007_   mpghat: model mpg = horsepower;_x0007_   * label the model 'mpghat' for naming predicted column in score outout dataset;_x0007_run;_x0007__x0007_* apply model to test data and estimate MSE;_x0007_* > mean((mpg-predict(lm.fit,Auto))[-train]^2);_x0007_proc score data=AutoTest1 score=RegEst1 type=parms out=ScoreOut1;_x0007_   var horsepower;_x0007_run;_x0007__x0007_data ScoreOut1;_x0007_   set ScoreOut1;_x0007_   SqError = (mpg - mpghat)**2;_x0007_run;_x0007__x0007_proc means data=ScoreOut1 mean;_x0007_   var SqError;_x0007_run;_x0007__x0007_* repeat for quadratic model;_x0007_* > lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train);_x0007_proc reg data=AutoTraining1 outest=RegEst2 noprint;_x0007_   title2 'Seed = 1, 2nd Order Model';_x0007_   mpghat: model mpg = horsepower horsepowersq;_x0007_run;_x0007__x0007_* > mean((mpg-predict(lm.fit2,Auto))[-train]^2);_x0007_proc score data=AutoTest1 score=RegEst2 type=parms out=ScoreOut2;_x0007_   var horsepower horsepowersq;_x0007_run;_x0007__x0007_data ScoreOut2;_x0007_   set ScoreO""",1,0,0
"* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;_x0007_* No warranty.  :-);_x0007_* Sorry about the formatting. It appears to be automatic.;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 5.3.3 k-Fold Cross-Validation *****;_x0007_title '5.3.3 k-Fold Cross-Validation';_x0007__x0007_* csv file created from Auto dataset (> write.csv(Auto,file=\Auto.csv\"");_x0007_* note: need to delete 1st column in resulting csv file before importing into sas;_x0007_* note: sas imports horsepower as character;_x0007_proc import out=Auto(rename=(horsepower=horsepowerchar)) dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookAuto.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_* > set.seed(17)_x0007_  > cv.error.10= rep(0,10)_x0007_  > for(i in 1:10){_x0007_  + glm.fit=glm(mpg~poly(horsepower,i),data=Auto)_x0007_  + cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]_x0007_  + }_x0007_  > cv.error .10;_x0007__x0007_data Auto;_x0007_   set Auto;_x0007_   horsepower1 = horsepowerchar + 0;  * convert horsepower to numeric;_x0007_   * create horsepower squared, cubed ... 10th;_x0007_   array h{9} horsepower2-horsepower10;_x0007_   do j = 2 to 10;_x0007_      h{j-1} = horsepower1**j;_x0007_   end;_x0007_   drop j;_x0007_   fold = ranuni(1);  * needed to create folds using proc rank below;_x0007_run;_x0007__x0007_* create dataset for k=10;_x0007_* note: use of proc surveymeans with rep option will create datasets for RANDOM k-fold cross validation;_x0007_proc rank data=Auto out=AutoRanked groups=10; * divides data into k=10 folds;_x0007_   var fold;  * fold is assigned a value from 0 to 9;_x0007_run;_x0007__x0007_%macro KFCV;  * macro to loop thru each of the k=10 validation datasets and  increasing complex polynomial fits;_x0007__x0007_%do PolyOrder = 1 %to 10;  * loop thru increasing complex polynomial fits;_x0007__x0007_data MSEs&PolyOrder;  * blank dataset to append MSEs (from each of the k=10 validation datasets) to;_x0007_run;_x0007__x0007_   %do mfold = 0 %to 9;  * loop thru each of the k=10 validation (held-out) datasets;_x0007__x0007_* ensure that data from a previous loop is not used;_x0007_proc datasets memtype=data library=work; _x0007_   save AutoRanked MSEs&PolyOrder;_x0007_run;_x0007__x0007_* fit model on non-held-out folds;_x0007_proc reg data=AutoRanked outest=RegEst noprint;_x0007_   where fold ne &mfold;_x0007_   title2 \""Order &PolyOrder Model\"";_x0007_   mpghat: model mpg = %do i = 1 %to &PolyOrder; horsepower&i %end;;  * loop adds higher order variables each time thru;_x0007_   * label the model 'mpghat' for naming predicted column in score outout da""",1,0,0
"* I created SAS programs to duplicate the output from the R code in the Chapter 5 Lab.;_x0007_* No warranty.  :-);_x0007_* Sorry about the formatting. It appears to be automatic.;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 5.3.4 The Bootstrap *****;_x0007_title '5.3.4 The Bootstrap';_x0007__x0007_* csv file created from Portfolio dataset (> write.csv(Portfolio,file=\Portfolio.csv\"");_x0007_* note: need to add \""index\"" to 1st row in resulting csv file before importing into sas;_x0007_proc import out=Portfolio dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookPortfolio.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_* > alpha.fn=function (data ,index){_x0007_  + X=data$X [index]_x0007_  + Y=data$Y [index]_x0007_  + return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))_x0007_  + };_x0007_* > alpha.fn(Portfolio ,1:100);_x0007_* get variances & covariances;_x0007_ods listing close;_x0007_proc corr data=Portfolio cov;_x0007_   var X Y;_x0007_   ods output cov=CovMatrix1;_x0007_run;_x0007_ods listing;_x0007__x0007_* calculate alpha;_x0007_data Alphas1(keep=alpha where=(alpha ne .));_x0007_   set CovMatrix1;_x0007_   lagX = lag(X);  * lag function cant be used after a if-then;_x0007_   if Variable = 'Y' then do;_x0007_      VarX = lagX; VarY = Y; CovXY = X;_x0007_      alpha = (VarY - CovXY)/((VarX + VarY) - (2*CovXY));_x0007_   end;_x0007_run;_x0007__x0007_proc print data=Alphas1 noobs;_x0007_   title2 'Portfolio Data: All Data';_x0007_run;_x0007__x0007_* > set.seed(1)_x0007_  > alpha.fn(Portfolio,sample(100,100,replace=T));_x0007_* create sample of 100 obs w/ replacement;_x0007_proc surveyselect data=Portfolio method=urs sampsize=100 rep=1 seed=1 out=PortfolioSelect2 noprint;_x0007_   * 'method=urs' specifies with replacement and adds 'NumberHits' variable which MUST BE USED IN ALL SUBSEQUENT PROCS;_x0007_run;_x0007__x0007_ods listing close;_x0007_proc corr data=PortfolioSelect2 cov;_x0007_   var X Y;_x0007_   freq NumberHits;  * MUST BE USED IN ALL PROCS;_x0007_   ods output cov=CovMatrix2;_x0007_run;_x0007_ods listing;_x0007__x0007_* calculate alpha;_x0007_data Alphas2(keep=alpha where=(alpha ne .));_x0007_   set CovMatrix2;_x0007_   lagX = lag(X);  * lag function cant be used after a if-then;_x0007_   if Variable = 'Y' then do;_x0007_      VarX = lagX; VarY = Y; CovXY = X;_x0007_      alpha = (VarY - CovXY)/((VarX + VarY) - (2*CovXY));_x0007_   end;_x0007_run;_x0007__x0007_proc print data=Alphas2 noobs;_x0007_   title2 'Portfolio Data: Single Bootstrap Data';_x0007_run;_x0007__x0007_* > boot(Portfolio,alpha.fn,R=1000);_x0007_* sample 100 ods w/ replacement 1000 times;_x0007_proc surveyselect data=Portfolio method=urs sampsize=100 rep=1000 seed=1 out=P""",1,0,0
"For `tseries`, put your data. For `statistic`, put a function like the one in 5.R.R3, but it shouldn't take an index so you might need to modify what you had in 5.R.R3 a little.",1,0,1
"I was curious about the requirement that the answers in 5R parts 3 and 4 be \within 10%\""... of what, exactly?_x0007__x0007_It looks to me like the data are so non-linear that in fact one will get an answer > 10% of the given solution about 3% of the time, for part 3 (assuming a random selection of seeds for the RNG, set prior to attempting the solution.)_x0007__x0007_I collected the std error of `B1` for 300 trials of `boot(...R=1000)`.  The nominal solution (Show Answer) is in fact one of lowest values you will ever see, way over on the *left* in the 1st quintile; i.e., **the published answer is greater than 2 std deviations away from the mean** of this dataset.  So, while answers **not within** 10% of the official value are none too likely, they are certainly possible.""",1,1,1
"Orthogonal polynomials means that $x$ and $x^2$ created will be uncorrelated. Simply squaring $x$ will result in a variable that is strongly correlated with $x$, which is what `I(x^2)` does. The course has talked about issues of having correlated predictor variables ; in general these can be problematic for the fitting function and interpretation of the $hat{_x0008_eta}_j$._x0007__x0007_With `raw = TRUE` you are getting the same as with `I(x^2)`._x0007__x0007_Note that because the predictors are changed with `poly(x, 2)` relative to the other two approaches, the coefficients $hat{_x0008_eta}_j$ will also be different, but the fitted models will be the same.",1,0,0
Try the function `tsboot()` in the same package **boot** that is used in the R Sessions,1,0,1
"As the question the OP refers to is in an **R** session and requires one to use **R** to answer it, what would the point be in not referring to or being specific about using **R**?_x0007__x0007_**R** is the course software, so expect **R** questions here. If you don't like that, then don't read those practical questions (or quickly ignore them when you come across them)._x0007__x0007_As this is a practical course (or one in which *doing* the analysis in practice) you should expect R-related, practical questions.",1,0,1
"@alhf, thank you so much, very eloquently explained!",1,0,0
I get the following message_x0007_package ‘ISLR’ is not available (for R version 2.15.0)_x0007_i have downloaded the laterst RSutio version though (Version 0.98.501)_x0007__x0007_any idea?_x0007__x0007_tks,1,1,1
"upgrade to a new version of 3.0.2, then type these commands:_x0007__x0007_    install.packages(\ISLR\"")_x0007_    library(ISLR)""",1,0,1
Do you mean maximizing the likelihood?,1,0,1
"R has many ways of importing data._x0007__x0007_If you want to read data from a csv file, then read.csv will work. But you dont have that file, instead you are getting your data from a library! so typing library(ISLR) will load the library, and therefore the Auto data as well.",1,0,1
"OK, William, thanks -- that helps.",1,0,1
There has GOT to be a better way than this:_x0007__x0007_    # Get the probabilities_x0007_    prob = (exp(predict(fit)) / (1+exp(predict(fit))))_x0007_    # Order things by the balance_x0007_    prob = prob[order(balance)]_x0007_    balance = balance[order(balance)]_x0007_    # Get the lowest index that has a probability > 0.5_x0007_    lowest.index = min(which(prob>0.5))_x0007_    # Find out the balance at this index_x0007_    balance[lowest.index],1,0,1
Thank you!,1,0,1
"As shown in slide 15/40 of the classification presentation:_x0007__x0007_When a model is built with multiple correlated variables (say ldl and obesity) the model only assigns significance to one of the variables. Is this a random process? or does it go for the \best\"" one. _x0007__x0007_If one of the variables is better than the others, then how is it better? ie. How is each variable judged?""",1,1,1
"Hi all, I've changed my working directory to where my file is located and I load the data with the command data<-load(\5.R.RData\""). But everytime I open data, it will only show one element \""Xy\"". Obviously this isn't the data we are supposed to fit the linear model to. How do I load my data, am I doing something wrong? Thanks in advance!""",1,1,1
See the first reply to the following question:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/52fa<phoneRedac>a6ca<zipRedac><zipRedac><zipRedac><zipRedac>4e,1,0,1
So is the moral of hypercube story that when one has a lot of dimensions then almost every point is extreme so modeling is almost pointless? Therefore dimensionality reduction is important?,1,1,1
I replicated the syntax on p.46 in R studio on my Dell running Windows. I guess this is supposed to produce a contour plot but I got no plot and no error message. Anyone have a clue as to what is going on.,1,1,1
"<redacted>: _x0007__x0007_> but what do you mean when you say that I might not need_x0007_> boostrap? Which approach would be needed?_x0007__x0007_What scottPowers may be getting at is that you could use_x0007_some standard statistical results._x0007__x0007_For example, in a random independent sample, yo",1,0,1
"http://www.nature.com/news/scientific-method-statistical-errors-<zipRedac>.<zipRedac>4700_x0007__x0007_**P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.**_x0007__x0007_An excerpt:_x0007__x0007_> For a brief moment in 20<zipRedac>0, Matt Motyl was on the brink of scientific_x0007_> glory: he had discovered that extremists quite literally see the world_x0007_> in black and white._x0007_> _x0007_> _x0007_> Stephen Hawking questions the nature of black holes Bumblebees can fly_x0007_> at altitudes higher than peak of Mount Everest UK immigration debate_x0007_> alienating scientists and students The results were “plain as day”,_x0007_> recalls Motyl, a psychology PhD student at the University of Virginia_x0007_> in Charlottesville. Data from a study of nearly 2,000 people seemed to_x0007_> show that political moderates saw shades of grey more accurately than_x0007_> did either left-wing or right-wing extremists. “The hypothesis was_x0007_> sexy,” he says, “and the data provided clear support.” The P value, a_x0007_> common index for the strength of evidence, was 0.0<zipRedac> — usually_x0007_> interpreted as 'very significant'. Publication in a high-impact_x0007_> journal seemed within Motyl's grasp._x0007_> _x0007_> But then reality intervened. Sensitive to controversies over_x0007_> reproducibility, Motyl and his adviser, Brian Nosek, decided to_x0007_> replicate the study. With extra data, the P value came out as 0.59 —_x0007_> not even close to the conventional level of significance, 0.05. The_x0007_> effect had disappeared, and with it, Motyl's dreams of youthful fame._x0007_> _x0007_> It turned out that the problem was not in the data or in Motyl's_x0007_> analyses. It lay in the surprisingly slippery nature of the P value,_x0007_> which is neither as reliable nor as objective as most scientists_x0007_> assume._x0007__x0007_As far as I know, the p-Value is to be interpreted as_x0007_\Assuming that the null hypothesis is true, how likely is it to see data_x0007_as extreme or extremer than the data we have seen.\""_x0007__x0007_This sounds rather reliable, so where is the catch?""",1,1,1
"Interesting article — thanks._x0007__x0007_> As far as I know, the p-Value is to be interpreted as_x0007_> \Assuming that the null hypothesis is true, how likely is it_x0007_> to see data as extreme or extremer than the data we have_x0007_> seen.\""_x0007__x0007_That sounds correct._x0007__x0007_> This sounds rather reliable, so where is the catch?_x0007__x0007__x0007_## **Firstly**_x0007__x0007__x0007_The p-value doesn't tell how likely it is that that the null_x0007_is true in the first place.  In particular, a low p-value_x0007_does **not** necessarily mean that the null is unlikely._x0007__x0007_## **Secondly**_x0007__x0007_If the p-value is 0.05, it means that there's a 5 percent_x0007_chance that the results could have happened even if there_x0007_were no real effect._x0007__x0007_This means that 5 percent of tests where there's no real_x0007_effect will give \""significant\"" p-values just due to the_x0007_natural random variability._x0007__x0007__x0007_----_x0007__x0007_**More detail on first point**_x0007__x0007_To make up an outlandish example..._x0007__x0007_Suppose we want to study whether Professor Hastie has been_x0007_abducted by aliens._x0007__x0007_We will study this question by observing whether he posts_x0007_any comments on the forum today._x0007__x0007_Our null hypothesis is that he has *not* been abducted by_x0007_aliens._x0007__x0007_Assume, for the sake of argument, that Professor Hastie_x0007_posts comments on 96 percent of days.  (This may not be_x0007_accurate, but the story is more colorful with a high-profile_x0007_figure.)_x0007__x0007_Suppose at the end of the day, there is no post from_x0007_Professor Hastie._x0007__x0007_Well, we know there's only a 0.04 chance of that happening_x0007_when the null is true (that is, when he is not abducted by_x0007_aliens.)_x0007__x0007_So the p-value is 0.04, which is less than 0.05._x0007__x0007_Does that mean that we've got good evidence that Professor_x0007_Hastie has been abducted by aliens ?_x0007__x0007_Of course not._x0007__x0007_In particular, does it mean that there's only a 4 percent_x0007_chance the null is true ?_x0007__x0007_No._x0007__x0007_If you are highly skeptical about alien abduction, you will_x0007_say that the \""prior\"" probability of alien abduction is so_x0007_low, that the posterior probability is still low (even_x0007_though the lack of postings is consistent with alien abduction.)_x0007__x0007__x0007_----_x0007__x0007_**More detail on second point**_x0007__x0007_What the article seems to be concerned about is the following._x0007__x0007_If there are a lot of people doing tests where there's no_x0007_real effect, 5 percent of these people will get_x0007_\""significant\"" results by chance.  Call these \""false_x0007_positives\""._x0007__x0007_If, in addition, there are only a few doing tests where_x0007_there *is* a real effect, the total number of false_x0007_positives could be more than the number of real positives._x0007__x0007_If all \""significant\"" result were published, this could mean_x0007_that most published""",1,0,0
"Staff EDIT:  This post is misleading because of an error, see Staff response below for more._x0007__x0007_@alhf:_x0007__x0007_I know your alien abduction example was a made up one. Still, I can't resist to draw your attention to the fact that Frequentists (under whose domain p-value falls) would object to model the event that {Prof. Hastie has been abducted by aliens}. For them, the probability of that is either 0 or 1, i.e either he has been abducted or not. The event is not something which can be trialed/experimented/repeated under identical conditions a lot of times._x0007__x0007_Of course, Bayesians would take a different course :-)_x0007__x0007_BTW, good overall explanation!",1,1,1
"<redacted>:_x0007__x0007_> When a model is built with multiple correlated variables_x0007_> (say ldl and obesity) the model only assigns significance to_x0007_> one of the variables._x0007__x0007_On the scatter-plots on page 14/40, ldl and obesity do not_x0007_look particularly highly correlated",1,0,1
"I don´t understand the wording of this question: *Download the file 5.R.RData and load it into R using load(\5.R.RData\""). Consider the linear regression model of y on X1 and X2.  To within 10%, what is the standard error for ?1?*_x0007__x0007_What are they exactly asking by \""To within 10%\""._x0007__x0007_Any help welcome.""",1,1,1
"I think you are getting confused because the order of the parts of the formula changes between the first formula (the general statement of Bayes theorem, and the second formula (which shows the specific application for discriminant analysis). The two parts of the numerator of the right-hand side of the first equation are:_x0007__x0007_1. The conditional probability of $X=x$ given that $Y=k$. In discriminant analysis, this is the probability density function of $X$ (but only for those values of $X$ for which the class is $k$). Since probability density functions are usually written $f(x)$, this has been written as $f_k(x)$. _x0007_2. The probability of $Y=k$: in discriminant analysis this is called the prior probability (it's the probability of an observation having class $k$ if you know nothing else about that observation). In discriminant analysis, the notation for this is $pi_k$. _x0007__x0007_What's confusing here is that in the second formula on slide 20, the order of these two parts of the numerator is reversed ... this is because in maths it's more conventional to write a variable _before_ a function, not after it (ie $f_k(x)pi_k$ means the same but looks a bit less conventional). A bit confusing ... but $f_k(x)pi_k equiv pi_k f_k(x)$._x0007__x0007_Hope that helps.",1,0,0
"One paper that deal with this (from a quick Google Scholar search) is [Factors Influencing the Optimal Control-to-Case Ratio in Matched Case-Control Studies][1]. Helpfully, that paper includes citations in relation to the 5:1 rule-of-thumb:_x0007__x0007_1. Rothman KJ. Modern epidemiology. Boston: Little, Brown_x0007_and Company, 1986._x0007_2. Hennekens CH, Buring JE. Epidemiology in medicine._x0007_Boston: Little, Brown and Company, 1987_x0007__x0007__x0007_  [1]: http://aje.oxfordjournals.org/content/149/2/195.full.pdf+html",1,0,1
"I am still not clear on this, so how will I use this and for what_x0007__x0007_will I take n bootstrap samples, fit a model on each and then just use those n models to get confidence intervals for the parameters? Is that the case?",1,1,1
"Well, logistic regression effectively predicts the log of the odds ratio as:_x0007__x0007__x0008_egin{equation}_x0007_log(p/(1-p))=_x0008_eta_0+_x0008_eta_1 x_x0007_end{equation}_x0007__x0007_Now if $hat{p}=0.5$, then the odds ratio is $log(0.5/(1-0.5)$ or $log(1)$ ie 0._x0007__x0007_So, the point at which $hat{p}=0.5$ is the $x$ for which $_x0008_eta_0+_x0008_eta_1 x=0$. That is $x=-_x0008_eta_0/_x0008_eta_1$._x0007__x0007_In R code, you can use ``coef(fit)`` to extract the coefficients as follows:_x0007__x0007_``_x0007_-coef(fit)[1]/coef(fit)[2]_x0007_``",1,0,0
"Yes._x0007__x0007_You could get confidence intervals, standard errors, or even_x0007_histograms (the shape) of the sampling distribution.",1,0,1
"If you have collinear variables You can regress one variable on the other and than use the residuals instead of the original variable._x0007_Other more sophisticated option is to project the variables on an affine space e.g. using the principal components instead of the original variables, but it is generally overkilling.",1,0,0
It is asking how many n-vectors can be created picking each element at random from n elements with re-sampling.,1,0,1
"Thank you for your help.  Here is the code I wrote:_x0007__x0007_tsboot(Xy,boot.fn,1000,l=100,sim=\fixed\"")_x0007__x0007_And what follows is the output._x0007_ _x0007__x0007_Show Traceback_x0007_ _x0007_ Rerun with Debug_x0007_ Error in eval(expr, envir, enclos) : _x0007_  argument \""index\"" is missing, with no default""",1,0,1
"alhf says_x0007__x0007_SECONDLY_x0007__x0007_> If the p-value is 0.05, it means that there's a 5 percent chance that_x0007_> the results could have happened even if there were no real effect._x0007__x0007_Actually, it should be \ there's a 5% chance that results such as those obtained **or more extreme** can happen if there's no ... \""""",1,1,1
"Hello, I am new to cross validation tecniques and after I have read Chapter 5 about resampling methods I would like to ask two questions:  _x0007__x0007_ 1) If I had a single response variable but fitted different models according to the number of predictor variables, is there a rule of thumb to decide from MSEP values which model is different or similar? _x0007_Eg. a 10% difference between MSEP values can be a guide?_x0007__x0007_2) Can I compare the ability of prediction between two models with the same predictor variables but with different response variables, using MSEP values?     _x0007__x0007_Thank you for attention!",1,1,1
"@alhf:_x0007__x0007_> I hope so :-)_x0007__x0007_I can't imagine Dear Hastie giving lecture on LASSO to Martians:-) Let we earthlings continue to \suffer\"", rather than our poor cousins somewhere out there:-)_x0007__x0007_> And thanks for making the frequentist point clearer._x0007__x0007_My pleasure._x0007_",1,0,0
Thank you. Dramatically better!,1,0,1
I don't know about the quality of the solutions but there is a blog whith many:_x0007_http://srividya-rajesh.com/statlearning/,1,0,1
"Since the answer to the Question is a floating point real number, different students can opt to answer it upto different precisions (no. of significant digits). That's why they have given some leeway (10% here or there) from their recorded answer.",1,0,1
"The difference between \boot()\"" and \""tsboot()\"" comes here from:_x0007__x0007_- Y, X1 and X2 are waves_x0007_- little correlation between predictors and response_x0007_- \""tsboot()\"" resamples intervals whereas \""boot()\"" resamples points_x0007__x0007_The correlation in bootstrap samples using \""tsboot()\"" has more variance because some intervals can be at crests, other at valleys, other at nodes._x0007__x0007_On the contrary, \""boot()\"" resamples points, which are more scattered across all parts of the wave...""",1,0,0
"@OP:_x0007__x0007_Your line of thinking is correct. But I think we have to put the notation in order._x0007__x0007_In this course (and in general, in Statistics) ***n*** denotes the no. of observations, the sample size. So suppose your response is **y** and you have only a singl",1,0,1
"Hi, I'm trying to solve question 5.R.R3, in a similar way as lesson:_x0007__x0007_alpha<-function(d){_x0007_glm(y~X1+X2,data=d)}_x0007__x0007_alpha.fn<-function(data,index){_x0007_with(data[index,],alpha(data))_x0007_}_x0007__x0007_boot.out<-boot(Xy,alpha.fn,R=1000)_x0007__x0007_And i get this error: Error in boot(Xy, alpha.fn, R = 1000) : _x0007_  incorrect number of subscripts on matrix_x0007__x0007_I can't figure out what's happening here. Any suggestions?_x0007__x0007_Thanks!!",1,1,1
"50,250 and 1 WHAT? unit? $1's ? $1000's?.. I think a \unit\"" should mean $1000's which results in a red X (wrong answer\"".. so can the course instructors please clarify. thank you""",1,1,1
"I am trying to install ISLR with no success. I get the following error. Please help!_x0007__x0007_> install.packages(\ISLR\"")_x0007_> Installing package into ‘C:/Users/tomba_000/Documents/R/win-library/3.0’_x0007_(as ‘lib’ is unspecified)_x0007_> Warning: package ‘ISLR’ is in use and will not be installed""",1,1,1
"And if you want more info, Bootstrap Methods and their Application by A. C. Davison & D. V. Hinkley is excellent (but not cheap - 77$ on amazon)",1,0,1
Nice! thanks!,1,0,1
"That makes sense, thank you._x0007__x0007_> If the variables are highly correlated, the association with the dependent variable could be assigned to one, or the other, or a mixture-of-both predictor variables._x0007__x0007_So if I have X1, and X2 which are highly correlated, then I can expect either:_x0007__x0007_ - High Beta1, Low Beta2_x0007_ - Low Beta1, High Beta2_x0007_ - Medium Beta1, Medium Beta2?_x0007__x0007_However, whichever combination the algorithm does pick (based on the best fit) it will pick every time for that dataset (not random). Since the pick of the three options above are probably so dataset dependant (since the correlations are so close) I would expect that when doing something like k-fold validation you may get rather different betas (and significance predictions)",1,0,0
"Assignments that build confidence would be a big part too. The people I observed taking this class were \getting it\"" early on. But without targeted practice and an opportunity to strengthen their skills, they weren't prepared to handle the more difficult material coming down the road.""",1,0,1
"In the lecture it is argued that there are two drawbacks of LOOCV in comparison to five- or tenfold cross validation. The first is computational burden, which I understand. But the second argument is that the variance of LOOCV is larger. This certainly is true for one particular instance. But to estimate the test error you average over all n 'leave one out' models. As I do the math, I see that the variance of the test error estimate using LOOCV is exactly equal to the variance of the test error estaimate using five- or tenfold averaging. Where do I go wrong?",1,1,1
"I agree that the lack of interactivity does make the course harder to learn from._x0007__x0007_I've abandoned working through the lecture videos, and am instead working through the textbook and its exercises at my own pace, making use of the lecture videos for supplementary material, and the course forums to get assistance with issues - this is incredibly valuable. The book is excellent, but there is a lot of material to take in._x0007__x0007_3 hours per week on this course is not enough time to take in all of the material being presented. I think this course would be better structured as an 8-10 hours per week course, with homework tied closely to the material to develop that greater understanding of the material._x0007__x0007_Additionally, I feel that the questions testing participants on topics are not as effective as they could be - often they feel too focused on a very specific detail in one of the lectures, with at times slightly ambiguous questions, hence a certain amount of trial and error may be necessary to get the right answer._x0007__x0007_The concepts being taught here are very valuable, and really complement the material in Professor Ng's Machine Learning course, however, the practical homework with that course helps it shine. _x0007__x0007_The foundations are there - a very helpful and knowledgeable community of users on the forums, excellent presentations in the lecture videos, good accompanying slides and of course the book itself. However, I just don't think I'm learning much from the course, following it's prescribed format if I only spend a few hours per week on it. _x0007__x0007_Being disciplined and working through the book is where the real benefit lies. But the course in its current format is not setup to ensure you really develop that deeper understanding. Which I believe may put a lot of people off, as I think with the right structure, the payoff from getting through this course could be much greater.",0,1,1
"As per <redacted> comment, is there a certain percentage of the questions that have to be answered correctly to get the Statement of Accomplishment, or do you simply have to finish all of them?",1,1,1
"You need to read the help for `?cv.glm` closely. Note the argument `cost`, which has the following description:_x0007__x0007_>    `cost`: A function of two vector arguments specifying the cost_x0007_          function for the cross-validation.  The first argument to_x0007_          `cost` should correspond to the observed responses and the_x0007_          second argument should correspond to the predicted or fitted_x0007_          responses from the generalized linear model.  `cost` must_x0007_          return a non-negative scalar value.  **The default is the_x0007_          average squared error function.**_x0007__x0007_(Emphasis mine). The highlight bit is important; by default `cv.glm()` is computing the MSE of the predictions under CV and you are computing the error rate. You are using a different, but more appropriate, cost function compared to the default in `cv.glm()`._x0007__x0007_The solution is actually given in the **Examples** section of `?cv.glm`; supply to the `cost` argument a suitable cost function, such as the one in the last example:_x0007__x0007_     # ... Since the response is a binary variable an_x0007_     # appropriate cost function is_x0007_     cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)_x0007__x0007_Then if you proceed with_x0007__x0007_    cv.err <- cv.glm(Weekly, glmfit, cost = cost)_x0007__x0007_You get the same LOOCV error as you computed by hand:_x0007__x0007_    R> cv.err$delta_x0007_    [1] 0.4500 0.4501_x0007__x0007_----_x0007__x0007_If you are interested in learning more about writing R code etc, you could make your code considerably more efficient by stopping after the prediction step in the lop, and save each predicted value, and then do the later steps on the entire data set. E.g._x0007__x0007_    pred <- numeric(length = nrow(Weekly))_x0007_    #loop thru the cases_x0007_    for (i in 1:nrow(Weekly)){_x0007_       #fit a glm for all but case i_x0007_       butone <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i,], _x0007_                     family=binomial)_x0007_       # use this glm to calc a probability of point i_x0007_       pred[i] <- predict(butone, newdata=Weekly[i,],_x0007_                          type=\response\"")_x0007_    }_x0007_    # calculate average error rate_x0007_    rate <- sum((pred > 0.5) + 1 != as.numeric(Weekly$Direction)) / _x0007_              nrow(Weekly)_x0007__x0007_The reason this can be more efficient is that we can do all the comparisons of the predicted values in a single vectorised shot (and I've employed a bit of jiggery-pokery to simplify those steps into a single line of code), whereas your version was calling `ifelse()` 2178 times (`nrow(Weekly) * 2`), plus `+` 1089 times. Each of these has an overhead so it helps to only do in the loop """,1,0,0
"I have not seen the math you have done and it is interesting to look through, but I will try to say my thoughts about that. _x0007__x0007_In case of LOOCV y_hat_i (i = 1,..,n) almost does not depend from i and its almost equal to the y_hat estimated from the whole dataset (y_hat_wholeset). Because it differs only in one point. So y_hat_i ~= y_hat_wholeset. What we have when we try y_hat_wholeset on the same set? High variance and low bias. Overfitting._x0007__x0007_Now in the case of 5-folds y_hat_i (i = 1,..,5) are different because we use 80% of the set to get y_hat_i and then calculate MSE on the other 20% each time. So we have lower variance than in the first case and higher bias._x0007__x0007_In the case of 2-folds we have too high bias and low variance. And 5-10-folds CV is like a balance which we want to reach._x0007__x0007_If the variance of the test error estimate using LOOCV were exactly equal to the variance of the test error estimate using five- or tenfold averaging one would not consider the 5,10,n-cases. If it equals to each other mathematically. And also there is h_i, the leverage in the case of LOOCV. Which formula do you use for h_i?",1,1,1
They mention in lectures that folds are equal and that the diagram is not correct.,1,0,1
"This correction is wrong._x0007__x0007_The HYPOTHESIS was that Prof Hastie was abducted.  The p-value makes no claim about the probability of a hypothesis, only the probability of the data (# of posts) given the hypothesis.",1,0,1
Had a longer post that got eaten.  Here is the gist of it:_x0007__x0007_ - p-values have issues_x0007_ - so do Bayesian probabilistic statements_x0007_ - p-values provide science with historical continuity_x0007_ - false positives don't matter in a regime where reproducibility is king (see science)_x0007_ - Science Journalism tends to oversimplify and sensationalize,1,0,1
"Hi hope someone can take a look see what I did wrong here for the block bootstrap:_x0007_when numBlocks=1 (build lm model with a single block), I got var(beta_1)^0.5=0.1677; and when set numBlocks=1 =10 (using 10 random blocks to build lm model), var(beta_1)^0.5=0.0475, neither matches the correct answer:_x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>04.png",1,1,1
"The post above got super scrambled and I can't edit it.  It should read:_x0007__x0007_Hi Conor,_x0007__x0007_The summations.... over $i'$ can end up being different..._x0007__x0007_EQUATIONS_x0007__x0007_Hopefully that helps",1,0,1
Great suggestion William.  Will pass along.,1,0,1
Thanks for the suggestions conor,1,0,1
Thanks for the feedback.  I have no idea what WEKA is and Prof Hastie is not from NZ.,1,0,1
Not sure what you mean without more details._x0007__x0007_Are you sure that you are actually looping through your indices instead of just repeatedly predicting the first point?,1,0,1
I agree. There should be more hands-on activities with feedback about your solution. It would be nice to have homework/laboratory after each module to put in practice the tools learned.,1,0,1
"Are you sure you are using the seq function correctly?_x0007__x0007_say that blocknum[j] is 5. Then i guess seq(from=5,to=5*100) gives you a sequence of 496 values. _x0007__x0007_try something like:_x0007__x0007_blocks <- split(1:1000, ceiling(seq_along(1:1000)/100))_x0007__x0007_B <- 2000_x0007__x0007_est <- numeric(B)_x0007__x0007_for(i in 1:B) {_x0007__x0007_b.sample <- unlist(sample(blocks,replace=TRUE))_x0007__x0007_est[i] <- alpha(Xy,b.sample)_x0007__x0007_}_x0007__x0007_sd(est)",1,0,1
"Mark79,_x0007__x0007_>The only difference is that y_hat is estimated with more data in the case of LOOCV. _x0007_This makes sense. If you sum different numbers there is a possibility that you will get the different answer._x0007__x0007_Also you did not answer about formula from page 12 where the leverage is considered. So it seems you mean only the formula from page 11. Ok, lets consider this case and the function_x0007_y = 0 for x = [1,..,10]_x0007_y = 1 for x = [11,..,20]_x0007_y = 0 for x = [21,..,30]_x0007_y = 1 for x = [31,..,40]_x0007_y = 0 for x = [41,..,50]_x0007__x0007_You can try the code below running it in Octave or Matlab if you would like to. It's very straightforward. _x0007__x0007_Here are the CVs and metrics for y_hat_i: _x0007_CV for 5-folds : 0.5298_x0007_[5-folds] y_hat_i : mean 0.4406, median 0.3420, min 0.1096, max 1.0535, std 0.3433_x0007__x0007_CV for N-folds : 0.2589_x0007_[N-folds] y_hat_i : mean 0.4043, median 0.4084, min 0.3749, max 0.4337, std 0.0204_x0007__x0007_There is a difference in 2 times. Because of reasons I mentioned in the previous post. I did not tell that I can proof that \the test error estimate using LOOCV is exactly equal to the variance of the test error estaimate using five- or tenfold averaging\"". This is your point. Could you please show your analytical proof if you have already done this. If your proof that you have changed K to N this is not enough. You should show that sums are equal. Perhaps you need to consider formulas for y_hat_i also. _x0007__x0007__x0007__x0007_clc; close all; clear all;_x0007__x0007_MULT = 10;_x0007_x1 = 1:1*MULT;_x0007_x2 = 1*MULT+1:2*MULT;_x0007_x3 = 2*MULT+1:3*MULT;_x0007_x4 = 3*MULT+1:4*MULT;_x0007_x5 = 4*MULT+1:5*MULT;_x0007__x0007_y = zeros(1,5*MULT);_x0007_y(x1) = 0;_x0007_y(x2) = 1;_x0007_y(x3) = 0;_x0007_y(x4) = 1;_x0007_y(x5) = 0;_x0007__x0007_n = length([x1 x2 x3 x4 x5]);_x0007__x0007__x0007_nk = length(x1); % for 5-fold_x0007_cvs = [];_x0007_mses = [];_x0007_yhatis = [];_x0007_for K = 1:4_x0007__x0007_    if K == 1, x = [x2, x3, x4, x5]; xtest = x1;_x0007_    elseif K == 2, x = [x1, x3, x4, x5]; xtest = x2;_x0007_    elseif K == 3, x = [x1, x2, x4, x5]; xtest = x3;_x0007_    elseif K == 4, x = [x1, x2, x3, x5]; xtest = x4;_x0007_    elseif K == 5, x = [x1, x2, x3, x4]; xtest = x5;_x0007_    end_x0007__x0007_    ylearn = y(x);_x0007_    ytest = y(xtest);_x0007__x0007_    p = polyfit(x,ylearn,1);_x0007__x0007_    fprintf('K = %d_x0007_', K);_x0007_    fprintf('X : %d, ', x);_x0007_    fprintf('_x0007_');_x0007_    fprintf('Y : %d, ', ylearn);_x0007_    fprintf('_x0007_Y = %.4f*X + %.4f_x0007_', p(1), p(2));_x0007_    _x0007__x0007_    mse = 0;_x0007_    for i = 1:length(xtest)_x0007_        xi = xtest(i);_x0007_        yi = ytest(i);_x0007_        yhati = polyval(p, xi);_x0007_        yhatis = [yhatis; yhati;];_x0007_        mse = mse + (yi-yhati)*(yi-yhati)/nk;_x0007_    end_x0007_    mses = [mses; mse;];_x0007_    _x0007_end_x0007__x0007_CV = sum(mses)*nk/n;_x0007_fprintf('_x0007__x0007__x0007_""",1,0,0
"Well we can't find a model with a string \Xy\"" and we're supposed to fit X1 and X2 to Y, so that can't be right.""",1,1,1
"I think this course and the book are at the perfect level for people who need to model but don't want to understand all the math behind things, like programmers or business types.  With statistical software like R, you don't have to know the math, in the sense of writing algorithms, but you still need to know how to choose and assess models.  The course and book perfectly fit that niche, and I'm sure they'll be very popular for a long time.",1,0,1
Thanks._x0007__x0007_Can I just confirm that for the purpose of Exercise 11.d) and e) it is the case that in this instance the sum of i and i' are in fact over the same indices of x and y? i.e. the t-statistic can be calculated as:_x0007__x0007_    (sqrt((length(x) - 1)) * sum(x * y)) / (sqrt(sum(x^2)*sum(y^2) - (sum(x * y))^2)),1,1,1
"Yes, it does. I'm confused about the number $2^p$.",1,0,1
"Yes, I believe so.",1,0,1
This is not what the poster is referring to. 5.R.R1 is not 5.5.R1 (:,1,0,1
You can try also change MULT = 1000; xs=[x1 x2 x3 x4 x5];y = 3*xs+3*sin(xs); and there will be a difference about 25% with 5000 values,1,0,1
"Yes, updating R is probably what you need to do.",1,0,1
"I got an incorrect result as well, using both the \tsboot\"" function and my own code. The answer recorded is also different by a factor of 10 from the other two given, which I would not usually expect. Are we sure that the answer to this question is correct?""",1,1,1
@smgross:_x0007__x0007_I did not say Pr{Hypothesis|Data}=p-value=0 or 1. I am aware that p-value=Pr{Data|Hypothesis}. That is why in my second post I give a outline about how would a Bayesian typically go about finding Pr{Hypotheis|Data}.,1,0,1
"I am trying to understand which classification algorithms suffer from the curse of dimensionality. Two questions:_x0007__x0007_(1) Does **LDA** suffer from the curse of dimensionality?_x0007_In lecture 4.7, at time 7:50, professor says, \*what LDA is really doing is it is measuring which centroid is the closest. It measures distance in way that takes into account the covariance of the variables.*\""_x0007_ **Since LDA classifies points to the nearest centroid, isn't it going suffer from the curse of dimensionality** for the same reason that k-nn does?   (2) We know from the lectures that **k-NN** suffers from the the curse of dimensionality. However, In lecture 2.4, at time 15:04, professor says, \""*On the handwritten zip code problem, the classifying handwritten digits, nearest neighbor classifiers do about as well as in any other method tried.*\""  Given that the images of zip code are high-dimensional (I think 256-dimensional), **why does k-NN do so well on the handwritten zip code problem?**""",1,1,1
"I managed to get the right answer, via a very simple implementation of tsboot._x0007__x0007_This is my code for the standard bootstrap (5.R.R3)_x0007__x0007_set.seed(1)_x0007__x0007_bs <- function(formula, data, indices) {_x0007__x0007_  d <- data[indices,] _x0007__x0007_  fit <- lm(formula, data=d)_x0007__x0007_  return(coef(fit))_x0007__x0007_}_x0007__x0007_results <- boot(data=Xy, statistic=bs,_x0007_   R=1000, formula=y~X1+X2)_x0007__x0007_results_x0007__x0007_Note that this gives you all of the coefficiencts (the X1 coefficient is called t2; t1 is the intercept).  You can pull out just the coefficient for X1 by putting \summary(fit)$coef[2]\"" in the return() line above, in place of \""coef(fit)\"" _x0007__x0007_And all I needed to do for 5.R.R4 was change boot() to tsboot(), making the requisite changes inside the brackets to match what that function wants:_x0007_set.seed(1)_x0007__x0007_bs2 <- function(formula, data, indices) {_x0007__x0007_  d <- data[indices,] _x0007__x0007_  fit <- lm(formula, data=d)_x0007__x0007_  return(coef(fit))_x0007__x0007_}_x0007__x0007_results2 <- tsboot(tseries=Xy, statistic=bs2,_x0007_   R=1000, l=100, sim=\""fixed\"",formula=y~X1+X2)_x0007__x0007_results2_x0007__x0007_The only real difference between boot() and tsboot() is that you call the data \""tseries\"" in tsboot, and you need to specify the type of block (sim=\""fixed\"") and the length of the block (\""l=100\"")._x0007__x0007_This is much simpler that Liyu's code above, and thank heavens, because I couldn't code my way out of a wet paper bag, and was having a great deal of trouble following it.  I don't know what grantbrown used within tsboot, but could the problem be that you were missing set.seed(1) at the top?  I missed it the first try, and got the wrong result.""",1,0,0
"When **p** (number of features) is very large, how does Naive Bayes compare with Logistic Regression? _x0007__x0007_I think Logistic Regression will perform better because it does not assume that features are independent. Are there reasons that favor Naive Bayes over Logistic Regression when **p** is large?",1,1,1
"For time series data, you should not use the ordinary bootstrap, because unlike cross-sectional data, order of the observations is very important here. That is why block bootstrapping is used as at least you will get something like a week or fortnight or month etc. (depending on the length of the block and *n*) **together as a block** in your **resampled data** assuming your original time series was a daily one. Otherwise due to Monte Carlo sampling, observations will go haywire._x0007__x0007_As an aside, you could have framed the Question without giving the numbers:-).",1,0,0
"Hi, seeing that there are many experts in R here can someone tell me a good way to learn R ? It seems to me R is a great tool for data analysis, and coding in R will save time when checking for good models for data before coding in more general purpose languages like python or c.",1,1,1
"Hi!_x0007_I don't understand matplot(Xy,type=\l\""): where are y, X1, X2, beta1hat?""",1,1,1
"Hi alhf,_x0007_I think there is an interesting problem with your abduction example that might be worthwhile discussing: namely that the null and the alternative hypothesis are both way too specific to be analysed with the available data. I mean that there is no logical way in which the deduction chain \A is not posting on the forum today ergo he has been abducted by aliens\"" would be sound._x0007__x0007_On the other hand if I reformulate your example this way: _x0007__x0007_1 . The Null Hypothesis is \""Nothing is wrong today with professor Hastie\"" and_x0007__x0007_2. The Alternative hypothesis is \"" Something unusual is going on that needs to be checked, though we do not know what exactly\""_x0007__x0007_then the reasoning based on the p-value is entirely sound. If your kid in college called you every Sunday and then on one Sunday he disn't wouldn't you call him?""",1,0,0
"The command `matplot()` will produce a graph by pitting one data series against the other ones. The black curve is for `X1`, the red curve represents `X2` and the green one is for `y`, assuming you have not tinkered with the `col` argument.",1,0,1
You can learn R interactively by trying out the Swirl package. Details here: http://swirlstats.com/,1,0,1
"Hi_x0007__x0007_My name is <nameRedac_<anon_screen_name_redacted>>, student of horse racing and the statistical aspects of form",1,0,1
"No^3, you got me wrong. I said both `lm()` and `glm()` [the way you ran it (`family=gaussian`), the default] will produce the same output. However `lm()` is faster.",1,0,1
Check zeros in your code line_x0007_n=<zipRedac>0000_x0007__x0007_There is <zipRedac>000 samples and not <zipRedac>0000,1,0,1
What does it mean to load into R? Now I can see how fresh I am in all this..,1,1,1
Hi! Is it normal to have with block boot a standard error for beta1 nearly 8 times greater than with simple lm regression model?,1,1,1
Thank you Cencerri!,1,0,1
Yes. Your lm() standard errors make strict assumptions about the data.,1,0,1
"<redacted>, <redacted>, thank you for the answers_x0007__x0007_<redacted>, there are a lot of topics with solutions so I guess there are no people who cannot solve it..",1,0,1
"Okay, one last try. At a minimum you must have R installed in your computer. Now type and run the following command:_x0007__x0007_    getwd()_x0007__x0007_Copy the downloaded .RData file into the folder/directory shown as the output of the above command._x0007__x0007_Now run the following expression at the R console:_x0007__x0007_    load(file=\5.R.RData\"")_x0007__x0007_Finally you can run that `write.csv()` bit or do the analysis in R itself.""",1,0,0
"I'm glad that I'm not the only one who found ESL intimidating. ISLR has the right balance of math, intuition, and programming examples!",1,0,1
"I can NOT download \5.R.RData\"". I clicked the link. There is no data file loaded. It just opened R with any data. Any help is highly appreciated._x0007_Thanks!""",1,1,1
"Right. It stands for \matrix plot\"", and it simply draws a graph of every column vector in a matrix._x0007__x0007_For data frames (which is what the **Xy** object is... *slightly* different from a matrix object), it will still treat it like a matrix and draw a graph for every column in the data frame (X1, X2, and y in this case).""",1,0,0
"Looks like your read.table function is failing, so the ch5...Copy object never gets anything assigned to it._x0007__x0007_Do you actually have a file called \ch5 - Copy.txt\"" on your computer somewhere?_x0007__x0007_Does it live at the \""~/Data Science/\"" location? (Where ~ is your home/user directory)_x0007__x0007_If you do have the file, what does it look like when you open it in something like NotePad or TextEdit? Does it look like a data file?""",1,0,1
"* SAS has recently introduced the experimental \modelaverage\"" statement to the GLMSELECT procedure.  setting the options 'selection=none' and 'sampling=urs(percent=100)' appears to allow for bootstrap estimates of the parameters.  i've recreated the R code from the LAB using SAS below;_x0007__x0007__x0007_* > set.seed(1)_x0007_  > boot.fn(Auto,sample(392,392,replace=T));_x0007_* regression using single bootstrap;_x0007_proc glmselect data=Auto seed=1;_x0007_   title2 'Auto Data: Regression Using Single Bootstrap (1)';_x0007_   model mpg = horsepower / selection=none;  * keep all variables in the model;_x0007_   modelaverage nsamples=1 sampling=urs(percent=100);  * experimental, 'urs' indicates w/ replacement;_x0007_run;_x0007__x0007_*  > boot.fn(Auto,sample(392,392,replace=T));_x0007_proc glmselect data=Auto seed=2;_x0007_   title2 'Auto Data: Regression Using Single Bootstrap (2)';_x0007_   model mpg = horsepower / selection=none;  * keep all variables in the model;_x0007_   modelaverage nsamples=1 sampling=urs(percent=100);  * experimental, 'urs' indicates w/ replacement;_x0007_run;_x0007__x0007_* regression using 1000 bootstraps;_x0007_proc glmselect data=Auto seed=3;_x0007_   title2 'Auto Data: Regression Using 1000 Bootstraps';_x0007_   model mpg = horsepower / selection=none;  * keep all variables in the model;_x0007_   modelaverage nsamples=1000 sampling=urs(percent=100);  * experimental, 'urs' indicates w/ replacement;_x0007_run;_x0007__x0007_* regression using 1000 bootstraps (quadratic model);_x0007_proc glmselect data=Auto seed=3;_x0007_   title2 'Auto Data: Regression Using 1000 Bootstraps (Quadratic Model)';_x0007_   model mpg = horsepower horsepowersq / selection=none;  * keep all variables in the model;_x0007_   modelaverage nsamples=1000 sampling=urs(percent=100);  * experimental, 'urs' indicates w/ replacement;_x0007_run;_x0007__x0007_quit;""",1,0,1
"Right click and \save as\"" to your working directory.""",1,0,1
"Hello,_x0007__x0007_I am using the following code for questions 5.R.R3 & 5.R.R4, but cannot seem to get the \correct\"" answers. Would someone kindly point out where I might have messed up my code?_x0007__x0007_Thanks,_x0007_- VL._x0007__x0007__x0007_----------_x0007__x0007__x0007_    library(boot)_x0007_    load(\""5.R.RData\"")_x0007_    _x0007_    lmF0 <- lm(y~X1+X2, data=Xy)_x0007_    print(summary(lmF0))_x0007_    _x0007_    lmWF <- function(thisdata, index)_x0007_    {_x0007_    lmFI1 <- summary(lmF1) lmF1 <- with(thisdata[index,], lm(y~X1+X2))     return(lmFI1$coefficients[\""X1\"",\""Std. Error\""])     }          lmWFBlock <- function(thisData)     {      lmF2 <- lm(y~X1+X2, data=thisData)     lmFI2 <- summary(lmF2)     return(lmFI2$coefficients[\""X1\"",\""Std. Error\""])     }          set.seed(1)     boot.output1 <- boot(data=Xy, statistic=lmWF, R = 1000)     print(boot.output1)     print(mean(boot.output1$t))          set.seed(1)     boot.output2 <- tsboot(tseries=Xy, statistic=lmWFBlock , R = 1000, l = 100, sim = \""fixed\"")     print(boot.output2)     print(mean(boot.output2$t))""",1,1,1
"In the question 2.2 R2 in Chapter 2 I gave the answer calculated by formula _x0007_0.05^50 + (1 - 0.95^50) and according to class system it was wrong. I strongly disagree. Please explain, why my answer is wrong.",0,1,1
Thanks to all who participated in this discussion group. I was hopelessly lost. but thanks to comments I finally am able to understand at least some of it. Thanks,1,0,0
"Thank you Kushan, Swirl seems definitely interesting, and also easy enough for beginners.",1,0,1
I only got it wrong because I rounded up to dollars and cents! Don't round!,1,0,1
"I can't explain why it is wrong if you don't explain first why you think it's right._x0007_What is the first term supposed to represent? For all practical purposes it is zero. The second term points in the right direction, but a (two-dimensional) picture would show you that it is off.",1,1,1
the authors of the solutions manual couldn't even do some of the problems!,0,0,1
"I spent ages on this question, which turned out to not be as difficult as I was thinking.  The table on this page helped me: http://www.cs.sfu.ca/~ggbaker/zju/math/perm-comb-more.html",1,0,1
standard errors from block bootstrap are more reliable than standard errors from lm(),1,0,1
"Thanks, I think that´s the problem. I had made some mistakes indexing the dataset. I believe I´ve solved it: I got an mean error of 0.0275. But I´m having some problems with indexing objects in R...",1,0,0
Ok. I see. I have some idea... but it is not enough to define the exact design. I´m pretty sure that it´s some kind of stratified sample._x0007__x0007_Thanks!,1,0,1
"Hi,_x0007__x0007_I have the same question that <redacted> about the question meaning._x0007_Is it about the best method or all methods that apply to the case?_x0007__x0007_Should I consider random guessing as a potential answer even if it was not presented during the lecture or not?_x0007__x0007_I tried several answers, reasonable for my point of view, but without success._x0007__x0007_First I selected only random guessing due to the fact it seems that height has no influence on the grade (now I see it was stupid answer)._x0007__x0007_Next, when it occured to be wrong, I marked all the answers, including linear regression. I followed the same reasoning as about Default classicicaion example, assuming that Y = { <zipRedac> if a student gets A, 1 otherwise }. I know the results could be difficult to interpret (linear regression does not estimate Pr(Y = 1 | X) well), but the method could be applied as well as random guessing._x0007__x0007_Next, I eliminated random guessing as it was not included in the course (and probably it could bring the worst results from all the methods), leaving all the rest._x0007__x0007_At that moment I started to read this discussion._x0007_According to ah<zipRedac>7171 none of combination LDA, Logistic is true, what is strange because, in my opinion both can be used in considered case._x0007__x0007_Based on that, I eliminated linear regression - due to the fact it doesn't work well (a key word in a question) leaving all the rest - the answer once again was wrong._x0007__x0007_I have my last chance and I'm completely in stuck._x0007_Can anybody help me?_x0007__x0007_Sorry for bad English but I'm not a native speaker.",1,1,1
Just double click the Ch 5R file and the complete exercise file will be properly loaded into you R Studio.,1,0,1
"Geez, this one was hard!",0,0,1
"I got the answer (after trying the other two, sadly) I am a bit lost at the answer provided by the professors, can somebody help me or point me to any links which are potentially helpful ?",1,1,1
"Speak for yourself! I work full-time, teach part-time, and 3 hours is just about right for me.",1,0,1
"The R-session videos are hard to follow. I can't read Dr. Hastie's desktop, can you? When I maximize the video, part of it gets truncated with the text on the right.",1,1,1
"I guess you also forget to take the square root of sum((outmat[,2]-mean(outmat[,2]))^2)/(n-1), as indicated by formula 5.8",1,0,1
"Thanks for the explanation. And I agree with your conclusion._x0007__x0007_However, I'm share GBL's confusion that the other \text-book\"" formula for calculating standard error is to divide standard deviation (of population) by sqrt(n), n is the sample size. Often the population stats are unknown, we thus estimate its SD with the sample SD. _x0007__x0007_Here is the wiki page on this http://en.wikipedia.org/wiki/Standard_error#Standard_error_of_the_mean_x0007__x0007_Could any one help to further clarify our confusion here? Thanks!""",1,1,1
"You're talking about the data for the R questions at the end of chapter 5 ?_x0007__x0007_In question 5.R.R1, it says:_x0007__x0007_> Download the file [5.R.RData](https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/5.R.RData) and load it into R using load(\5.R.RData\"")._x0007__x0007_If you click on the link, [5.R.RData](https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/5.R.RData), in the first sentence you will get it. _x0007__x0007_It points to_x0007_[https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/5.R.RData](https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/5.R.RData)""",1,0,1
Yhank you it works. Y X1 X2 data is ready to use!,1,0,0
"I have compared your results to mine.. They were similar .. Nevertheless, 5.R.R.3 was giving me a correct answer, but it was not the case with 5.R.R.4. I am not sure what is the problem .. Is it **with** and **summary** that make it so ? Hmmph or the  approach is incorrect .. I hope who got it right answers =)",1,0,1
"I was talking about products with same/ different indices... Anyhow, smgross has explained it beautifully... Regards!",1,0,0
"??. _x0007_The volume of the n-cube is r^n, where r is a side of a n-cube. _x0007__x0007_Our n-cube is settled in positive part of a space, beginning from the origin (0,0,...,0). The boundary region of the n-cube is the set of points that in segments (0; 0.0<zipRedac>) or (0.<zipRedac><zipRedac>; <zipRedac>). So we have <zipRedac> n-cubes, putted one in another: _x0007__x0007_<zipRedac>) the little n-cube with sides 0.0<zipRedac>, _x0007__x0007_2) the bigger middle n-cube with sides 0.<zipRedac><zipRedac>_x0007__x0007_<zipRedac>) the biggest n-cube with sides <zipRedac>. All cubes are beginning in the origin._x0007__x0007_The proportion is: volume(boundary region) / volume(biggest n-cube). _x0007__x0007_volume(biggest n-cube) = <zipRedac>, because <zipRedac>^n = <zipRedac>, so we need to find only volume(boundary region)._x0007__x0007_volume(boundary region) = volume(little n-cube) + difference between volume(biggest n-cube) and volume(middle n-cube). _x0007__x0007_So the formula is:_x0007__x0007_volume(boundary region) = volume(little n-cube) + ( volume(biggest n-cube) -volume(middle n-cube) ) = 0.0<zipRedac>^n + (<zipRedac>^n - 0.<zipRedac><zipRedac>^n)._x0007__x0007_The R code is:_x0007__x0007_for(i in <zipRedac>:<zipRedac>0){ print(paste0(i, \-cube, 2-sided boundery:  \"", 0.0<zipRedac>^i + (<zipRedac> - 0.<zipRedac><zipRedac>^i))) }_x0007__x0007_The main result is:_x0007__x0007_[<zipRedac>] \""<zipRedac>-cube, 2-sided boundary: 0.<zipRedac>\""_x0007__x0007_[<zipRedac>] \""2-cube, 2-sided boundary: 0.<zipRedac>\""_x0007__x0007_[<zipRedac>] \""<zipRedac>-cube, 2-sided boundary: 0.<zipRedac>427<zipRedac>\""_x0007__x0007_[<zipRedac>] \""<zipRedac>-cube, 2-sided boundary: 0.<phoneRedac>7<zipRedac>\""_x0007__x0007_[<zipRedac>] \""<zipRedac>0-cube, 2-sided boundary: 0.<phoneRedac><zipRedac><zipRedac>7<zipRedac><zipRedac>\""_x0007__x0007_[<zipRedac>] \""20-cube, 2-sided boundary: 0.<phoneRedac><zipRedac><zipRedac>4<zipRedac>8\""_x0007__x0007__x0007_[<zipRedac>] \""<zipRedac>0-cube, 2-sided boundary: 0.<phoneRedac><zipRedac>70<zipRedac><zipRedac>\""_x0007__x0007_[<zipRedac>] \""40-cube, 2-sided boundary: 0.<phoneRedac><zipRedac>48<zipRedac>7\""_x0007__x0007_[<zipRedac>] \""<zipRedac>0-cube, 2-sided boundary: 0.<phoneRedac><phoneRedac>7\""""",1,0,1
"Due to the constraint, the nonzero betas will tend to be smaller than they would be if estimated with OLS.  Think of the constraint as saying that there is a limited amount s of \regression coefficient stuff\"" (i.e., the stuff regression coefficients are made of) that is available to be divided among all of the regression coefficients.  In being frugal with this regression coefficient stuff, the lasso will set some coefficients to zero, but it will also tend to make all of the others smaller than OLS regression would, unless s is really big (in the limit as s goes to infinity, you'd just have OLS regression).""",1,0,0
"I do the opposite, I read the book first and I find the videos help clarify things further.",1,0,1
"For that question, I referred back to slide 9 of chapter 2:_x0007__x0007_> Although it is *almost never correct*, a linear model often serves as_x0007_> a good and interpretable approximation to the unknown true function_x0007_> f(X)._x0007__x0007_But there's also page 19 of the book:_x0007__x0007_> Historically, most methods for estimating f have taken a linear form._x0007_> [...] But often the true relationship is more complicated_x0007__x0007_And page 35:_x0007__x0007_> ...linear regression assumes that there is a linear relationship_x0007_> between Y and X1, X2, . . . , Xp. It is unlikely that any real-life_x0007_> problem truly has such a simple linear relationship..._x0007__x0007_And the footnote at the bottom of page 63:_x0007__x0007_> The assumption of linearity is often a useful working model. However,_x0007_> despite what many textbooks might tell us, we seldom believe that the_x0007_> true relationship is linear._x0007__x0007_As far as my own personal experience, I'd have to agree. Real-world relationships are hardly ever *truly* linear, in the same way that objects in nature are almost never perfect lines/circles/spheres/etc._x0007__x0007_But that doesn't mean you can't get useful scientific results by acting as if they are.",1,0,1
"I found it means the tolerable error in my answer (might be caused by different random seed setting or different number of bootstrapping iterations used by students and the teachers)._x0007__x0007_Anyway, \to within xx%\"" is a little bit difficult for a non-English speaking student to understand. :)""",1,0,1
What test statistic are you trying to estimate in applying bootstrap to lm(y~X1+X2)?,1,1,1
+1 for references,1,0,1
"I do these commands, and R seems to run it but does not output anything?",1,1,1
"I read it like <redacted> does, hence my original questions.  Thanks to both Dan and <redacted> for their responses.",1,0,1
"I understand. In the case of estimating the mean we can calculate directly from the data._x0007__x0007_For other estimators and when no assumptions can be done over the population distribution, calculating the estimator might no be as straightforward._x0007__x0007_Question is: is it possible using the bootstrap to calculate the estimator too?",1,1,1
There is no Download Video capability to the 6.4 lecture.  There is no information on this in the Course Info tab as nothing has been posted there since February 6.  All other videos can be downloaded at this time except for that one.  Please enable the Download Video capability for this lecture.  Thanks,1,1,1
"I would like to say that including real world examples to the content of the lectures adds a great deal of understanding to the particular principle being discussed.  I really wish more examples were included - especially when formulas are presented.  Real life examples have the added benefit of pushing one (like me) to go look for more understanding on that subject.  That being said, I am enjoying the course and the book.",1,0,0
"There are two basic math facts you must know to read these machine learning books:_x0007__x0007_1. The gradient of a multivariate function is a vector that points in the direction of greatest change.  This is the main idea behind gradient descent.  You start at a plausible point and keep subtracting small multiples of minus the gradient until you arrive at a minimum._x0007__x0007_2. Any matrix $A$ with full column rank has a left inverse $(A^{T}A)^{-1}A^{T}$.  This is used to get the coefficients of a linear model since the data matrix will normally have linear independent columns._x0007__x0007_This isn't all the math you need but it seems to be the two main facts these books assume.  The first fact will be in any calculus textbook, and the second fact in any linear algebra book.",1,0,1
"Do you have an example? Because \an estimator is a rule for calculating an estimate of a given quantity based on observed data\"" (Wikipedia \""Estimator\""). So how can it not be straightforward to calculate from your data?""",1,1,1
Thanks very much.,1,0,1
"Thanks boethian, I have clarified my question below, hopefully my meaning is more clear now.",1,0,1
"Hmm, left invertibility of a matrix does not imply Ax = b always has a solution, just that the solution is unique if it exists.  Sorry about that.  So fact #2 should be: full column rank implies a left inverse $(A^{T}A)^{-1}A^{T}$, and full row rank implies a right inverse $A^{T}(AA^{T})^{-1}$.",1,0,1
"figured it out. Data=Xy, not the filename itself---confusing here",1,0,1
"Gavin- I try that exact function and It doesn't return anything. What am I doing wrong? Also, explain the last line of the first function (What does [2] mean and the ## for part)",1,1,1
"`cv.glm` expects a full fitted model object, `?cv.glm`. It uses it to get the formula, the initial cost of the full model, etc.",1,1,1
"For 1) or 2), you may want to compute the SD of your MSE as well, which in CV you can easily do or have a package do for you (like the `caret` packgae). This allows you to see the variation in your prediction error through CV, so can inform if, for example, MSE 10% is a lot and reliable given say an MSE SD of 2% vs. 20%.",1,0,0
Yeah. I noticed that too.,1,0,1
"i'm think in the same way, in the continuous probability function a point estimation doesn't exist, you need a range or interval to used it. so i don´t have idea how can use this formula.",1,0,1
"i'm think in the same way, in the continuous probability function a point estimation doesn't exist, you need a range or interval to used it.",1,0,1
"Hi,_x0007__x0007_I was wondering where MARS (Multivariate adaptive regression splines) is located in the graph chapter 2 \Flexibility and interpretability\"". Is MARS more or less in the middle like GAM (generalized additive models) ?_x0007__x0007_And when is it necessary to use MARS rather than another non-parametric model ?_x0007__x0007_Best,""",1,1,1
"Some of the explanations to answers in Ch. 5 have the term \i.i.d\"".  For example: \""i.i.d. bootstrap\"". What does that mean?""",1,1,1
1+20*21/2 is not 220.5_x0007_it is 1+10*21=211._x0007_The 1 is not divided by the 2.,1,0,1
"Gavin, thank you so much!",1,0,1
Independent and identically distributed,1,0,1
"Hey guys, _x0007__x0007_I was reading the book and got stuck at section 7.7.1._x0007_Equation (7.15) says that, for each feature x_ij, a general non-linear function f_j(x_ij) can be used to replace beta_j * x_ij. My question is, if, say, my f_j is chosen to be a smooth spline or a regression spline for example, how do i actually find the coefficients?  Are all coefficients for f_j, j = 1...p be jointly optimized or do we find them one by one for each j independently? I am so confused. Can someone give the steps or algorithm of how this is done? In the book, it is described as it is a simple or trival problem, maybe it is, but can somebody help explain it a little? _x0007__x0007_Thank you!_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
One has to make sure to set the seed before running boot in order to get the same result that's expected to pass the quiz._x0007_set.seed(1),1,0,1
"I'd think the answer to your (2) is that there shouldn't be a difference. In the case that there is no autocorrelation anyway, it seems to me it'd be exactly the same (but I could be wrong). _x0007__x0007_However why would you apply block bootstrap to non-time-series data? _x0007__x0007_BTW all that `help(Portfolio)` reveals is that it is simulated data, not simulated time-series data.",1,0,1
@asadoughi is there a way to make your solutions appear with mathematical notation? It's hard for me to follow this type of syntax:_x0007__x0007_frac {pi_k_x0007_                frac {1} {sqrt{2 pi} sigma}_x0007_                exp(- frac {1} {2 sigma^2} (x - mu_k)^2)_x0007_          ,1,0,1
"This might be answered somewhere, and maybe I just missed it._x0007_But why does linear regression use a t-statistic, and logistic uses a z-statistic?",1,1,1
"I think that by taking the mean of ever so many bootstrap samples, you will end up going right back to what your original estimate is. Fundamentally, all you have is your own sample, and that's your best shot at the population. I'm not sure much can be done to \improve\"" your data or estimate without additional assumptions about it (or of course, more data). _x0007__x0007_But as an example, you can try this on the `Xy` data in computing the $_x0008_eta_1$ regression coefficient asked in the quiz. When I computed this on larger and larger bootstrap samples, I get an ever *decreasing* difference between the mean of the bootstrap sample estimates and the actual estimate given by the `lm`. _x0007__x0007_The largest number of iterations I computed was $1<zipRedac><zipRedac>,<zipRedac><zipRedac><zipRedac>$, where the difference between the mean bootstrap estimate and the ordinary estimate was less than $<zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>1$.""",1,0,1
I can't find anything in the lecture except the statement that an observation may or may not be in the set.,1,0,1
"Why when I wrote library(leaps),it shows that Error in library(leaps) : there is no package called ‘leaps’",1,1,1
"Since you didn't have a question, please search the discussion forum if you are interested in the topic, e.g. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/52f7912c2365ecafc<phoneRedac>",1,0,1
"You first need to load `5.R.RData` and you will have a new `Xy` `data.frame` in you workspace:_x0007__x0007_    > load(\5.R.RData\"")_x0007_    > head(Xy)_x0007_            X<zipRedac>        X<zipRedac>         y_x0007_    <zipRedac> <zipRedac>.<zipRedac>977<zipRedac>0 0.<phoneRedac> 0.<phoneRedac>_x0007_    <zipRedac> <zipRedac>.<zipRedac>673<zipRedac>3 0.<phoneRedac> 0.<phoneRedac>_x0007_    3 <zipRedac>.<zipRedac>3688<zipRedac> 0.<phoneRedac> 0.<phoneRedac>_x0007_    4 <zipRedac>.<zipRedac>063<zipRedac>7 0.<phoneRedac> 0.<phoneRedac>_x0007_    5 <zipRedac>.<zipRedac>75553 0.<phoneRedac> 0.<phoneRedac>_x0007_    6 <zipRedac>.<zipRedac>445<zipRedac>3 0.<phoneRedac> 0.<phoneRedac>_x0007__x0007_And use `Xy` as `data` input for `lm`.""",1,0,0
"Linear regression uses a t-statistic (for small dataset size n) as the test-statistics (coefficients divided by their standard errors) have a t-distribution._x0007_Logistic regression is based on the Bernoulli distribution with parameter p. The test statistics, which are sums, have the binomial distribution, which can be approximated by the normal distribution. For large n (>= 100), the two coincide.",1,0,0
"In the lecture it was suggested that for a time series with dependencies between successive data points we could use blocks of 3 (or n, I assume). However, the blocks were not overlapping. It seems no matter how large the blocks are, there would be a dependency between the last data point of the preceding block and the first data point of the succeeding block.  Would it not be better to have overlapping blocks?",1,1,1
"Same here.  I double checked.  Here's all the ones I could find/take for section 6, week 5, lectures 8a-k:_x0007__x0007_6.1.R1_x0007_6.2.R1_x0007_6.2.R2_x0007_6.3.R1_x0007_6.3.R2_x0007_6.4.R1_x0007_6.5.R1 _x0007_6.6.R1_x0007_6.6.R2_x0007_6.7 R1_x0007_6.8 R1_x0007_6.10.R1_x0007_6.10.R2_x0007_6.Q.1 _x0007_6.Q.2 _x0007_6.Q.3 _x0007_6.Q.4 _x0007_6.Q.5",1,0,1
"\the ideal or optimal predictor of Y with regard to mean-squared prediction error: f(x) = E(Y|X=x) is the function that minimizes E[(Y-g(X))^2|X=x] over all functions g at all points X=x\""_x0007__x0007_ALL functions g?  Really?""",1,1,1
"Ah, thank you.  What should I have read to know to do that?  The problem doesn't mention at Xy at all, how would I find out I can do head(Xy) to learn about the data in the dataset?_x0007__x0007_What I am asking is, is there a general strategy to explore a data set like that in 5.R.RData?  Admittedly, usually one would know what one put into a data set.",1,1,1
"I am struggling to understand why LOOCV's cross-validation error has high variance. _x0007__x0007_Here is the explanation from the book:_x0007_\**Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated**, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\""_x0007__x0007_I am not able to prove why the claim in bold is true. Could someone please explain/prove this claim?""",1,1,1
"I see your point that \LDA looks for averages of the groups, and these are not new or moving targets, they are fixed from the beginning relative to all data points.\""_x0007__x0007_However, there is still an issue: when LDA classifies a fresh new observation in a high dimensional space, **the new observation will be equally far away from all the group/class centroids**. _x0007__x0007_Reference:_x0007_[Mining of Massive Datasets][1]_x0007__x0007_Section: 7.1.3 The Curse of Dimensionality_x0007__x0007_*in high dimensions, almost all pairs of points are equally far away from_x0007_one another.*_x0007__x0007__x0007_  [1]: http://infolab.stanford.edu/~ullman/mmds/book.pdf""",1,0,1
"I think <redacted> original question is an interesting one._x0007__x0007_I'll have another attempt._x0007_(But it would be interesting to hear from someone who has_x0007_more experience with the lasso than I do, maybe <redacted>)_x0007__x0007_- The lasso gives us a model with some coefficients set to_x0007_  zero, and the remaining ones shrunken to some extent._x0007__x0007_- It would also be possible do an ordinary (unconstrained)_x0007_  least squares for a model containing only the variables_x0007_  which had non-zero coefficients in the lasso._x0007__x0007_<redacted> question is whether doing the second_x0007_regression would be useful._x0007__x0007_I'd imagine that if the $lambda$ for the lasso was chosen_x0007_to give good cross-validation error, then any shrinkage of_x0007_the non-zero coefficients might be part of why that_x0007_cross-validation error is low._x0007__x0007_If the lasso is shrinking the non-zero $_x0008_eta,$s quite a_x0007_bit, this might be helping to keep the variance down and the_x0007_second regression above might have higher cross-validation_x0007_error._x0007__x0007_If so, then the reduced lasso coefficients might be more_x0007_useful to interpret than the larger, overfitted,_x0007_coefficients from the second regression._x0007_I suppose cross-validation could be used to compare these_x0007_two models directly._x0007__x0007_And if the non-zero coefficients from the lasso are not much_x0007_different from those in the second regression above, then it_x0007_wouldn't matter much which ones were used in giving an_x0007_interpretation.",1,0,0
"In lecture 5.2, slide 16, the formula for the standard error of cross-validation estimate seems to be wrong; it needs to be divided by sqrt(K)._x0007__x0007_For details, please see [page 18: Standard errors for cross-validation][1] _x0007__x0007__x0007_  [1]: http://www.stat.cmu.edu/~ryantibs/datamining/lectures/18-val1-marked.pdf",1,1,1
"Check alhf excellent post, it provides a visual intuition behind block bootstrap having higher variance than regular bootstrap for the homework exercise, different blocks seems to be highly correlated, averaging them leads to higher variance._x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/52fb<zipRedac>f27a78f4f362d<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>1_x0007__x0007_Another way to look at this:_x0007__x0007_Assuming we are sampling many different random variables._x0007__x0007_If the variables are independent,  when the sampled means are averaged, two things happen: 1) the normal components of each variable get reinforced (the sum of two normals is another normal) and 2) the non-normal components of each variable get washed out and at big sample sizes will be cross-cancelled because of the independence assumption._x0007__x0007_With non-correlation and big sample sizes, the result is a normal distribution averaging the normal components of each sampled random variable, with a bounded variance._x0007__x0007_When the variables are not independent, the non-normal component can be reinforced in the averaging, and the averaging of the samples will include cross-correlation terms. Thus, variance in general will be higher (because of the cross-correlation terms) and it does not decrease adding new samples or with a bigger sample size.",1,0,1
I think this is established by the Gauss-Markov theorem._x0007__x0007_https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem,1,0,1
"\*It has always been quite confusing to me when to use n-1 and when to use n. I would also appreciate some guidance about this point.*\""_x0007__x0007_Divide by n-1 to get an **unbiased** estimator: *low bias, high variance*_x0007__x0007_Divide by n to get a biased estimator: *high bias, low variance*_x0007__x0007_This course covers it well: https://www.coursera.org/course/biostats_x0007__x0007__x0007_The formula on slide 16 is for estimating the **sample standard deviation** of **validation errors in each fold**. We need to divide the **sample standard deviation** by sqrt(K) to obtain the **standard error of cross-validation estimate**. It's explained really well here: http://www.stat.cmu.edu/~ryantibs/datamining/lectures/18-val1-marked.pdf. Someone please correct me if I am wrong.""",1,1,1
"c holds the bootstrap indices, which are sampled each time._x0007__x0007_So, if your dataframe has, for simplicity, 5 observation, the boot call may use for the first run observations `<zipRedac>,4,2,3,5` then for the second `<zipRedac>,<zipRedac>,4,3,3` then `2,3,5,5,<zipRedac>` and so on._x0007__x0007_`c` will be equal to those series of ids every time, so in your function when you use `d = data[c,]` you are getting rows `<zipRedac>,4,2,3,5` (or whatever the sequence is) and all of the columns. That is, you are generating a bootstrap sample of your data._x0007__x0007_You can easily visualize this with this code:_x0007__x0007_    library(boot)_x0007_    _x0007_    set.seed(<zipRedac>2345) # For reproducibility_x0007_    printIDs <- function(mydata, ids)_x0007_      {_x0007_      print(ids)_x0007_      }_x0007_    _x0007_    df <- data.frame(x=<zipRedac>:<zipRedac>0, y=rnorm(<zipRedac>0))_x0007_    res <- boot(df, printIDs, R=<zipRedac>0)_x0007__x0007_Which outputs:_x0007__x0007_     [<zipRedac>]  <zipRedac>  2  3  4  5  6  7  8  9 <zipRedac>0_x0007_     [<zipRedac>]  5  8  8 <zipRedac>0  8  7  9 <zipRedac>0  3  7_x0007_     [<zipRedac>]  4  <zipRedac>  5  9  3  6  6  9  7  4_x0007_     [<zipRedac>] <zipRedac>0  2 <zipRedac>0  4 <zipRedac>0  4  <zipRedac>  <zipRedac> <zipRedac>0  8_x0007_     [<zipRedac>]  8  7  8  3  8  4  <zipRedac>  7  7  4_x0007_     [<zipRedac>]  7  4  3  8 <zipRedac>0  <zipRedac>  2  8  6  8_x0007_     [<zipRedac>]  4  4  4  5  3  7  4  6 <zipRedac>0 <zipRedac>0_x0007_     [<zipRedac>]  7  9  <zipRedac>  8 <zipRedac>0 <zipRedac>0  9  8  7 <zipRedac>0_x0007_     [<zipRedac>]  6 <zipRedac>0  <zipRedac>  <zipRedac>  2  7  6  8  9  6_x0007_     [<zipRedac>]  3  7  <zipRedac>  5  7  6  9  <zipRedac>  3  <zipRedac>_x0007_     [<zipRedac>]  5  2  7  3 <zipRedac>0  2  <zipRedac>  4  3  4",1,0,0
"Thanks, MikeML, I am mostly ok with the difference._x0007__x0007_What confuses me is why somebody would want a biased estimator? I.e. why not always dividing by n-1 and getting an unbiased estimator?_x0007__x0007_Greeting from a fellow bio-stats trooper !!! In the slide 18 in the linked pdf, I think the n-1 is assumed in calculating $SD(	heta)$, otherwise the procedure would use a biased estimator of the sample standard deviation. See slides 18-20 in lecture 4 of biostats course [1]._x0007__x0007_[1] http://github.com/bcaffo/Caffo-Coursera/raw/master/lecture4.pdf",0,1,1
@pochete:_x0007__x0007_> This has something about using n-1 instead of n to try to_x0007_> correct some of the bias in the estimate of the population_x0007_> SD._x0007_> _x0007_> It has always been quite confusing to me when to use n-1 and_x0007_> when to use n. I would also appreciate some guida,1,1,1
"I removed the \dollar\"" ref to Xy, Xy\""dollar\""X1 e.g. uses the complete vector X1 and bootstrap cannot reassign the row indices. You´ll find more information by typing ?'$' in the R console._x0007__x0007_boot.fn = function (data, index) { _x0007_  return(coef(lm(y ~ X1 + X2, data = data, subset=index))) _x0007_}""",1,0,1
"My next to last submission was graded wrong, so I changed it, only to see that it was the right answer!",0,0,1
"Hi,_x0007__x0007_I am <nameRedac_<anon_screen_name_redacted>>, from Geneva and live Zurich in Switzerland. I just graduated in mathematics and now am a graduate trainee in a hedge fund._x0007__x0007_Thanks a lot for the teaching :-)_x0007__x0007_Cheers,",1,0,1
"when we fit a linear regression model, near 0 coefficient implies that the corresponding predictor has little correlation with the outcome. _x0007_So is there a way of characterizing the relationship between the coefficient and the  correlation more precisely?_x0007__x0007_(excuse me for still on ch3, I think I would catch up with the progress soon)",1,1,1
I am sorry..I thought that the time was out...I got the answer wrong any way..So I wanted to know the answer without affecting my score. Thanks for catching this.,0,0,1
Thank you so much for the answer,1,0,1
"In the previous lectures about crossvalidation, we have been informed to do model selection (feature selection) within the crossvalidation step, but in the Chapter 6 lectures they select the most predictive variables for each model of dimension d, to later perform crossvalidation across dimensions. Isn't this contradictual?",1,1,1
"No, it shows that there is a quiz 40 between 6.8 and 6.10.",1,0,1
"After running lasso or ridge regression and using k-fold cross-validation, presumably one has k estimates of the model coefficients for each lambda.  After choosing a lambda, one is still left with the entire cross-validation set for that lambda.  And even when regularization is not being done the same is true -- there will be k sets of coefficients and standard errors._x0007__x0007_What coefficients would one report?  And what would the correct standard error be for the coefficients?",1,1,1
"Nico--I did that and it worked BUT it gave me an answer that it deemed wrong? _x0007__x0007_Call:_x0007_tsboot(tseries = Xy, statistic = bs2, R = <zipRedac><zipRedac><zipRedac><zipRedac>, l = <zipRedac><zipRedac><zipRedac>, sim = \fixed\"", _x0007_    formula = y ~ X<zipRedac> + X2)_x0007__x0007__x0007_Bootstrap Statistics :_x0007_     original      bias    std. error_x0007_t<zipRedac>* <zipRedac>.<phoneRedac> <zipRedac>.<zipRedac><zipRedac><zipRedac>679755  <zipRedac>.<zipRedac><phoneRedac>_x0007_t2* <zipRedac>.<zipRedac>453263 <zipRedac>.<zipRedac><zipRedac><phoneRedac>  <zipRedac>.<phoneRedac><zipRedac>_x0007_t3* <zipRedac>.<phoneRedac> <zipRedac>.<zipRedac><phoneRedac>7  <zipRedac>.<phoneRedac>3""",1,0,1
"Sure, just prepend htmlpreview.github.io before the link to the html file if you're browsing via Github's website. For example, https://github.com/<nameRedac_<anon_screen_name_redacted>>/stat-learning/blob/master/ch5/5.html becomes http://htmlpreview.github.io/?https://github.com/<nameRedac_<anon_screen_name_redacted>>/stat-learning/blob/master/ch5/5.html",1,0,1
"Both from The Atlantic._x0007__x0007_First, \[The Dawn of the Age of Artificial Intelligence][1]\"" by Erik Brynjolfsson and Andrew McAfee contains this quote:_x0007__x0007_[snip]_x0007__x0007_We do have one quibble with Simon, however. He wrote that, “The main fuel to speed the world’s progress is our stock of knowledge, and the brake is our lack of imagination.” We agree about the fuel but disagree about the brake. The main impediment to progress has been that, until quite recently, a sizable portion of the world’s people had no effective way to access the world’s stock of knowledge or to add to it._x0007__x0007_In the industrialized West we have long been accustomed to having libraries, telephones, and computers at our disposal, but these have been unimaginable luxuries to the people of the developing world. That situation is rapidly changing. In <zipRedac>000, for example, there were approximately seven hundred million mobile phone subscriptions in the world, fewer than 30 percent of which were in developing countries._x0007__x0007_[/snip]_x0007__x0007_I my opinion, statistical learning techniques will be required to efficently analyze this data._x0007__x0007_The second article is \""[When You Fall in Love, This Is What Facebook Sees][<zipRedac>]\"" by  Robinson Meyer.  Note the graphs (both done in R using \""ggplot<zipRedac>\""!) and how one might use statistical learning to automatically classify people in a relationship._x0007__x0007_We live in a world of wonder._x0007__x0007__x0007_  [1]: http://www.theatlantic.com/business/archive/<zipRedac>014/0<zipRedac>/the-dawn-of-the-age-of-artificial-intelligence/<zipRedac>83730/_x0007_  [<zipRedac>]: http://www.theatlantic.com/technology/archive/<zipRedac>014/0<zipRedac>/when-you-fall-in-love-this-is-what-facebook-sees/<zipRedac>83865/""",1,0,1
"See [here][1].  Dividing by n-1 gives an unbiased estimate of the sample variance, but NOT of the sample standard deviation.  Taking the square root introduces some bias.  _x0007__x0007__x0007_  [1]: http://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation",0,0,1
"@alhf appreciate the post, but seemed to me to hinge on a semantic issue of \zero deviation\"" vs \""no deviation\"" combined with the concept of degrees of freedom which wasn't totally clear. so couldn't really put me over the top in terms of understanding.",0,0,1
"Understood, Thanks",1,0,1
"more generally, `predict` will use your fitted model like `fit1` to predict values for new data in a `data.frame`. apparently, if this code runs, your model was only fit on a single predictor variable `lstat` so `predict` needs that named column in a `data.frame` to come up with predicted values.",1,0,1
Do you mean bootstrapping?,1,0,1
"Thanks, corrected the post.",1,0,1
"Say you have a large number of categorical predictors x and you fit a model using lasso that picks the best components of x that influence the dependent variable y. As discussed in the lecture lasso will tend to throw out correlated dimensions of x. _x0007__x0007_----------_x0007_After the model is build, can one use it and 'PCA style' identify groups of components of x that are clustered together and drove y?_x0007__x0007__x0007_----------_x0007__x0007__x0007_For example in genetics, for have genes x1, x2, x3, x4 … and for example x1 is correlated with x2 and x3 goes with x4. So lasso should keep either x1 or x2, and x3 or x4, but not all dimensions of x as predictors. So assuming lasso keeps x1 and x3. How can one recover that x1 and x2 and similarly x3 and x4 are related in predicting y, thus clustering those dimensions together? Is there a standard approach for doing this kind of study?",1,1,1
The equation is the same for FS and BS.,1,0,1
"Imagine for example a data set with one data point per day. If you were to do normal bootstrapping with this, one bootstrap sample might have the data point for 4 May followed by the data point for 25 December followed by the data point for 15 March, and so on. In time series data, subsequent data points are often not entirely independent, so normal bootstrapping would mean losing these dependencies in the noise._x0007__x0007_To get around that problem at least a little, block bootstrapping groups data points into blocks (of equal size, ideally), and these blocks are then randomly recombined into bootstrap samples. For example, with a data set of one point per day, the block size could be 7, i.e. one week. A bootstrap sample might then have the data points for 27 February to 5 May, followed by the data points for 23 to 30 December, followed by the data points for 11 to 17 March, etc. Some dependencies are still lost (the data of 11 March follows that of 30 December, though it can't possibly depend on that), but the dependecies within blocks (of 12 March on 11 March, of 13 March on 11 and 12 March, etc.) are retained.",1,0,0
"In PCA you group the independent variables, the x's or features, into factors that maximize the variance of those new variables. However, it does not at all take into account how x relates to the dependent variable y. _x0007__x0007__x0007_----------_x0007__x0007_The advantage of PCA is that it can help to better visualize the dynamics of variability of the x variables, and that's often interesting by itself._x0007__x0007__x0007_----------_x0007__x0007__x0007_As an example where PCA is often used in financial risk measurements. Imagine the x dimensions represent interest rates for loan tenors, so x1 is the rate for a one year loan, just making up some numbers, say 1%, x2 is the rate for a two year loan etc., and you write down the vector x as you observe those rates in the market today:_x0007__x0007__x0007_----------_x0007__x0007__x0007_x(today) = (%1, %1.5, %2, %4, %5)_x0007__x0007__x0007_----------_x0007__x0007_Now, in finance those vectors are called a 'yield-curve', one field of study is to record instances of it into a large matrix and observing how the curve changes over time. A PCA analysis would then be used to visualize the dynamics of the curve. For example the first two factors would possibly look like:_x0007__x0007__x0007_----------_x0007__x0007_F1 = (%5, %3, %2, %1, %1)_x0007_F2 = (%5, %3, 0, -%2, -%1)_x0007_ _x0007_----------_x0007_The first factor shows that the curve thens to shift up or down in parallel, but the short tenors are more volatile. The second factor represents a tilting, where the front goes opposite to the far end. The goal for those models is then to describe the curve dynamics entirely in terms of those factors._x0007__x0007__x0007_----------_x0007__x0007_So going back to the original question, in my understanding PCA is more about the dynamics of the regressors and does less in terms of learning from the responses. However, if it works, for example one finds that most of the response can be explained by only a few PCA factors, the PCA coefficients, like in the finance example, are quite illustrative in terms of explaining the model._x0007__x0007__x0007_----------",1,0,0
"I had the same question as well.  My guess was that after choosing the number of parameters to use from the cross-validation results, you would then repeat the same model fitting process, whatever that may be (stepwise selection, best subset, etc.) using the chosen number of parameters on the entire data set.  That model would be your final model.  I think the bottom of slide 41 in Chapter 6 confirms this idea.",1,0,1
"What are the \requirements\""? Are they spelled out anywhere?""",1,1,1
"I was really enjoying the lectures with Rob and Daniela when I idly googled Prof. <redacted> and discovered she is the daughter of <redacted> <redacted>.  As a former math student I was acquainted with the name.  They are probably tired of hearing it, but for those who don't know, <redacted> <redacted> is one of the great math/physics super geniuses of our time.  Just thought I'd pass that along.  :)",1,0,1
http://www.sns.ias.edu/~witten_x0007__x0007_http://en.wikipedia.org/wiki/Edward_Witten,1,0,1
"None of these is correct. Try 2^p as shown in the notes (at least for 6.3.R1, haven't figured out the second part yet even though the formula should be the same -- but the system will not accept both answers as equal)",1,0,1
"How about this argument: while the distance for LDA may increase, the absolute distance is not so important. Since LDA is based on Normal density functions, the main concern is a new observation's z score. The assumption is that the sigma if the distribution increases as the dimensions increase, so the z score remains the same. Thus, LDA would not suffer as much as KNN from higher dimensionality.     _x0007__x0007_Note -- I don't know how increasing dimensionality would affect a z score. I'm assuming the 1-2-3 sigma distances would expand along with the dimensions; therefore, while a given point's absolute distance from the centroid may increase, it's z score would stay the same, or if it were 1 sigma away in 2-D, it would remain 1 sigma away also in 5-d. If anyone knows and confirm or dispute this assumption, it would be greatly appreciated, it would help me to learn.",1,1,1
"\in high dimensions, almost all pairs of points are equally far away from one another.\""     _x0007_Does this mean the bell curve at lower dimensions eventually becomes uniform in high dimensions? Then that would probably undo the average or centroid of the LDA where most of the data points group together.""",1,1,1
Thanks Robert,1,0,1
I am looking at this problem and I am not sure where to even start. I mean I can guess that it should be somewhere in the order of 90-95% but I have no idea how to get an actual answer. How did other people start this off?,1,1,1
"Thanks. Yes, I had tried what you suggested but it didn't work. However, when I rebooted my pc the same syntax worked just fine. (mac users would say \that's a pc for you.\"" LOL). Thanks again.""",1,0,0
"The question ask me to estimate s.e.(?^1). But I have done it a lof of times using the method of block bootstrap and got wrong answers. By the way, the correct answer seems quite unreasonable(too large). _x0007__x0007_My code and result are shown below:_x0007__x0007_     lm2.fn <- function(tsb) {_x0007_      lm2 <- lm(y ~ X1 + X2, data = tsb)_x0007_      coef(summary(lm2))[5] _x0007_    }_x0007_    > boot.block<-tsboot(Xy,lm2.fn,R=1<zipRedac><zipRedac>,sim=\fixed\"",l=1<zipRedac><zipRedac>)_x0007_    > boot.block_x0007_    _x0007_    BLOCK BOOTSTRAP FOR TIME SERIES_x0007_    _x0007_    Fixed Block Length of 1<zipRedac><zipRedac> _x0007_    _x0007_    Call:_x0007_    tsboot(tseries = Xy, statistic = lm2.fn, R = 1<zipRedac><zipRedac>, l = 1<zipRedac><zipRedac>, sim = \""fixed\"")_x0007_    _x0007_    _x0007_    Bootstrap Statistics :_x0007_          original       bias    std. error_x0007_    t1* <zipRedac>.<zipRedac><phoneRedac> -<zipRedac>.<zipRedac><zipRedac>14<zipRedac>5983 <zipRedac>.<zipRedac><zipRedac><phoneRedac>_x0007_    > mean(boot.block$t)_x0007_    [1] <zipRedac>.<zipRedac><phoneRedac>""",1,1,1
Hi_x0007__x0007_Xy data frame has to columns._x0007_Example from lecture uses data with one column for X and one column for Y_x0007__x0007_What is the alpha formula for this case?_x0007__x0007_How boot function will understand what is predictor and what is outcome value?,1,1,1
I'm stuck.  Where did you find 5.R.RData?,1,1,1
"I guess \null\"" models are not counted. :D.""",1,0,1
"Thanks for the suggestions Gavin; however, even after using your suggestions, I am not getting the \correct\"" answers. Any thoughts about what might be the issue?""",1,1,1
"Ok the course is free so I won't complain, rather I will offer a suggestion.  It would be helpful to list all of the packages required in advance. For example, in chapter 6, I need knitr, which needs formatR.  Now I need leaps.",0,1,1
"since this is apparently the WRONG way of figuring this, I guess it's ok to show it. (If it's wrong, it's not giving the answer, right?) ... so... any reason why this formula in R gives <zipRedac>0% but is still wrong?_x0007_> -10.6<zipRedac>13 + (0.00<zipRedac><zipRedac> * 2027.<zipRedac>1)_x0007_[1] 0.<zipRedac>0000<zipRedac>",1,1,1
"In the Lecture Slides PDF, there is an equation, and the values to plug into that equation.",1,0,1
"That puzzled me too._x0007__x0007_(Although having Adjusted $R^2$ > 1 is not what concerns me.)_x0007__x0007_----_x0007__x0007_In lecture 6.4, at about time 11:50, on slide 22/57 \Adjusted $R^2$\""._x0007__x0007_> For a least squares model with d variables, the adjusted_x0007_> $R^2$ statistic is calculated as_x0007_> _x0007_> $ mathrm{Adjusted}R^2 = 1 - {RSS/(n-d-1) over TSS/(n-1)}$_x0007__x0007_They say that the Adjusted $R^2$ does not require an_x0007_estimate of $hatsigma^2$, so it could, in principle, be_x0007_used where the number of variables, $d$, is greater than the_x0007_number of observations, $n$._x0007__x0007_In practice, if $d>n$, unless there were enough perfect_x0007_correlations[*] among the $d$ variables, the model with $d$_x0007_variables could be made to fit the data *perfectly*. (In_x0007_fact, there would be multiple different sets of coefficients_x0007_which give a perfect fit.)_x0007__x0007_This would mean $RSS=0$ so the Adjusted $R^2$ would be 1 for_x0007_all such models._x0007_So the Adjusted $R^2$ would not distinguish between such_x0007_models._x0007__x0007_Also, the Adjusted $R^2$ of 1 for such models would_x0007_generally \""beat\"" the adjusted $R^2$ of less than 1 for any_x0007_model where $d<n$[**]. _x0007__x0007_So the Adjusted $R^2$ would \""choose\"" a model where $d>n$_x0007_even though such a model is most likely overfitted._x0007__x0007_So I don't see how adjusted $R^2$ could be used for a least_x0007_squares model, where $d>n$.  _x0007__x0007_(Even trying to fit such a model would give problems.)_x0007__x0007_----_x0007__x0007_[*] Even if there was a lot of correlation, just a small amount_x0007_of noise would prevent it from being perfect.  And if it was perfect, _x0007_the multicolinearity would mess up the fitting process._x0007__x0007_[**] Unless the $d<n$ model was very lucky and managed to_x0007_get an Adjusted $R^2$ of 1 as well.""",1,1,1
Here's another thread that may help. _x0007__x0007_[Hypercube question thread][1]_x0007__x0007__x0007_  [1]: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/52df<zipRedac>138182f4b<zipRedac>ffc<zipRedac><zipRedac><zipRedac><zipRedac>19,1,0,1
"Should you not use R=1000, l=100 and return the std.error as asked.",1,0,1
I can not download 6.4 video. There is no download button.,1,1,1
"alpha=function(formula,data,indices){_x0007_  _x0007_  d<-data[indices,]  _x0007_  fit<-lm(formula,data=d)  _x0007_  return(fit$coef[2])  _x0007_}_x0007__x0007__x0007_result<-tsboot(tseries=Xy,statistic=alpha,R=100,l=100,formula=y~X1+X2,sim = \fixed\"")""",1,0,1
"Hi,_x0007__x0007_Can someone post the instructions in R of how to get the scatter plots for Fisher Iris example? Would be great if the instructions would generate the same plots as in the slides._x0007__x0007_Thanks!",1,1,1
"Quiz for 6.9 Dimension Reduction Methods is missing, only the Video Lecture shows up.",1,1,1
pairs(iris),1,0,1
Counted as zero,1,0,1
"So I figure non overlapping blocks all starting at eg 201. then you have 10 blocks, but with bootstrapping 1/3 of the blocks are repeats. _x0007__x0007_Then I calc the s.d of these 10 blocks?",1,1,1
"The problem with 5R, parts 3 and 4, as I see it, is the way the question is phrased, asking for an answer \within 10%\"".  In this case, the 10% applies to the answer that the author got when performing this experiment 1 time.  But the bootstrap method itself depends on the state of the experimenter's random number generator (RNG) at the time the experiment is run; thus the published answer depends directly on how the author's RNG was seeded and how many times the RNG had been called prior to running **boot()**._x0007__x0007_Is it necessarily the case that all students in the class will get such an answer?  You can see from the above that a student's answer will depend on her RNG.  When I tested this for part 3, by running the experiment 300 times and keeping up with all the estimates for std. err., I found that about 3% of the values returned from **boot()** exceeded the 10% limit (relative to the published value).  The reason for this is that the estimate of the std. err. used by the author just happened to be a rather unlikely low value, more than 2 std deviations away from the mean of all the estimates of the std err in my sample of 300.""",1,1,1
"Question 3.5 from the book says:_x0007__x0007_> Consider the fitted values that result from performing linear_x0007_> regression without an intercept..._x0007_> _x0007_> Note: We interpret this result by saying that the *fitted values from_x0007_> linear regression are linear combinations of the response values*._x0007__x0007_My question is: so what? I can do some algebra and get an equation expressing the fitted values as a linear combination of the response values. However, why would I ever want to do that? I want to know the relationship of the fitted values to the observed data. I don't understand the deeper point of this question. Help?",1,1,1
I downloaded this file. There is no y and X1 or X2 in that!!! I'm stuck.,1,1,1
"I got the answer by working out the formula, but I'm more curious how you leveraged WolframAlpha to do this. This is a fascinating search engine and I'd love to try it further with examples of this nature. Any hints or tips or what you actually put in would be valuable. Thanks.",1,1,1
which page number?,1,0,1
"I don't really know, but you may want to look at some of Efron's recent work.  He hinted in his interview here that he thinks there is a close connection between the bootstrap and Bayesian inference (I believe he said he was working on a book that explores that connection).  You may also be interested in this classic paper by Donald Rubin on a Bayesian version of the bootstrap that came out just a couple years after Efron's original paper:_x0007__x0007_http://projecteuclid.org/euclid.aos/11<phoneRedac>8_x0007__x0007_Hope that helps.",1,0,1
"I am sorry, I must have misread it, he did not. But he did win a ton of other prizes as listed here:_x0007_http://en.wikipedia.org/wiki/Edward_Witten",1,0,1
"Like this?_x0007__x0007_    > boot.block<-tsboot(Xy,lm2.fn,R=1<zipRedac><zipRedac><zipRedac>,sim=\fixed\"",l=1<zipRedac><zipRedac>)_x0007_    > boot.block_x0007_    _x0007_    BLOCK BOOTSTRAP FOR TIME SERIES_x0007_    _x0007_    Fixed Block Length of 1<zipRedac><zipRedac> _x0007_    _x0007_    Call:_x0007_    tsboot(tseries = Xy, statistic = lm2.fn, R = 1<zipRedac><zipRedac><zipRedac>, l = 1<zipRedac><zipRedac>, sim = \""fixed\"")_x0007_    _x0007_    _x0007_    Bootstrap Statistics :_x0007_          original       bias    std. error_x0007_    t1* <zipRedac>.<zipRedac><phoneRedac> -<zipRedac>.<zipRedac><zipRedac>1246918 <zipRedac>.<zipRedac><zipRedac><phoneRedac>_x0007_    > mean(boot.block$t)_x0007_    [1] <zipRedac>.<zipRedac><phoneRedac>""",1,0,1
"Yeah, you are right! I test it with the R3,too. It seems reasonable. Thank you!",1,0,1
Thank you!,1,0,1
"Yeah, the RNG will influence the result but here I just misunderstood what the question is trying to ask.",1,0,1
"How does this look? Compare it to the \alpha\"" code:_x0007__x0007_beta<-function(data){_x0007_  myfit<-lm(data[,3]~data[,1]+data[,2])_x0007_  return(summary(myfit)$coefficients[2,1])_x0007_  }_x0007__x0007_beta.fun<-function(data,index){_x0007_  with(data[index,],beta(data[index,]))_x0007_  }_x0007__x0007_betas<-boot(Xy,beta.fun,R=1000)""",1,1,1
"Same with me , <nameRedac_<anon_screen_name_redacted>>",1,0,1
"No worries.  I know there has been talk about him perhaps receiving a Nobel and I thought maybe I had missed it somehow.  Nevertheless, he is certainly one of the great minds of out time.",1,0,1
"Could someone please explain to me how I get the model for question 3.5.R1 I have tried to answer the question but I can't wrap my head around it? _x0007_Thank you,",1,1,1
"Thanks for that. It's very helpfull to me, since i have to use SAS on my Job._x0007__x0007_An additional suggestion:_x0007__x0007_if you specify the OUTHITS option, the output data set will include a distinct copy of each selected unit. For example, a unit that is selected three times is represented by three copies in the output data set, so you don't have to use  the 'NumberHits'-Variable in subsequent Procs.",1,0,1
"Sorry about that. It was edited, and left as \private\"" by mistake. I have now made it public.""",1,0,1
I rebooted RStudio and the problem went away.   Never mind!   :-),1,0,1
>ls()_x0007__x0007_>fix(Xy),1,0,1
Thanks all for the comments.,1,0,1
"A useful way of inspecting data frames without getting drowned in the response is:_x0007__x0007_head(Xy)_x0007__x0007_By default, this will give you the first 6 rows, but you can change that amount if you want.  Also, there is its related function, tail().",1,0,0
"This is the right idea, but I found you can do this with one function:_x0007__x0007_b1fn <- function(data, index) {_x0007_  with(data[index,],lm(y ~ X1 + X2)$coefficients[2])_x0007_}",1,0,1
The fact that the fitted values are a linear combination of the response values allows us to figure out the distribution of the fitted values since we know the distribution of the response values.,1,0,0
"I understand that this forum is not laid out in the best possible way, but please make an attempt to check whether a question has been asked before you post one._x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/53<zipRedac>1d73fd<zipRedac>4322b<phoneRedac><zipRedac>9",0,0,1
"Interesting reference, thanks.",1,0,1
"I have seen these Bias and Variance graphs multiple times throughout the book. I never figured out how training or test data is used (or in other cases if the true model that generated the training data is even involved). How are the data for these graphs computed really?_x0007__x0007_Take Figure 6.5 as an example._x0007__x0007_![Figure 6.5 from ISL][1]_x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>77.png_x0007__x0007_For example, Wikipedia says Bias is E[f_i - E(f_hat_i)]^2  But if I am not mistaken, all these terms need to be estimated? But how?_x0007__x0007_I just need an equation that explicitly explains how test data or training data is used. I can handle x_ij or Sigma notation. I just don't know if it's right to obtain E(f_hat_i) by averaging over predicted values y_hat_i from all training data x_i (vector).",0,1,1
Re-fit the lasso or ridge regression with the whole training set to get one set of coefficients.,1,0,0
"In the univariate case, regressing y on x with an intercept, a formula for the slope coefficient is $hat_x0008_eta_1 = r_{xy}frac{s_y}{s_x}$, where $r_{xy}$ is the correlation between x and y, $s_y$ is the standard deviation of y and $s_x$ is the standard deviation of x._x0007__x0007_In the multivariate case, it is more complicated.",1,0,0
Check again. The quiz has since been made available.,1,0,1
"Hi, if you are still on it, the previous comment from Soaring helped me a lot:_x0007__x0007__x0007_    _x0007__x0007_  _x0007__x0007_>  First of all, the grade \A\"" is a qualitative variable, which_x0007_>     automatically eliminates at least one of the tools.    Second, this is a classification problem._x0007_>       Third, think what kind of likelihoods applies to get the probabilities._x0007_> _x0007_>     And finally, go one more time through the lecture notes and you should be able to find a clue._x0007__x0007_Well, in the first stept, you must eliminate a method not appropiate for qualitative variables... thats important, the rest it is easy._x0007__x0007_Hope this helps!""",1,0,0
"I've gone to the StatLearning site and it's not there.  I did a library(\ISLR\"") and that didn't help.  Where is the file?  Other courseware files coming up hiding somewhere else?_x0007__x0007_Thanks.""",1,1,1
DW -- the question includes a link. Couldn't see it on my tablet. See below. (Sorry just seeing this.),1,0,1
Thanks a lot,1,0,1
"Also, as far as I understand (but could be wrong), in the univariate case $R^2 = r^2$, i.e. the square root of the $R^2$ statistic is the correlation between $x$ and $y$.",1,0,1
good tip - thanks.,1,0,1
"I would like to know when the office hours are as well, @smgross.  Thanks!",1,1,1
Sweet! This is a much nicer workaround._x0007__x0007_However this obscures all of the controls and doesn't let me pull up the closed-captions for skipping back to replay a section.,1,1,1
"Start here: [<zipRedac>.R.R4 thread][1] _x0007__x0007_But it's really pretty much an adaptation of, so this may help too: [<zipRedac>.R.R3 thread][2]_x0007__x0007__x0007_  [1]: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-quiz-review/threads/<zipRedac>3<zipRedac>14e4c8899b2<zipRedac>98c<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_  [2]: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-quiz-review/threads/<phoneRedac><zipRedac>767<zipRedac>a1ed<phoneRedac>2",1,0,1
Awesome to hear that! :),1,0,1
I believe you are confusing $K$-fold with the $k$ predictor variables asked by the OP.,1,0,1
you can also run a whole chunk at once by Ctrl+Alt+C / Command+Option+C. https://www.rstudio.com/ide/docs/using/keyboard_shortcuts,1,0,1
"I agree with jaxx.  BRM explanation is right, except there is a typo.  Thanks for the hint, BRM, and thanks for relentlessly pointing us back to it.",1,0,0
"If you are struggling with this one, follow the hint above given by BRM but notice that there is a typo as correctly noted by jaxx.",1,0,1
My reading of tsboot() is that the statistic function does not take the same_x0007_arguments as the boot() statistic function;  it only takes the data (time series).,1,0,1
"Hi,_x0007_I believe, the problem with this approach would be that lasso will eliminate a variable also on the grounds that it is not related to the dependent variable, so there is no clear indication of clustering in the lasso results.",1,0,1
"I saw that thread, but it is still people offering their interpretations and multiple variations on possible code._x0007__x0007_Since the question has evidently caused some confusion, I would still find it helpful for an explanation by one of the course supervisors.",1,1,1
"<redacted>:_x0007__x0007_> I'm trying to check for correlation between successive rows,_x0007_> but can't figure it out just by looking. I suppose one could_x0007_> formally calculate (i.e. using Durbin-Watson statistic for_x0007_> auto-correlation) ... but I think the spirit of the ",1,0,1
"Hi, I'm <nameRedac_<anon_screen_name_redacted>>, on the faculty of Seoul National University, where I teach and research Korean social history of the Choson period (1<phoneRedac>). Thanks much for the great course.  It's a genuine educational luxury to learn R, the bootstrap, and the lasso from their early pioneers!",1,0,1
"On page 250 of the textbook (page 265 of the textbook PDF), here is the code used to set up the folds for 10-fold cross-validation:_x0007__x0007_    k=10_x0007_    set.seed(1)_x0007_    folds=sample(1:k,nrow(Hitters),replace=TRUE)_x0007__x0007_If you run that code, you will see that the folds produced are not even close to being equal sizes. The smallest fold has 13 observations and the largest fold has 33 observations._x0007__x0007_Thankfully, Dr. Hastie (in the video) uses a different method that actually creates equal-sized folds._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Could someone explain this sentence?_x0007__x0007_\CV does charge for the aggressive search of best subset, and the cost could be higher than for forward stepwise.\""_x0007__x0007_I'm not clear on what Dr. Hastie means by \""charge\"" and \""cost\"". Thanks!""",1,1,1
"You bring up a good point. Fortunately, if your training set is big, the differences between fold size diminishes:_x0007__x0007_    > k=<zipRedac>0_x0007_    > set.seed(<zipRedac>)_x0007_    > folds=sample(<zipRedac>:k,<zipRedac>000000,replace=TRUE)_x0007_    > table(folds)_x0007_    folds_x0007_         <zipRedac>      2      3      4      5      6      7      8      9     <zipRedac>0 _x0007_     99<phoneRedac>56  99<phoneRedac><zipRedac>  99<phoneRedac>4<zipRedac> <zipRedac>00<phoneRedac>5 <zipRedac>00<phoneRedac>3 _x0007__x0007_If your fold sizes have substantial variance, as you mentioned <zipRedac>3 obs vs 33 obs, you can take a weighted average of the per-fold MSE._x0007__x0007_$$MSE_{cv} = sum_{k=<zipRedac>}^{<zipRedac>0} frac{n_k}{n}MSE_k$$",1,0,1
"I liked the approach that whenov took in the 5.R.R3 thread:_x0007__x0007_    boot(Xy, function(data,index){lm(y~.,data[index,])$coefficients[2]}, R=1000)_x0007__x0007_And so used the same approach for R4:_x0007__x0007_    tsboot(Xy,function(d,i){lm(y~.,d[i,])$coefficients[2]},R=1000,sim='fixed',l=100)",1,0,1
Thanks! silly errors.,1,0,0
I am facing the same problem. Can you please tell me how many decimal digits are expected in 4.3.R1? Much appreciated.,1,1,1
"Hi,_x0007_As per the question, two data sets are different if they have the same points but in different order. For example if n = 3, then six combinations are possible. I have this question if datasets if n-1, n-2 and so on are also permitted. Since factorial doesn't seem to be the answer. Correct me if I am wrong._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"If 	heta_hat is the estimator of 	heta, then bias of the estimate = E[	heta_hat]-	heta.",1,0,1
"> Blockquote As per the question, two data sets are different if they have the same points but in different order_x0007_> Blockquote_x0007__x0007_If you have 3 data points:1,2,3. Then 111,222,112,211,123,231,132,..... these are all possible resamples. So in this case, the answer is much more than 6 = 3! (! means FACTORIAL). In factorial, things are NOT repeated. _x0007__x0007_To answer the question, you may want to work out the problem for n=3. Then see how you can generalize the result.",1,0,0
"Please help me with the following:_x0007__x0007_i have a  classification setting (2 Classes), over 300 Predictors and 10.000 cases._x0007__x0007_I understood, that it would be wrong to use ALL THE DATA  for variable selection (prescreening) an then pass them over to Model Selection using Cross-Validation, for example._x0007__x0007_But  what about the following 2-Step approach:_x0007__x0007_Step 1: I use repeated k-Fold Cross-Validation (5 repeats, k=10) for calculating the Information Value of each predictor (actually i calculate an adjusted Information Value by introducing a penalty based on the discrepancy between training and validation data )._x0007_At the end of Step 1 i have 50 estimates for the Information Value of each Predictor. I average them, calculate the standard error in order to get the t-statistic and p-values, which help me deciding, whether the Information Value for a given predictor is different from zero._x0007__x0007_Step 2: I use all the Predictors from Step 1 which have p-value < 0.05 for further Modeling. Again, i use k-fold cross-validation (different seeds as in Step 1) and forward stepwise Selection to chose the Model with lowest test error.  _x0007__x0007_Is this approach wrong or can i just separate the process of prescreening the variables and model selection by using cross-validation sequentially?_x0007__x0007_Please leave some comments on that._x0007__x0007_Thank You very much.",1,1,1
Is the expression we are to minimize in answering the questions in Quiz 6 written correctly?  I think not...,0,1,1
What would happen if you simply didn't consider the variables with any (or at least large numbers of) NAs? How many of the 800 initial predictors would that leave you with?,1,1,1
"So, does it mean that regsubsets() is not valid in the context of logisitc regressions?",1,1,1
One of the issues here is that it is easy to get exactly the right answer for 5.R.R3 using completely the wrong approach (which then puts you on the back foot for 5.R.R4) -- thanks for the jolt back to reality.,1,1,1
"I'm looking at the documentation, and it does not mention glm or logistic models. http://cran.r-project.org/web/packages/leaps/leaps.pdf",1,1,1
"will this course be offered again this year ?_x0007_I am unable to finish this right now as I have exams coming up. _x0007__x0007_I would love for it to be offered again. Say, July ;)_x0007__x0007_Thanks.",1,1,1
I suggest filling in the mean for missings and then creating a dummy variable that says you filled in a missing.,1,0,1
The full LS model is equivalent to pcr with ALL of the components included.,1,0,0
bestglm(),1,0,1
"ightarrow infty$, the probability of such an event becomes 0.""",1,0,1
The problem is illustrating one reason ordinary regression may be inappropriate for the data.,1,0,1
"The problem has been corrected (the RSS term was missing a square).  In general, the more specific you can be in describing bugs, the faster they will be fixed._x0007__x0007_Thanks!",1,0,0
"Yes there are problems, each section contains lectures AND quiz problems.  There is a menu bar towards the top of the courseware view.  See other discussions or an edx tutorial for details.",1,0,1
The former,1,0,1
"Hi all,_x0007__x0007_Thanks for the many helpful comments regarding quiz questions.  As I'm sure anyone who has teaching experience can vouch, it is VERY difficult to write good questions.  Especially for a class where people will have diverse levels of experience.  We wanted to make the class as interesting as possible for a range of participants.  _x0007__x0007_We also tried to be fairly lenient in the grading policy so that people wouldn't be put off by some of the more intense problems (like the hypercube).",1,0,1
"Again, in any regression problem (here I am using regression in the sense of logistic regression, not regression vs classification, do as I say not as I do) $Y$ is random but $X$ is not.",1,0,0
"Good point Phil, unfortunately none of the course staff actually control the software.  I would encourage you to pass along any constructive feedback to the stanford edx staff.  Especially if you plan to continue to enjoy free courses!",1,0,1
"Very interesting, thanks, and perhaps predictable for stepwise regression with a messy set of regressors, given the larger penalty term that BIC incorporates. I am more interested, though, in whether AIC would indeed, in general, select better than BIC among all sets of possible regressors and whether its elegant connections to information theory and information geometry are borne out.",1,1,0
"Also, I should clarify my interest in AIC has been spurred by the need to select among parametric survival models (e.g. exponential, Weibull, generalized gamma, etc.) in my own work. I've been implicitly relying on AIC but wonder if so doing is overly dogmatic.",1,0,1
"Just wondering if anyone had a recommendation? I have been using the word formula editor, but I find it cumbersome",1,1,1
"> The smallest fold has 13 observations and the largest fold has 33 observations._x0007__x0007_Really? I copied and pasted that code (including the set.seed) and ran_x0007__x0007_        > table(folds)_x0007_    folds  _x0007_    1  2  3  4  5  6  7  8  9 10  _x0007_    22 39 36 39 38 29 31 35 25 28 _x0007__x0007_Though I agree, not ideal.",1,0,1
"It is emphasized that all features must be included when data is separated into K folds for model training. This makes sense, but I wonder what how that thought extends to additional higher order features (i.e., X1*X2, X1^2)? Shouldn't those be initially included as well, in theory?",1,1,1
Not sure how to visualize how RSS varies as a function of the Lasso tuning parameter Lambda?_x0007__x0007_Any suggestions?,1,1,1
"\Obviously not right\"" is wrong. :-)_x0007__x0007_Look at the equations on slide 6.""",0,0,1
"I've done a few simulations of very simple situations to_x0007_help with intuitive understanding of linear discriminant_x0007_plots.  They might be of interest to others who have been_x0007_thinking about this._x0007_    _x0007_----_x0007_## Example <zipRedac>_x0007__x0007_![LDA example <zipRedac> plots][<zipRedac>]_x0007__x0007_On the left is a plot of the data in terms of x<zipRedac> and x2._x0007__x0007_On the right is the linear discriminant plot._x0007__x0007_There are two predictor variables, x<zipRedac> and x2._x0007__x0007_There are 4 groups (<zipRedac>,2,3,4)._x0007__x0007_The variance in the x<zipRedac> direction and x2 direction are both <zipRedac>_x0007_(and there is no correlation)._x0007__x0007_The groups have been widely separated to make it clear_x0007_what's going on._x0007__x0007_Note the separation of group means (the centres the separate_x0007_group \clouds\"")._x0007_This separation is greater in the x<zipRedac> direction_x0007_(between groups <zipRedac> and 2, or between groups 3 and 4)_x0007_is bigger than the separation in the x2 direction_x0007_(between groups <zipRedac> and 3, or between groups 2 and 4)._x0007__x0007_Here, the linear discriminant plot shows bigger separation_x0007_between groups <zipRedac> and 2 than between groups <zipRedac> and 3._x0007__x0007_R output gives:_x0007__x0007_    Coefficients of linear discriminants:_x0007_              LD<zipRedac>        LD2_x0007_    x<zipRedac>  <zipRedac>.<zipRedac>9427<zipRedac>3 <zipRedac>.<zipRedac><phoneRedac> -<zipRedac>.<zipRedac>4288<zipRedac><zipRedac> <zipRedac>.<phoneRedac>6_x0007__x0007_So LD<zipRedac>, the first linear discriminant consists mostly of x<zipRedac>._x0007_And LD2, the second linear discriminant consists mostly of x2._x0007_The \""directions\"" of the linear discriminants are pretty much_x0007_the directions of the original predictor variables x<zipRedac> and_x0007_x2._x0007_LD<zipRedac> is the direction of greatest separation of the groups._x0007_(which here is pretty much along the x<zipRedac> axis)_x0007__x0007_----    _x0007_## Example 2_x0007__x0007_![LDA example 2 plots][2]_x0007__x0007_This is similar to example <zipRedac>.  The group means are the same, and_x0007_so their separation, as before, is greater in the x<zipRedac> direction_x0007_than in the x2 direction._x0007__x0007_But here, the variance in the x<zipRedac> direction is much larger_x0007_than the x2 direction.  So despite the greater separation of_x0007_group *centres* in the x<zipRedac> direction, the greater variance in_x0007_that direction makes it appear that the separation is not as_x0007_\""clean\"" in that x<zipRedac> direction as in the x2 direction._x0007__x0007_The linear discriminant plot reflects this — it shows the_x0007_\""distance\"" (taking the variance into account) between groups_x0007_<zipRedac> and 3 to be greater than between groups <zipRedac> and 2._x0007__x0007_R outpu""",1,0,0
"Here's some code for the sort of simulations above (for_x0007_those who want to try themselves). It's easy to adjust it to_x0007_change group means, variance, covariance, or number of_x0007_groups._x0007__x0007_This is just to get \set up\"":_x0007__x0007_    require(MASS)_x0007_    sim.groups <- function(mu,Sigma=matrix(c(1, 0, 0, 1)),n=100) {_x0007_      mu <- as.matrix(mu)_x0007_      ngroups <- dim(mu)[1]_x0007_      x <- data.frame()_x0007_      for(i in 1:ngroups) {_x0007_        group.data <- cbind(group=rep(i,times=n),  mvrnorm(n=n, mu=mu[i,], Sigma=Sigma))_x0007_        x <- rbind(x, group.data)_x0007_      }_x0007_      x_x0007_    }_x0007__x0007_This is to do the first example:_x0007__x0007_    ## Adjust the 2 lines below as required    _x0007_    mu <- data.frame(x1=c(10,30,10,30), x2=c(10,10,20,20))_x0007_    Sigma <- matrix(c(1, 0, 0, 1), nrow=2)_x0007_    _x0007_    x <- sim.groups(mu, Sigma, 20)_x0007_    plot(x2 ~ x1 , data=x, col=group, pch=as.character(group))_x0007_    _x0007_    lda.fit <- lda(group ~ x1 + x2, data=x)_x0007_    lda.fit_x0007_    plot(lda.fit)_x0007__x0007_Second example is similar, but with these changes:_x0007_    _x0007_    mu <- data.frame(x1=c(10,30,10,30), x2=c(10,10,20,20))_x0007_    Sigma <- matrix(c(16, 0, 0, 1), nrow=2)_x0007_    _x0007_Third example is similar, but with these changes:_x0007__x0007_    mu <- data.frame(x1=c(20,40,10,30), x2=c(10,30,20,40))_x0007_    Sigma <- matrix(c(1, 0, 0, 1), nrow=2)""",1,0,0
"Thanks smgross for explaining it quite succinctly. I will not pretend that I fully grasped your second point, but I will need to think over it even more._x0007__x0007_When you said it is bad, could you please explain in what way it may be bad. Perhaps a small example would greatly help. Thanks once again.",1,0,1
"Thanks - I was able to take 6.9 quiz._x0007_However, progress mentions a quiz for 6.R but the arrow never takes me there -_x0007_is this the same issue?",1,1,1
Why SE(a) is roughly the same as SD(a)?_x0007__x0007_pls help out. Thanks!,1,1,1
"I would be so grateful if Professor Trevor and Professor Bob would please make clear in all use of logs whether they refer to log or ln, so that we do not have to guess what is their intent.  _x0007__x0007_I have had quite a few graduate courses in math, engineering, and finance and have yet to run across the use of log to indicate either log or ln. _x0007__x0007_Professor Trevor and Professor Bob, please eliminate this possible source of error!_x0007__x0007_Thank you very much.  Great course.",1,1,1
"Dear <redacted>,_x0007_thanks for your comment. _x0007_By 5 Repeats i mean then following: i do 10-fold cross-validation with seed(1) then i change to seed(2) and do another 10-fold cross-validation with and so on. So in total i have 5*10 \different\"" out-folds._x0007__x0007_By \""KS\"" you mean Kolmogorov-Smirnov?""",1,1,1
"You have \p\"" coefficients, one for each predictor. In the book there're examples for 1 predictor (figure 4.4, p.140) or more (figure 4.6, p.146)._x0007__x0007_HTH_x0007__x0007_EDITED: In order to correct my answer: more precisely, you have \""p\"" coefficients, one for each predictor, BUT FOR EACH LINEAR DISCRIMINANT._x0007__x0007_So if you have only two clases you need only one linear discriminant. But if you have 3 clases you need in general 3 linear discriminants: 1 to separate class \""A\"" from \""B&C\"", other to separate class \""B\"" from \""A&C\"" and another to separate class \""C\"" from \""A&B\"". If you're lucky sometimes you can separate 3 clases with 2 linear discriminants...""",1,0,0
"\If I perform a linear regression and get confidence interval from 0.4 to 0.5, then there is a 95% probability that the true parameter is between 0.4 and 0.5\"". _x0007__x0007_This statement seems to be false, which according to the letter of the theory makes sense. But, as a future management scholar, I find that this interpretation is the one brought forward in all management articles. The C.I. containts the true value of beta with probability (1- alpha). I understand that ideally we'd have 100 samples or even draw 100 training data sets from the same sample and check the confidence intervals but I don't see anyone really doing that in practice. _x0007__x0007_So I wonder, what can I meaningfully write in a paper about the confidence interval of a specific beta if I cannot say it is (1-alpha)% likely to contain the true parameter?_x0007__x0007_Thanks_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"Hello,_x0007_I still have to answer the question of the final quiz in chapter 3. Somehow it looks as I am not getting the right answers. So I would like to discuss my answers with you._x0007__x0007_Question1:_x0007_In the balance vs. income * student model plotted on slide 44, the estimate of beta3 is negative. => I would consider that as TRUE_x0007__x0007_Question2:_x0007_One advantage of using linear models is that the true regression function is often linear. => FALSE_x0007__x0007_Question3:_x0007_If the F statistic is significant, all of the predictors have statistically significant effects. => FALSE_x0007__x0007_Question4:_x0007_In a linear regression with several variables, a variable has a positive regression coefficient if and only if its correlation with the response is positive. => TRUE_x0007__x0007_But obviously some of these choices are wrong. Who can give me a hint concerning my misinterpretations. _x0007__x0007_Thanks already in advance!",1,1,1
"Yes, basically the CV criterion will penalise overfitted models that could (erroneously) be found by the best subset selection and help avoid them._x0007__x0007_However, what I would like clarification of is the phrase \a better place\"". We certainly can't mean better in the sense that it performs better on any (a priori) agreed criterion. This only leaves the possibility that it performs better in terms of MSE on the test data. However, surely this goes without saying, as any model chosen even at random COULD perform better in such a scenario and is no reason to choose a methodology when statistical theory suggests there is something better._x0007__x0007_Of course, I am coming from a naive perspective and having little experience, don't doubt that Prof Hastie is completely accurate in the assertion that forward stepwise selection often does as well as best subset selection.""",1,0,1
"Question 4 is false. Due to variable interactions and multicollinearity, the multivariate relationship between x1 and y (given x2,x3) may be different from the bivariate relationship.",1,0,1
"$LaTeX$ is the standard in academia, and it powers Ipython notebook, R markdown, and also this forum. Lyx is a good way to get started with it. http://www.lyx.org/",1,0,1
"I hope I can get some theoretical as well as practical feedback on this. I am trying to use cross-validation to select the best value for a linear regression parameter. The particular linear regression I'm doing is Local L.R. , and the parameter I am trying to \optimize\"" is called the bandwidth parameter (or smoothing parameter). What I do is to run the experiment many times and each run I randomly change the fold assignment of the available measurement (using 5 folds). Number of measurements varies in my experiment between 25-50 measurement of the unknown function I'm trying to recover using local L.R. So, here's what I found so far: if I run this experiment M times, (100-1000), I find that most run give an optimal parameter value that are in a very small interval (they agree on the optimal value), however, a significant portion (about 5% I think) agree on a very different value for that parameter. So one can immediately say that you can't just randomly assign folds to the measurements ONE TIME and go since it appears that there is a dependence on the fold assignment that needs to be \""averaged out\""._x0007__x0007_Now, I don't know if this situation is somewhat peculiar to local linear regression or not, but I suspect it's not._x0007__x0007_I'm very interested in hearing commentary on this, especially theory._x0007_Regards,""",1,1,1
"Absolutely.. That's how it should be done. However, notice that since you've used a portion of the data (the CV set) to select the model or the model parameters, that portion of the data is \less efficient\"", i.e. you have used some of it's essence... as a comparison, that entire data set is now a little \""contaminated\"" and is less powerful for Training than an entirely new set -which you will most likely never have- that has not been inspected by the learning algorithm.""",1,0,1
"$_x0008_eta_1$ is an unknown constant (non-random), a 95% confidence interval means that with a large number of repeated samples, 95% of the calculated confidence intervals would include the true value of $_x0008_eta_1$.",1,0,0
Below each video there's a bar with arrows for navigating back and forth between the video and problems for that section.,1,0,1
"grid <- 10^seq(10,-2,length=100)_x0007_    _x0007_    lasso.mod <- glmnet(x, y, alpha=1, lambda=grid)",1,0,1
You can find my answer to the same question here: https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-quiz-review/threads/52ff89ae85bcee<phoneRedac>01a,1,0,1
Efron's R-square seems to be a good metric. It's easy to calculate._x0007__x0007_http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm,1,0,1
"Thanks, <redacted> and <redacted>.  I find problems in each section.               Have a nice day!",1,0,1
"install.packages(\ISLR\"")_x0007_    library(ISLR)""",1,0,1
"Will solutions to the problems be posted at any point?  In particular, I have not been able to solve 5.R.R4, and would like to see how to do it (beyond just seeing the numerical answer).  Thanks!",1,1,1
"I found the Lasso picture (slide 6.37 or Figure 6.7 on page <zipRedac><zipRedac><zipRedac>) illuminating.  Having seen that, the question that occurred was: what if the geometry was changed to be even more extreme than the diamond?  We have the constraints:  _x0007_Ridge Regression: $$Sigma_{j=<zipRedac>}^{p} _x0007__x0008_eta_j^<zipRedac> le s$$_x0007_Lasso: $$Sigma_{j=<zipRedac>}^{p} _x0007_|_x0008_eta_j| le s$$_x0007_What if we were to do something like: $$Sigma_{j=<zipRedac>}^{p} _x0007_log(|_x0008_eta_j|+<zipRedac>) le s$$_x0007_For those who find visualization helpful (like me :) here is an example contour from [Wolfram Alpha][<zipRedac>]:  _x0007_![Example contour][<zipRedac>]_x0007__x0007_My first observation is that the curve is no longer convex so I believe (perhaps I should't have stopped following Stephen Boyd's [CVX<zipRedac>0<zipRedac>][3]) we lose the nice convex optimization property of the Lasso and Ridge Regression.  Thus this idea is of at best intellectual interest only.  A geometrical argument indicates this constraint would even more strongly force coefficients to zero, but I don't know how to quantify this._x0007__x0007_More interesting is thinking about this in the context of exercise 6.5 on page <zipRedac>6<zipRedac>.  If Ridge Regression tends to give similar values to the coefficients of correlated variables (sensible since minimizing sum of squares) while the Lasso gives a variety of possibilities (sensible since multiple solutions to minimizing the sum) it seems to me that the constraint above would tend to give two possible solutions (each with one coefficient 0) which seems like an interesting computational property._x0007__x0007_Still more interesting is thinking about this in terms of \A Simple Special Case for Ridge Regression and the Lasso\"" and \""Bayesian Interpretation for Ridge Regression and the Lasso\"" from pp. <zipRedac><zipRedac>4-<zipRedac><zipRedac>7 of ISLR.  But I don't see how to do similar analyses for this case._x0007__x0007_Any thoughts?_x0007__x0007__x0007_  [<zipRedac>]: http://www.wolframalpha.com/input/?i=ln%<zipRedac>8abs%<zipRedac>8x%<zipRedac>9%<zipRedac>b<zipRedac>%<zipRedac>9%<zipRedac>0%<zipRedac>b%<zipRedac>0ln%<zipRedac>8abs%<zipRedac>8y%<zipRedac>9%<zipRedac>b<zipRedac>%<zipRedac>9%<zipRedac>0=%<zipRedac>0<zipRedac>_x0007_  [<zipRedac>]: http://www3.wolframalpha.com/Calculate/MSP/MSP<zipRedac>964<zipRedac>ei<zipRedac>8<zipRedac>8<zipRedac><zipRedac>4bggh<phoneRedac><zipRedac>ga657befch3i7b?MSPStoreType=image/gif&s=<zipRedac>7&w=<zipRedac>00.&h=<zipRedac>04.&cdf=RangeControl_x0007_  [3]: https://class.stanford.edu/courses/Engineering/CVX<z""",1,1,1
"Repeating k-fold cross-validation can be used to effectively decrease the variance (i.e. the precision of the estimates) while still maintaining a smal bias (i.e. the difference between the estimated and true values)._x0007__x0007_See: Molinaro A (2005) \Prediction Error Estimation: A Comparision of Resampling Methods\"" *Bioinformatics*, 21(15), 3<phoneRedac>_x0007__x0007_Kim JH (2009) \""Estimating Classification Error Rate: Repeated Cross-Validation, Repeated Hold-Out and Bootstrap\"" *Computational Statistics & Data Analysis*, 53(11), 3<phoneRedac>_x0007__x0007_See also: Kuhn, Johnson (2013) \""*Applied Predictive Modelling*\"". Springer, Chapter 4.4""",1,0,1
"Importance of coefficients can be determined from multiple perspectives.  P-values only address \statistical significance.\""  That is, they provide evidence for whether or not a variable has an effect on the response._x0007__x0007_A user might also be interested in \""practical significance,\"" which tries to address whether we actually care about said effect (how large is it compared to the noise level for example)._x0007__x0007_Anyways, p-values do now exist for the Lasso, based on something called a covariance test.  It is all new though and above the level of this course.""",1,0,1
Hi! I'm really in love with this course :) ! I never laughed so much before while learning statistics._x0007_I appreciate that you introduce the concepts first and put a bigger stress on understanding instead of inondating the audience with all the mathy notations right away. Also I find interesting all the historical and inter-subject references : they really put thing into perspective for me. The massive use of plots and illustrations is really helpful too._x0007_You really make studing statistics sexy :)  !_x0007_Thank you for such a wonderful and exciting learning experience!,1,0,1
"Due to the random nature of the technique being used, you might not get exactly the same, precise answer that the instructor did. \Within 10%\"" means that you can deviate from the quiz's official answer up to 10% of that answer *higher or lower* and still get credit for the answer._x0007__x0007_It's sort of like if I bet you that you couldn't guess the year that Napoleon died, \""within five years\"". He really died in 1821, but you could guess as early as 1816 or as late as 1826 and still win the bet. :-)_x0007__x0007_And you're right... using linear regression, you should get the same answer and the 10% range isn't really needed. Although you still might vary a bit if you rounded to a lower level of precision.""",1,0,1
"Right. Probably a better way to put the statement is:_x0007__x0007_In a linear regression with several variables, a variable has a positive regression coefficient if and only if its correlation with the response, ***if you created an entirely separate model with just that one variable***, is positive._x0007__x0007_As we've seen in the book/lectures, it's possible for a variable to correlate one way when you just look at it by itself, but act another way when you look at it in a model along with other (correlated) variables.",1,0,0
"FWIW, a pad of paper and a pen works great._x0007__x0007_I've upgraded my setup to include several different colors of pen, but you don't have to get that fancy if you don't want to. :-)",1,0,1
"And the quiz question relates to the next subsection, not the \dimension reduction method\"".""",1,0,1
Just curious. How do you decide how many times you repeat the experiment to construct the CI just to be 95% sure that your constructed CI contains the true population parameter?,1,1,1
Isn't this where the bootstrap bias comes in; as a measure of the discrepancy between the sample estimate and that implied by the sampling distribution based on the bootstrap?,1,1,1
"Hi,_x0007_I am having confusion regarding the probability if a particular sample does not occur in the bootstrap sample. I thought that every sample has an equal probability of getting selected , that is for n points it would be 1/n. So not getting selected would be the complement of that. But its giving me a wrong answer. Am I missing some detail. _x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
How do I access the Course Notes?  I'm trying to catch up after missing a few weeks and one of the questions references a graph on page 30 of those notes but I'm unable to find them anywhere.,1,1,1
I have no complaints with the error range on the answer - I am in the group that found an error of the order 0.02.  _x0007__x0007_I have subsequently tried many approaches and nothing I seem to do gets me even close to the answer.  Obviously I am doing something wrong and will have to revisit as time permits._x0007__x0007_Thanks for taking the trouble to get back to me_x0007__x0007_Chris,1,1,0
"I find it interesting that linear regression can be considered a flexible model. In one sense linear regression is not considered a flexible model and as such, is more interpretable. This is traditional thought based on the availability of a large number of observations in the data set with a relatively small number of predictors. In the sense of chapter six, the linear regression model becomes flexible due to the number of observations in the data set being not much larger than the number of predictors. So now the linear regression model suffers from coefficient estimates with large variance and is at risk of over-fitting. In the case of p > n, we have multiple solutions fitting the least squares and must consider how to choose between them. For this high-dimensional case, we also fall short on assessing model fit with traditional measures such as RSS, p-values, and R^2. In spite of all this, the linear model continues to be a simple, interpretable model and worth this study. In the end, we know not the true form of the relationship or even if there is a relationship. However, we can do our best to choose the appropriate method to answer the question. I wish I had these tools in my statistical belt 10 years ago :)",1,0,0
"Thanks!  I should have thought to look in ESL (I had it open looking at something else ~10 pages later)._x0007__x0007_Looking into things further, the elastic-net discussion is interesting.  I don't see mention of it in ISLR, so I'm guessing we won't cover it (but I thought it was mentioned briefly in the lecture?)._x0007__x0007_Page 92 has a discussion of the non-convexity of SCAD and $$|_x0008_eta_j|^{1-_x0007_u}$$ then mentions the adaptive lasso as a computationally tractable alternative._x0007__x0007_There is a brief mention of concave penalties towards the end (slide 38/53) of http://www.stanford.edu/~hastie/TALKS/hastie_baylearn.pdf",1,0,1
"Hi_x0007_I prefer to ask and to seem dumb to be quiet and to be it. So from this perspective, please be all lenient as you patient let you :-P_x0007__x0007_Thank you for your explanation, Alhf. I understand you look for softness in the plot of Errors derived from a lm._x0007_resid <- residuals(lm.fit)_x0007_plot(resid, type=\l\"")_x0007__x0007_And I \""almost\"" understand too correlation is present when you plot errors from X1 Vs X2. Y say almost because it is clear in the first case but not too much in the second plot, 10 measures before._x0007__x0007_But... plot suggested in the quizz is over Xy directly not over errors resulting from a fitted lm_x0007__x0007_Well, i think you can figure there is correlations because both plots are \""continues\"" (smooth and can be derived in all its points) ¿right? ¿It is enough condition really?_x0007__x0007_i have done a little test. Maybe (Indeed pretty sure) i'm wrong but... _x0007_when you run plot(Xy$X1 ~ Xy$X2, type='l' )_x0007_You get something similar a rich man signature, I can't paste that plot but it easy to reproduce. It is enough regular to consider there are correlation between those vars X1 and X2?, Because I was looking forward to find a single line._x0007_It is similar to you second graph but it is referred to samples not to errors._x0007__x0007_And finally, if you enter in R plot(acf(Xy)) you get a lot of plots. Sincerely I do not understand it, but they are similar to your last plot_x0007__x0007_Thank you, Again""",0,1,1
"You're on the right track:  selecting one point has probability 1/n, and that point's non-selection is probability (1-1/n).  But you're selecting n points, not just one.  Look up the multiplication rule in a book covering basic probabilities if this doesn't get you to the answer just yet.",1,0,0
Could you make this code available?,1,1,1
"You're correct- penalties like_x0007_one you suggested are not convex and hence much harder to deal with computationally._x0007__x0007_However they can produce sparser models than the lasso, which can be useful._x0007_see http://www.stanford.edu/~hastie/Papers/Sparsenet/jasa_MFH_final.pdf_x0007_and the SparseNet package on CRAN",1,0,1
"library(ggplot2)_x0007_qplot(Sepal.Length,Sepal.Width,data=iris,facets=Species~.)_x0007__x0007_library(lattice)_x0007_parallelplot(~iris[1:4]|Species,data=iris)_x0007__x0007_library(MASS)_x0007_parcoord(iris[1:4],col=iris$Species)",1,0,1
you the man,1,0,1
"For those who may wonder, Prof. Tibshirani said the Lasso is a convex optimization problem (I think ridge is as well, but I do not claim to be right)_x0007__x0007_If you are curious, look up their Stamford Colleague Andrew Ng ML class (on itunes and on other place not to be mentioned) - he has a great explanation of at least one way to solve it iteratively - be warned Ridge regression is referred to as regularization on that class (the term was also used here briefly)",1,0,1
"I learned 2 ways - I took Roger Peng's class on coursera - and I also refused to say things were not possible, some of course are not, but some effort pays off_x0007__x0007_I can say it was worth the effort, it is great",1,0,1
Yes but you could consider all independent features a subset of the entire possible feature space.,1,0,1
I agree!,1,0,1
+1 for UT Austin's LAFF,1,0,1
*sigh*,0,0,1
"I want to estimate the 95% Confidence Interval of test set error (RMSE, F1-score, etc.). Is the following approach valid?_x0007__x0007_(1) Get model's predictions for the test data set._x0007__x0007_(2) Draw a large number (e.g. 10,000) of **bootstrap** samples from the test data set._x0007__x0007_(3) For each of the 10,000 bootstrap samples, compute the RMSE *[from step (1) we know the predicted values]*_x0007__x0007_(4) Sort the 10,000 RMSE values, and pick the middle 95% range as the 95% CI of test set error (RMSE).",1,1,1
Is it possible to remove the duplicate/transition slides from the lecture slides posted for download?  It's pretty unhelpful to have multiple copies (all but the last one incomplete) of many of the slides -- painful for reviewing.,0,1,1
"<redacted>:  (and anyone else who's interested)_x0007__x0007_Yes — of course._x0007__x0007_I should have posted code earlier, but the \code\"" was just a_x0007_bunch of not-that-well-organized stuff from an interactive_x0007_session._x0007__x0007_Anyway, I've tidied it up a bit and included it below._x0007_(",1,0,1
"In repeated CV, the SD of your parameter will most likely be large since your sample is so small -- and even smaller on training. _x0007__x0007_I agree it seems better to average over many sets of K-fold CV.",1,0,1
"In \Ridge regression: scaling of predictors\"", the formula for standardizing the predictors does not **center** the predictors (*by subtracting mean*). Is there a reason for not centering? _x0007__x0007_Here is a made-up example to illustrate the benefits of centering:_x0007__x0007_> a = <zipRedac>:<zipRedac><zipRedac>_x0007__x0007_> b = <zipRedac><zipRedac><zipRedac>:<zipRedac><zipRedac><zipRedac>_x0007__x0007_**without centering**_x0007_> a/sd(a)_x0007__x0007_ [<zipRedac>] <zipRedac>.<phoneRedac> <zipRedac>.<phoneRedac> <zipRedac>.<phoneRedac> <zipRedac>.<phoneRedac> <zipRedac>.<phoneRedac> <zipRedac>.<phoneRedac> 2.<phoneRedac>_x0007_ [8] 2.<phoneRedac> 2.<phoneRedac> <zipRedac>.<phoneRedac>_x0007_> b/sd(b)_x0007__x0007_ [<zipRedac>] <zipRedac><zipRedac>.<zipRedac>592<zipRedac> <zipRedac><zipRedac>.<zipRedac>8949 <zipRedac>4.<zipRedac><zipRedac>978 <zipRedac>4.<zipRedac>5<zipRedac><zipRedac>7 <zipRedac>4.<zipRedac>8<zipRedac><zipRedac><zipRedac> <zipRedac>5.<zipRedac><zipRedac><zipRedac><zipRedac>5 <zipRedac>5.<zipRedac>4<zipRedac>94 <zipRedac>5.<zipRedac>7<zipRedac>2<zipRedac>_x0007_ [9] <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac>52 <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac>8<zipRedac>_x0007__x0007__x0007_**with centering**_x0007__x0007_> (a - mean(a)) / sd(a)_x0007__x0007_ [<zipRedac>] -<zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac>5<zipRedac><zipRedac><zipRedac>2<zipRedac> -<zipRedac>.<phoneRedac> -<zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac><zipRedac>5<zipRedac>44<zipRedac>  <zipRedac>.<zipRedac><zipRedac>5<zipRedac>44<zipRedac>_x0007_ [7]  <zipRedac>.<phoneRedac>  <zipRedac>.<phoneRedac>  <zipRedac>.<zipRedac>5<zipRedac><zipRedac><zipRedac>2<zipRedac>  <zipRedac>.<phoneRedac>_x0007_> (b-mean(b)) / sd(b)_x0007__x0007_ [<zipRedac>] -<zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac>5<zipRedac><zipRedac><zipRedac>2<zipRedac> -<zipRedac>.<phoneRedac> -<zipRedac>.<phoneRedac> -<zipRedac>.<zipRedac><zipRedac>5<zipRedac>44<zipRedac>  <zipRedac>.<zipRedac><zipRedac>5<zipRedac>44<zipRedac>_x0007_ [7]  <zipRedac>.<phoneRedac>  <zipRedac>.<phoneRedac>  <zipRedac>.<zipRedac>5<zipRedac><zipRedac><zipRedac>2<zipRedac>  <zipRedac>.<phoneRedac>""",1,1,1
"In Forward Stepwise Selection, in step 2.2 why do we use the **training** set error (RSS or R2) to choose the best among the p-k models? Isn't **cross-validation** error a better way of choosing the best among the p-k models?_x0007__x0007_In step 3, we do use cross-validation error to pick the best model, so I am confused why not also use cross-validation error in step 2.2?",1,1,1
"My understanding is that the bootstrap procedure is a for estimating the statistical accuracy of a parameter. The Sci Am article on bootstrap by Prof. Efron is a nice read. Search for \bootstrap scientific american\"".""",1,0,1
"Hello:_x0007__x0007_May I have your advice/comments about these 2 packages: bestglm, glmulti. I found out they can be used to apply all subset regression for logits, just what I need right now._x0007__x0007_Thank you._x0007__x0007_PD: a great great course, by the way :)",1,0,1
Yeah. Try this: http://statistical-research.com/wp-content/uploads/2012/09/database-connect.pdf_x0007_I worked for me.,1,0,1
"Hi,_x0007__x0007_Are glm.probs, glm.fit, and glm.pred in the first video posted in 4.R built-in commands or just a choice to name variables by the presenter?_x0007__x0007_When I did ?glm.fit I got help text but it was also assigned the result of calling glm._x0007__x0007_I didn't get any help messages on glm.probs or glm.pred._x0007__x0007_Thanks!",1,1,1
"For simplicity, consider a single predictor variable._x0007_(But the idea is general)_x0007__x0007_----_x0007_Original model_x0007__x0007_$_x0007_y = _x0008_eta_0 + _x0008_eta_1 x + epsilon_x0007_$_x0007__x0007_----_x0007_Scaling, but no centring_x0007__x0007__x0007_$_x0007_x^{prime} = frac{x}{s_x}_x0007_$_x0007__x0007_In terms of the scaled variable',_x0007_$x^{prime}$, the original model is:_x0007__x0007_$_x0007_y = _x0008_eta_0^prime + _x0008_eta_1^prime x^{prime} + epsilon_x0007_$_x0007__x0007_where the slope and intercept are_x0007_(in terms of what they were in the original model):_x0007__x0007_$_x0007__x0008_eta_1^prime = _x0008_eta_1 s_x_x0007_$_x0007__x0007_$_x0007__x0008_eta_0^prime = _x0008_eta_0_x0007_$_x0007__x0007__x0007_----_x0007_Centring and scaling_x0007__x0007_$_x0007_x^{primeprime} = frac{x - _x0008_ar x}{s_x}_x0007_$_x0007__x0007_In terms of the centred and scaled variable',_x0007_$x^{primeprime}$, the original model is:_x0007__x0007_$_x0007_y = _x0008_eta_0^{primeprime} + _x0008_eta_1^{primeprime} x^{primeprime} + epsilon_x0007_$_x0007__x0007_where the slope and intercept are_x0007_(in terms of what they were in the original model):_x0007__x0007_$_x0007__x0008_eta_1^{primeprime} = _x0008_eta_1 s_x_x0007_$_x0007__x0007_$_x0007__x0008_eta_0^{primeprime} = _x0008_eta_0 + _x0008_eta_1 _x0008_ar x_x0007_$_x0007__x0007_----_x0007__x0007_Note that, with scaling, the slope is the_x0007_same, regardless of whether there is centring:_x0007__x0007_$_x0007__x0008_eta_1^prime = _x0008_eta_1^{primeprime} = _x0008_eta_1 s_x_x0007_$_x0007__x0007_This is all that matters to the penalty function._x0007__x0007_The centring does make a difference to the intercept, but_x0007_that doesn't affect the penalty function.  In the end, _x0007_the intercept will be different but the predictions will_x0007_be the same.",1,0,0
They are just variable names chosen by the presenter.,1,0,1
"This is a fit case for calling the `anova()` function. When you have nested models, `anova(smaller model, bigger model)` will carry out a F-test. You can use the resultant p-value to come to a conclusion whether the bigger model is also better. The Null Hypothesis in this case is that both the models are equally good.",1,0,1
"MLE means Maximum Likelihood Estimate. This is a method for estimating the value of a parameter that has the highest probability of being correct. The general idea is that you look at the product of the distribution function (pdf) at each data point, and find the value of the parameter that maximizes this expression. This expression is called the likelihood function. Most often, it is much easier to compute the derivative of the log of the likelihood function. Since log is monotonic, the parameter estimate is the same._x0007__x0007_Perhaps more information than you wanted, but you asked!",1,0,0
"At that point in the lab, any observation with an NA has been removed from Hitters. Hitters begins with 322 observations, but only has 263 observations after running na.omit() - see page 244 of the textbook._x0007__x0007_All of the labs in the textbook for chapter 6 assume you are using Hitters with 263 observations. If you re-run the code after reducing Hitters to 263, you will see the result I described.",1,0,1
"It is the names that presenter defined himself, also it can change to another name.",1,0,1
Hey.._x0007__x0007_to my calculation it would be (n^2-1)/n^n but this is not allowed as a result._x0007__x0007_With n=3 there are 8/27 probability of a point not being in the sample. _x0007__x0007_Any help?,1,1,1
Search the forum for 'tsboot' - this does block bootstrap for time series data.,1,0,1
"Correction: delete \continuous\"" above.""",1,0,1
"So you're saying that the effects of \scaling\"" and \""scaling-and-centering\"" are about the same for Ridge and Lasso.""",1,0,1
use cross-validation to find the best lambda._x0007__x0007_    ?cv.glmnet(),1,0,1
"That really helps!Thank you, dimitrioschr and Neronus.",1,0,1
"It is confusing, I know._x0007__x0007_In brief, glm.fit is a method called by the glm() function to actually perform the fitting. However, R allows you to create objects (variables) that have the same names as methods. (Perhaps this is not true is all cases, but it's generally true.)_x0007__x0007_Thus, glm.fit = glm(...) is running the glm() function and storing the results in an object called glm.fit. When you call the help function for glm.fit, it brings up the help page for the glm() function, but that has nothing to do with the fact that you have created a glm.fit object._x0007__x0007_Here's a simpler example to clarify your understanding. There's a function called sum(). R doesn't stop you from typing \sum = 7\"" to create an object named \""sum\"". However, you can still use the sum() function and you can use the sum object. R figures out which one you are referring to by the context.""",1,0,0
"If there are some missing data for the predictors, what should we do?_x0007_For example, we can only observe the income, gender for an individual, but the age of him or her is not known. How could we do if we want to use income, gender and age (the predictors) to fit some response (like credit default)?",1,1,1
"Thanks William.  That does help for ridge, but then by that same logic, model size does apply to lasso, and you should still not use Cp, AIC or BIC in lasso. E.g., a plot of standardized coefficients vs lambda will show you that at various lambda you might have any number of p non-zero coefficients (therefore a series of models of various p).  Either way, I do get the value of doing cross-validation vs. just relying on Cp, AIC or BIC, but I'm just looking to further deepen my understanding of the concept as to what was meant by the parameters having a different definition resulting in your inability to interpret how many p you are left with in your model.",1,1,1
"Forward selection is a \Greedy\"" solution inspired by best subset. The number of models it computes is 1+(1+p)*p/2._x0007__x0007_If I understand correctly, you'd like insert CV during selection of each Mk, k from 1 to p. Then you would have to compute around 1+10*(1+p)*p/2 models (10-fold CV)._x0007__x0007_Greedy solution can not promise a global \""best\"" model, even if you use CV in between. Therefore I would still prefer a deviance test instead in each \""add one\"" feature step.""",1,0,1
"*The New York Review of Books* has an article in the current issue by Freeman Dyson that may be of interest to this community._x0007__x0007_I'll quote a section in which Dyson explains why Darwin in essence failed to discover Mendelian genetics._x0007__x0007_    Unlike Mendel, he [Darwin] had no understanding of statistical fluctuations.  He used a total of only 125 third-generation plants and obtained a value of 2.4 for the crucial ratio. This value is within the expected statistical uncertainty, either for a true value of 2 or for a true value of 3, with such a small sample of plants. Darwin did not understand that he would need a much larger sample to obtain a meaningful result._x0007__x0007_The full article is available without charge at_x0007__x0007_http://www.nybooks.com/articles/archives/2014/mar/06/darwin-einstein-case-for-blunders",1,0,1
"Could you use percent change from one time series value to another? The prices may be somewhat related, but wouldn't the percent change from time interval to time interval may be independent?",1,1,1
"Hi,_x0007_I'm confused about this concept, yes... it's is a basic question._x0007_Are degrees of freedom related to number of parameters or predictors, any clarification._x0007_Thank you very much in advance Great community!",1,1,1
Precision and F1-score if you have small actual event rate,1,0,1
Can we use lasso for logistic regression in the same way like for linear regression?,1,1,1
"glmnet package does this (scaling) automatically for ridge, lasso, and elastic net.",1,0,1
"While reading about Ridge Regression, I got to wondering why it's called Ridge Regression. I was not satisfied by my web search. So, I would appreciate if someone would like to share. _x0007__x0007_What I did learn is the method is also known as Tikhonov regularization. It is used to address the problem of multicollinearity, such as may be the case when time and world population may be two predictors in a model of global warming. Where time and population increase at a similar rate. Multicollinearity is quantified by the variance inflation factor or VIF, which we can find in the output of statistical software programs. When this is the case and we wish to keep both predictors in the model for valid reasons, we can use Ridge Regression to address the issues caused by multicollinearity. Such issues include changing coefficient estimates and p-values when a collinear predictor is added to the model. This makes interpretation very tricky. These issues are caused by the large variance of our coefficient estimates due to the determinant of our XtransposeX matrix being close to zero. Ridge Regression adds on a factor which penalizes large parameter values. This is the same as adding a factor to our XtransposeX matrix and results in a determinant far from zero. This in turn reduces the variance of our parameter estimates at the cost of now they are no longer unbiased estimates of the true value. Perhaps the determinant is the ridge?_x0007__x0007_Ref: http://tamino.wordpress.com/2011/02/12/ridge-regression/",1,1,1
I have the following set of data:_x0007_Intensity	Distance_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac><zipRedac>	<zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac><zipRedac>	<zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<phoneRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_<zipRedac>	<zipRedac><zipRedac><zipRedac>.<z,1,0,1
"why not first enter the data with R code to read it in as a matrix, at least, if not a data frame?_x0007__x0007_Without further info, the question doesn't seem to make sense. _x0007__x0007_K",0,1,1
"From reading the lab section of chapter 6, I think we can use cross-validation method to first choose the best model out of all the possible models we have when adding in the next predictor, in the case of forward stepwise regression, for example. So, if p=4 and we let k=0 to begin step 2 on page 207, we have 4 models to choose from. We use CV to choose the best model. Then, when k=1, we have 3 models to choose from and use CV to decide. When we complete this step, we have Msub4 models. From here we use CV again to select the best model. So, I think this is doing feature selection within CV step. This process is also used for the other two methods as far as I can tell.",1,0,0
"Yes. From the lecture on the Lasso shrinkage method we end up with a sparse model. A sparse model is one that contains a subset of the predictors. In this sense, Lasso performs feature selection while Ridge Regression uses all predictors and, I'm guessing, may be considered a dense model.",1,0,0
Thank you for the article.,1,0,1
"Thank you very much. If it were not for your help, I would still be stuck in chapter five. I tried using the example on page 195, which did not work. I still don't know why. It looks like it may be the \subset=index\"" portion of the code I was not using correctly.""",1,0,0
"\Finally, use the block bootstrap to estimate s.e.(?^1).\"": I would say it asks for the bootstrap estimate of the standard error.""",1,0,1
"I believe this is used in practice in econometrics under the name of \first difference\"".""",1,0,1
"Sorry. I guess it’s an effect of the preview. On clicking on edit a clean table appears. _x0007_I dropped the data in a Dropbox file in the following link:_x0007_https://dl.dropboxusercontent.com/u/<phoneRedac>/Int_vs_Distance.txt_x0007_and ploted figures with:_x0007_> mydata <- read.table(\Int_vs_Distance.txt\"", header=TRUE, sep=\""\"", strip.white=TRUE)_x0007_Int    <- as.factor(mydata$Int)_x0007_meand  <- tapply(mydata$Distance, list(Int=mydata$Int), mean)_x0007_figure<zipRedac> <- plot(mydata$Distance,mydata$Int)_x0007_figure2 <- plot(meand,levels(Int))_x0007_> Blockquote_x0007__x0007_My problem is to build a model between Intensity and Distance. As you can see in figure <zipRedac> Distance data are disperse and there are many outliers. I thought on building a model between Intensity and Mean Distance (with data as in figure 2) or Median Distance, instead of using Intensity and Distance directly. Would that be appropiate? _x0007_Thanks.!_x0007_![enter image description here][<zipRedac>]_x0007_![enter image description here][2]_x0007__x0007__x0007_  [<zipRedac>]: https://edx-west-uploads.s3.amazonaws.com/<zipRedac><phoneRedac><phoneRedac>84.jpeg_x0007_  [2]: https://edx-west-uploads.s3.amazonaws.com/<zipRedac><phoneRedac><zipRedac>073<zipRedac>232.jpeg""",1,1,1
"Thanks for the responses. My respectful qualms: _x0007__x0007_(1) doesn't the t-test effectively test whether the regression coefficient is, statistically significantly, nonzero? That is, *not* completely useless? Isn't this different from assessing whether any improvement it does make is a significant one (i.e. \big enough\"", not just nonzero)? I'm thinking of comparing whether one model fit statistic (like $R^2$) is significantly different from the other (which includes an additional predictor)?_x0007__x0007_(2) the ANOVA also seemed to be somewhat lax. For example, I tried comparing two single predictor models (different predictors) and since in this case $R^2= r^2$, the Fisher r-to-z transformation was used and tested whether the correlation coefficients were significantly different. They just missed significance (0.052). As it turns out, one of the single predictors actually could be teased out into two, one of which was the predictor in the other model, plus an additional predictor. Since these are nested models, I used ANOVA which was highly significant ($p < 0.001$). I wasn't sure why there was such a big discrepancy. _x0007__x0007_A third thing I tried was a ratio of $AIC$ of the models. This gives the relative likelihood of each model compared to the best model, which gave something like $99\\%$ probability that the additional-predictor-model was best, but this is just a relative comparison of course._x0007__x0007_In an ideal world, I'd think it'd be nice to test not just training set differences, but whether CV error is significantly different from one model compared to another. In my case, the models only differ by one variable, this would amount to testing whether that variable improved CV error significantly. Is there such a test?""",1,1,1
"\Where possible, avoid using names of existing functions and variables. This will cause confusion for the readers of your code.\"" http://adv-r.had.co.nz/Style.html""",1,0,1
To confirm: 5.R.R3 and 5.R.R4 ask to use bootstrap (standard and block respectively) to estimate s.e. for the model fitted in R1 rather than redo the fitting using bootstrap?,1,1,1
"The boundary region is defined in the question. It would be the 10% most extreme points in 1 dimension (5% on each side). However, it is not always 10%. It depends on the dimension. To see this most easily, you can compute it for 2 dimensions. _x0007__x0007_The question is about how this proportion changes as the dimension changes. _x0007__x0007_The boundary itself is $1$ for each dimension $x_j$. Yes, this can be visualized as a line, square, or cube for dimensions 1, 2, and 3. And you can visualize the boundary regions as well as regions in each of those._x0007__x0007_Be sure to check out this thread, too. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-quiz-review/threads/52de20d9033fb<phoneRedac>002b",1,0,0
"Hello,_x0007_I tried to download 5.R.RData for the long quiz for Ch.5. But after I load the data, it does not have anything in it. I was using load(5.R.RData) and then I used checked dimension and names all showing null. Did I do something wrong here? Thanks.",1,1,1
"`R` modeling functions differ as to whether they handle missing (NA) values smoothly, e.g. by throwing them out or providing functionality to impute them. For example, `lm` just deletes observations which contain missing values. But some algorithms, I believe the lasso and random forest, require that there are no missing values upfront, for which you can use `complete.cases` if you just want to throw them out: _x0007__x0007_    mydata <- mydata[complete.cases(mydata), ]",1,0,1
"imo kind of lame: \This is where ridge regression gets its name since the diagonal of ones in the correlation matrix may be thought _x0007_of as a ridge.\"" _x0007__x0007_http://ncss.wpengine.netdna-cdn.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf""",0,0,1
Thanks for the response. I see that it scales with `y`. I took a peek at the 'automagic' in the source code as well- my first time seeing Fortran 90! Cool stuff.,1,0,0
"Dear Professors,_x0007__x0007_In the ppt where you illustrate how the radius changes when dimension increases from 1 to 10, the radius is > 1 when dimension is 10. I am confused here. I probably don't understand correctly. But, is the radius equal to (volume fraction)^(1/dimension)? Because volume fraction is always < 1, the radius can not exceed one. Am I correct?_x0007__x0007_*****_x0007_For example, if we want KNN has 10% volume nearest neighbor, then the radius is 0.1^(1/10) = 0.8 in 10 dimension (suppose we consider sphere of unit radius)._x0007__x0007_*****_x0007_Please correct me if I am wrong.",1,1,1
"Use the following command to see the variables._x0007__x0007_    ls()_x0007_    Xy_x0007__x0007_You will see that Xy is a data frame that contains 3 variables y, X1, and X2.",1,0,1
I love Prof Hastie & Prof Tibshirani's lessons._x0007_They are wonderful at communicating the concepts and the hands-on skills._x0007_The concise and illuminating compilation of the theory and code are deeply appreciated._x0007_The above ensemble makes learning really fun.,1,0,1
"This class is fun and I am learning stuff, but . . . how does one handle bigger data.  Not BIG data necessarily, but say 50M rows and 200 features?  Clearly, you cannot load that into memory.  I believe there is some way to use R for bigger data.  Supposedly for Python too.  Is there any good references/classes people know about?",1,1,1
there is a quiz after each video.,1,0,1
"Technically, the word \normalization\"" is not correct. You should use the word \""scaled\"" or \""standardized\"". This is a subtle, but important point. Scaling or standardizing the data will not change its distribution. The purpose of scaling/standardizing is to strip off the units of measurement._x0007__x0007_    # generate non-normal random numbers_x0007_    _x0007_    x <- rexp(1000)_x0007_    _x0007_    std.x <- (x - mean(x)) / sd(x)_x0007_    _x0007_    par(mfrow=c(2,1))_x0007_    hist(x)_x0007_    hist(std.x)""",1,0,0
"You mean that in the glmnet there is a \\""scaled\"" or \""standardized\""\"" procedure, not normalization like you described above? In which way does it work in terms of code?""",1,1,1
"In terms of code, glmnet uses scaling of $X$s.",1,0,1
"Hello all, _x0007__x0007_I was wondering if anyone had comments on the thesis of this article? [http://www.nature.com/news/scientific-method-statistical-errors-<zipRedac>.<zipRedac>4700][<zipRedac>]_x0007__x0007__x0007_  [<zipRedac>]: http://www.nature.com/news/scientific-method-statistical-errors-<zipRedac>.<zipRedac>4700_x0007__x0007_The basic point is that much published research findings (especially in psychology, a focus of the article) are false, where researchers intent on finding \significant\"" results, publish unstable results. The search for \""statistical significance\"" means that researchers are unconsciously stopping data collection when they find *p < .05, or running all sorts of different model specifications to find this result. _x0007__x0007_Or simply taking a mechanical view of p < .05, without thinking about what it means. _x0007__x0007_Thanks for your thoughts!_x0007_K""",1,1,1
"i really like cv.glmnet combined with glmnet_x0007__x0007_    library(glmnet)_x0007_    ?cv.glmnet_x0007__x0007__x0007_For lasso, set alpha = 1, for ridge set alpha = 0, then 0 < alpha < 1 will be a mix of the two.  It will help you pick your tuning parameter.  In particular if you use _x0007__x0007_    cv.glm <- cv.glmnet(x,y, family='binomial', alpha = .5)_x0007_    plot(cv.glm) # plot deviance against log(lambda)_x0007_    cv.glm$lambda.1se_x0007__x0007__x0007__x0007__x0007_The cv.glm$lambda.1se provides the lambda that is within 1 standard deviation of the lambda that produces the smallest deviance.",1,0,1
"p-values can be very misleading. First, the p-values are calculated in the lm() function with very strict assumptions about the data:_x0007__x0007_1. residuals are uncorrelated_x0007_2. variance of residuals is constant_x0007__x0007_Second, stepwise regression or best subsets regression can search through the X variables space and find the ones with the lowest p-values. This could lead to overfitting (high variance, low bias). That is why I always ask for the cross-validation or test set MSE._x0007__x0007_Third, p-values can change substantially if ONE variable were added or removed from the model.",1,0,0
"Sorry, I just realized that the book also removes the intercept column by subsetting with [,-1]. So there's really no difference between book and lecture.",1,0,1
"I'm using SAS to code out logistic ridge regression and i wanted to test it against glmnet (as validation that I did it correctly).  I have framed the problem as a minimization problem instead of a maximization problem - just because I already had started this sometime ago.  _x0007__x0007_I'm using the following cost function_x0007__x0007_$ -frac{1}{2}sum_{i = 1}^n _x0008_ig[y log( {p(x_i_x0008_eta)} ) + (1 - y)log(1 - p(x_i_x0008_eta))_x0008_ig]  + lambda |_x0008_eta^*|_2$  _x0007__x0007_Where $_x0008_eta^*$ are the estimates except the intercept.  This is the penalized log likelihood reported in the glmnet documentation multiplied by -1.  Results are identical (mostly) when $lambda = 0$, but as I vary $lambda$, some of the estimates are close and some are very different.  I'm using GLMNET with alpha = 0 and I supply lambda.  All data except the response has been standardized prior.  Is this a result of the fact that glmnet use the quadratic approximation to the penalized log likelihood?",1,1,1
Thanks for the nice explanation. _x0007__x0007_So you keep only the best model in each fold? _x0007_How do you decide at the end of K folds which model-parameter (ie. which k variables) to choose?_x0007__x0007_And how do you interpred the standard deviation `SE(CV)` at the end?,1,1,1
"Could you please mention which formula is used for scaling, because sometimes in some sources these terms are synonyms",1,1,1
"Wish the Professors would consider offering a followup, advanced course building the material given in this one. I know we're still in the middle and there's more good stuff to come, but this has been great so far. In an advanced course, I think substantial programming Homeworks would be very beneficial, if not essential.",1,1,1
"Yeees, I love this course. Thanks",1,0,1
I'm using a iPad Mini to watch the lectures. No issues for me.,1,0,1
"For big data like that, you have a couple of options. Some operating entirely in R are listed above. Another alternative would be to use SQL to manipulate the data as much as possible, then feed whatever subset you are using into R. A third involves using SAS, which has clunky, (in my opinion) non-intuitive syntax, but is pretty much engineered around dealing with exactly the type of data you mention.",1,0,1
"Haven't seen this mentioned in the previous posts, so there may be some joy in_x0007_**CRAN's Large memory and out-of-memory data** packages listing, at their [HPC site][1]._x0007__x0007_  [1]: http://cran.r-project.org/web/views/HighPerformanceComputing.html",1,0,1
Thanks.Problem solved.,1,0,0
"Maybe the only reference you'll need. _x0007__x0007_http://warnercnr.colostate.edu/~anderson/thompson1.html_x0007__x0007_However, I enjoyed this paper recently._x0007__x0007_http://www.ejwagenmakers.com/2007/pValueProblems.pdf_x0007__x0007_Two striking things it shows is that a ridiculously lax Bayesian prior still does not end up claiming the null hypothesis is as likely as a p-value (p. 792-3). And second, that the null hypothesis significance testing paradigm does not compare the null hypothesis to the alternative. As a consequence, it rules out the very real possibility that, even if the data are unlikely given the null hypothesis, they are even more unlikely given the alternative hypothesis (p. 793).",1,0,1
"(0) it gives you a model, instead of just a plot. _x0007__x0007_(1) with this model, you can predict new data. how would you do that without a model?_x0007__x0007_(2) you want to quantify what you have seen in exploratory data analysis (EDA) to confirm if what you're seeing differs from chance, and if so, to what extent. _x0007__x0007_(3) although EDA is indispensable, sometimes your eyes can trick you, or not extend to higher dimensions, or not properly factor in the sample size. _x0007__x0007_(4) interactions are possible that may be hard or impossible to identify by eye. _x0007__x0007_(5) you don't have to use a linear model to test whether balances differ by gender. you could, for example, use a t-test. but, correct me if i'm wrong, this test is subsumed by linear regression, so amounts to the same thing.",1,0,0
> Blockquote_x0007__x0007_The most common issue I have seen is calculating the bootstrap distribution of the SE of beta instead of the SE of the bootstrap distribution of beta._x0007__x0007_> Blockquote_x0007__x0007_ Does Boot provide the SE of the distribution of re-sampled beta?  What does bootstrap distribution mean?  Thanks for enlightening.,1,1,1
"If we could *replace \Introduction\""* with *\""Applied\""*, and get more points for R sessions it would be a dream come true!_x0007__x0007_P.S. *Official* solutions to the ISLR book could do the trick, as well... :-)""",1,1,1
"`Hitters` contains 3 variables that are factors:_x0007__x0007_    | League    | Factor - 2 levels \A\"",\""N\"" |_x0007_    | Division  | Factor - 2 levels \""E\"",\""W\"" |_x0007_    | NewLeague | Factor - 2 levels \""A\"",\""N\"" |_x0007_    _x0007_Comparing the variables from the following `model.matrix` calls_x0007__x0007_    model.matrix(Salary~.-1,data=Hitters)_x0007_    model.matrix(Salary~.,data=Hitters)_x0007__x0007_we have the following (dummies indicated with an asterisk)_x0007_    _x0007_    |   | Salary~.-1 | Salary~.    |_x0007_    |---+------------+-------------|_x0007_    |   | -          | (Intercept) |_x0007_    |   | AtBat      | AtBat       |_x0007_    |   | Hits       | Hits        |_x0007_    |   | HmRun      | HmRun       |_x0007_    |   | Runs       | Runs        |_x0007_    |   | RBI        | RBI         |_x0007_    |   | Walks      | Walks       |_x0007_    |   | Years      | Years       |_x0007_    |   | CAtBat     | CAtBat      |_x0007_    |   | CHits      | CHits       |_x0007_    |   | CHmRun     | CHmRun      |_x0007_    |   | CRuns      | CRuns       |_x0007_    |   | CRBI       | CRBI        |_x0007_    |   | CWalks     | CWalks      |_x0007_    | * | LeagueA    | -           |_x0007_    | * | LeagueN    | LeagueN     |_x0007_    | * | DivisionW  | DivisionW   |_x0007_    |   | PutOuts    | PutOuts     |_x0007_    |   | Assists    | Assists     |_x0007_    |   | Errors     | Errors      |_x0007_    | * | NewLeagueN | NewLeagueN  |_x0007__x0007__x0007_Consider the \""base\"" case, where League=A, Division=E, NewLeague=A._x0007__x0007_So we have LeagueN=0, DivisionW=0, NewLeagueN=0._x0007__x0007_Now consider the intercept — the value when all the_x0007_quantitative variables are zero._x0007__x0007_----_x0007__x0007_In the second model there is an intercept variable.  If you_x0007_look at it, it's just a column of `1`s.  For the \""base\"" case_x0007_intercept, this will be the only variable which is not zero._x0007_So the coefficient on this variable (often called $_x0008_eta_0$)_x0007_will capture the intercept for the \""base\"" case._x0007__x0007_----_x0007__x0007_In the first model, there is no intercept variable, but we_x0007_have the `LeagueA` dummy, which in the \""base\"" case is equal_x0007_to 1. So the coefficient on that will be the intercept in_x0007_the \""base\"" case.[*]_x0007__x0007_If there were no LeagueA variable in the first model, the_x0007_intercept would be constrained to zero. (This would give a_x0007_poor fit if the intercept is not zero)._x0007__x0007_ _x0007_I suppose R is assuming we don't want to impose such a_x0007_constraint and that that if we really did, we could build the_x0007_model matrix explicitly with `cbind`.  Or maybe there's some_x0007_way of expressing that in the formula language.  Anyway, we_x0007_most likely do *not* want such a constraint._x0007__x0007_So what R has done seems like a reasonable thing, and_x0007_probably what we'd want._x0007__x0007_----_x0007__x0007_[*]_x0007_The""",1,0,0
"For linear regression, is it possible to estimate the p-values of coefficients using bootstrapping? _x0007__x0007_It seems straightforward to estimate the standard errors of coefficients using bootstrapping, but I am unable to figure out how to estimate the p-values of coefficients using bootstrapping.",1,1,1
Thank you very much!,1,0,1
Wouldn't you get a t-stat by taking the coefficient and dividing by the standard error.  Then you find the p-value from the t-stat?,1,1,1
"I am starting to work through some of \Applied Predictive Modeling\"" by Max Kuhn as an extension to this course - see http://appliedpredictivemodeling.com/blog/. It is another nice applied book to follow this one, but that uses caret (of which Kuhn is the author) to make parameter tuning and a wide variety of algorithms & pre-processing very quickly available.""",1,0,1
"I think your point is sensible.  With uncorrelated features you should have lower standard errors, but then you're dealing with variables that have no natural interpretation.  This is fine if the reason you care about standard errors is to test reproducibility, but it doesn't help you understand the data.",1,0,1
"In lecture video 7.1, prof. Hastie talks about \Logit scale\"", right around 8:05 min into the video._x0007_Can anyone explain what did he mean by it ?_x0007_Thanks.""",1,1,1
Can you not use Lasso ? Just a suggestion. Lasso might be able to catch the correlation and chOose accordingly.,1,0,1
"Thanks <redacted>. I will look it up. _x0007_By the way, do you have any idea how to implement <redacted> suggestion of incorporating dummy variable. _x0007_Thanks",1,0,1
"I have two questions regarding the R session for chapter 6._x0007_First of all, when I use regsubsets() function it gives me other than the parameters mentioned in the book such as BIC or CP. What's the problem?_x0007_Second, in both R and RStudio, there is no function of the name cv.glmnet().",1,1,1
The accepted answer is one less than the equation given. I also believe it is an error with the software.,1,1,1
"Problem solved. First I installed \glmnet\"" package and then called it.""",1,0,0
Sunny Southern California :),1,0,1
"From the mile high city of Denver, CO",1,0,1
"Each answer choice has three functions in the proposed basis.  You can think of them as functions $b_0, b_1, b_2$ in $y_i = _x0008_eta_0 b_0(x_i) + _x0008_eta_1 b_1(x_i) + _x0008_eta_2 b_2(x_i) + epsilon_i$.  Compare this with the \Linear Splines\"" slide for more context.""",1,0,0
"Question already been titled._x0007__x0007_Thanks,",1,0,1
"After loading the data, how can I know the name of the dataset",1,1,1
definitely available,1,0,1
"Einbeck, Germany",1,0,1
The ls() function will list all the items in memory.,1,0,1
thanks. didn't see it.,1,0,0
Thanks for the comments!,1,0,1
"There is an option with two constants switching alternatively between 0/1 for_x0007_$b_0, b_1$ like $1{x > t}$ and  $1{x leq t}$_x0007__x0007_is this still to be considered a linear spline (ie. constant with a knot)?_x0007__x0007_thanks",1,1,1
Can we just set to zero very small coefficients instead of doing Lasso? Will there be a difference and why?,1,1,1
"In the console type:_x0007__x0007_    install.packages('glmnet', dependencies=TRUE)_x0007__x0007_to install the 'glmnet' package. If it is your first package installation you will be asked to select a CRAN repository. Pick one close to your location. Packages and Libraries are the same thing, by the way. I believe 'library' is a hold-over from S.",1,0,1
"Charlottesville, Virginia, U.S.",1,0,1
"I am sure the first one is right,all the others seem wrong.But this is a wrong answer.",1,1,1
This is was my first choice too trying to adhere to the lectures...,1,0,1
"In page 18/44, what does it mean \ the class labels independent of the outcome\""?_x0007_why the true test error = 0.5?_x0007_how can the CV error estimate that ignores Step 1 is zero?_x0007_I'm totally confused about these?""",1,1,1
"My question is far more simple.  I'm trying to download the data ISLR.  I download the file and double click to expand, get and bunch of folders and then I am stuck.  I can't attach the files or anything else.  Within ISLR 2/data/ I have datalist, Rdata.rdb, Rdata.rds, Rdata.rdx.  I can the list of files as given in the book(data list), but I can't for the life of me find where they are or how to open them into R.  I'm running in R Console on a Mac(Lion).  Thank you so much for your help.",1,1,1
"Auckland, New Zealand.",1,0,1
"\Mars\""""",1,0,1
"I've actually wondered about this: if you have a really large data set, could you not take a subset and still conduct valid analysis? I am pretty new to statistics but would a random sample of the original set not be a good representation of the overall dataset (the case of time series wouldn't work, obviously)_x0007__x0007_This package was recently announced:_x0007_https://github.com/amplab-extras/SparkR-pkg",1,1,1
"the models by alhf only have 1 qualitative variable, no interactions.",1,0,1
"Hello, for the straightforward question 5.R.R1, is there something wrong with the regression below? _x0007_lm(formula = y ~ X1 + X2, data = Xy)_x0007__x0007_Looks like it should work but my inputs are marked wrong (both 0.0 and 0.3)_x0007__x0007_Appreciate your help",1,1,1
"New Jersey, U.S.A",1,0,1
"make sure you are entering the standard error, not the coefficient. and the right standard error, for $X_1$.",1,0,1
alhf ... thanks for that.  Very clear now.,1,0,0
I used a command install.packages and then library(ISLR) and lo and behold I had the data in R.  But the commands I'm supposed to be using don't work.  I can't use read.csv on this stuff or nothing.  What have I done?  Somebody please help me?  Hod do I get to the TA's etc.,1,1,1
"Thanks, that was helpful...I was struggling with the same thing.",1,0,0
"> Problem: We have a data set in which many features are_x0007_> correlated with one another. A linear model is fit to this_x0007_> data set. Therefore, correlated features' estimated_x0007_> coefficients have high standard error (as explained in_x0007_> previous lectures)._x0007_> _x0007_> Question: Can we avoid this problem by using PCR (Principal_x0007_> Components Regression) with M=p (# of principal components_x0007_> = # of original features)? Since all M features in PCR are_x0007_> uncorrelated, coefficients will not have high standard_x0007_> error._x0007_> _x0007_> I have a hunch that something is wrong with this_x0007_> argument. What am I missing?_x0007__x0007_A few thoughts about this:_x0007__x0007_----_x0007_You are Considering PCR with M=p._x0007__x0007_This means we're regressing on a *full* set of principal_x0007_components.  So the model we fit will be effectively the_x0007_same as an ordinary regression on the full set of features._x0007__x0007_(The PCR is indirectly regressing on the full set of original_x0007_features.  The ordinary regression is directly regressing on_x0007_the full set of original features.)_x0007__x0007_To be clear, the words \effectively the same\"" above mean:_x0007__x0007_- The fitted values will be the same._x0007__x0007_- If we take the coefficients on the principal components_x0007_  from PCR and transform them so they are back in terms of_x0007_  original features, we will have the same coefficients as_x0007_  if we just did in ordinary regression directly on those_x0007_  original features._x0007_  _x0007_By \""avoid this problem\"" (in your post), are you thinking of_x0007_the following  situation?_x0007__x0007_There are two highly correlated predictor variables which_x0007_are jointly associated with response, but because of their_x0007_high correlation with each other their standard errors are_x0007_large, making it difficult to \""accurately\"" divide up the_x0007_association (with the response) between the two predictors._x0007_That is, it is difficult to work out \""accurate\"" *separarate*_x0007_slope coefficients on each of the two correlated predictors._x0007__x0007_If you are hoping that principal components will provide a_x0007_way getting \""accurate\"" values for each of the two separate_x0007_slope coefficients, but suspect this is \""too good to be_x0007_true\"", then your suspicion is well-founded._x0007__x0007_Assume, for simplicity, that these two highly correlated_x0007_predictors are the only predictors and that their_x0007_correlation is positive._x0007__x0007_This is like the plot on slide 48/57 from the model_x0007_selection lectures._x0007__x0007_![Plot from lecture slide 48/57 from model selection lectures][1]_x0007__x0007_The first principal component will consist of some kind of_x0007_\""sum\"" of these two correlated predictors._x0007_In the plot from lectures, it might be_x0007__x0007_$PC_1 = 0.8(mat""",1,0,0
"Chicago, IL, USA",1,0,1
"Berkeley Lake (Atlanta Suburb), GA, USA",1,0,1
"I can't seem to get block bootstrapping to work as I keep getting the same estimate of std err of beta1 as without blocking. This was my R commands:_x0007__x0007_    > b1.stderr=function(y,X1,X2){_x0007_    +   Xy.fit<-glm(y~X1+X2)_x0007_    +   summary(Xy.fit)$coefficients[2,2]_x0007_    + }_x0007_    > _x0007_    > b1.stderr.fn=function(data, index){_x0007_    +   with(data[index,],b1.stderr(y,X1,X2))_x0007_    + }_x0007_    > _x0007_    > boot.out=tsboot(Xy,b1.stderr.fn,R=1<zipRedac><zipRedac><zipRedac>,sim=\fixed\"",l=1<zipRedac><zipRedac>)_x0007_    > boot.out_x0007_    _x0007_    BLOCK BOOTSTRAP FOR TIME SERIES_x0007_    _x0007_    Fixed Block Length of 1<zipRedac><zipRedac> _x0007_    _x0007_    Call:_x0007_    tsboot(tseries = Xy, statistic = b1.stderr.fn, R = 1<zipRedac><zipRedac><zipRedac>, l = 1<zipRedac><zipRedac>, _x0007_        sim = \""fixed\"")_x0007_    _x0007_    _x0007_    Bootstrap Statistics :_x0007_          original      bias    std. error_x0007_    t1* <zipRedac>.<zipRedac><phoneRedac> -<zipRedac>.<zipRedac><zipRedac>132419 <zipRedac>.<zipRedac><zipRedac><phoneRedac>""",1,1,1
The slide has K+4 basis functions for a linear spline.  I think it should be K+2 (2k+2 - K),1,1,1
"The data sets are in the package. You don't need to read them from an external file, so you don't need read.csv for these data sets. _x0007__x0007_You don't attach ISLR, it is a package which contains the data sets. _x0007_After you have added the package to your library enter:_x0007__x0007_data()_x0007__x0007_R will show you all the data sets available. There are 13 data sets from ISLR available after you load the package. _x0007__x0007_To add a data set to you workspace (memory) enter:_x0007__x0007_data(Auto)_x0007__x0007_Now the data set Auto is in your memory and you can use it. You can verify that by typing:_x0007__x0007_ls()  # list objects_x0007__x0007_If you attach it, it becomes the default data set and you can address the variables by name without specifying the data set (data frame). _x0007__x0007_attach(Auto)_x0007__x0007_summary(mpg)_x0007__x0007_If you did not attach Auto you would need to use the dollar sign notation to address the variables: Auto$mpg",1,0,0
"Berkeley, California, USA",1,0,1
"I've always thought it wrong to call a package a library, but everyone does.  ;-)_x0007__x0007_I see it as a metaphor. The package is a \book.\"" When you install a package you add it to your library. When you use the function \""library\"" you're fetching the book(package) from your library. A package is not a library. The collection of installed packages is the library. _x0007__x0007_But maybe I'm wrong. The nomenclature mixes metaphors. ;-)""",1,0,1
I wonder if anyone is familiar with different types of Lasso methods such as Grouped Lasso or TreeLasso? I would like to know how we solve their objective functions? I know there are packages like GLMNET or SLEP that are designed to solve these problems but I want to know how can we optimize them ourselves.,1,1,1
"Hey Lars, it's quite unfortunate that your solution is correct only for n=1 and n=3. In the plot below, I plotted your function in red dashed lines and the correct function (the correct solution to the quiz) as black solid line. The vertical line (n=3) shows you that your solution and the correct one coincide for for n=3. However, the larger n gets, the closer your function tends to 0. If you think about it, you wouldn't expect this to happen. Having a large sample of size n does not mean that you are sure that a given data point will belong to a particular bootstrap sample. The correct function instead monotonically increases as a function of n and converges._x0007__x0007_Hope this helps._x0007__x0007_![An example][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>62.png",1,0,0
"why do `y~poly(x,2)` and `y~1+x+I(x^2)` yield different answers ?",1,1,1
"I download the ISLR packages and want to use these data. But i do not know how to use these data through Rstudio. My system is windows, so can any one help on that? i am novel on R",1,1,1
"thanks, indeed ESL is much more clear in the explanation (at least to me)",1,0,0
"In 6.R Dr. Hastie introduced a really handy custom function that we use throughout the lab for validation and cross validation.  It is a really cool bit of code, and for my own delight and edification, I wanted to work through it line by line, and because many of you may be very new to R, I thought some of you might also appreciate the line by line walk through, so I am posting it to the course discussion site.  _x0007__x0007_First, here's the chunk all at once:_x0007__x0007_    val.errors=rep(NA,19)_x0007_    x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!_x0007_    _x0007_    for(i in 1:19){_x0007_          coefi=coef(regfit.fwd,id=i)_x0007_          pred=x.test[,names(coefi)]%*%coefi_x0007_          val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)_x0007_        }_x0007__x0007__x0007_Line 1 (Building a place  to store stuff)_x0007__x0007_    val.errors=rep(NA,19)_x0007__x0007_Line 1 is all about efficiency, what you are doing here is, essentially, building an empty vector of 19 NAs (which will serve as a kind of shelf to store something you will make later on).  Those somethings are the 19 test RMSE, that you will produce with your function._x0007__x0007__x0007_Line 2 (gathering one key ingredient)_x0007__x0007_    x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!_x0007__x0007_This is where you create your test set of data.  I think the really cool thing going on in this line is how the model.matrix function works.  By running the piece of code below, you'll see that it produces something similar to Hitters[-train,], but by using some additional specification (in the formula Salary~.) you tell it to remove the salary variable and include a vector of 1s which it reserves for an Intercept term (important during the matrix multiplication step)._x0007__x0007_Try this and you can see how model.matrix does it's work:_x0007__x0007_    model.matrix(Salary~.,data=Hitters[-train,])_x0007__x0007_Line 3 (Getting loopy...)_x0007__x0007_    for(i in 1:19){_x0007__x0007_This line just specifies how you want your loop to run.  Remember you are looping from 1-19 because you created 19 distinct models using the nvmax=19 above in regsubsets()._x0007__x0007__x0007_Line 4 (Why looping constructs are cool)_x0007__x0007_coefi=coef(regfit.fwd,id=i)_x0007__x0007_Here you are putting the index (i) that the loop is looping through to good use.  As the loop does it's thing it will use this piece of code to create 19 different vectors, one for each model returned by regsubsets above.  Don't take my word for it, try this:_x0007__x0007_    coefi=coef(regfit.fwd,id=6)_x0007__x0007_Line 5 (The magic of matrix multiplication)_x0007__x0007_pred=x.test[,names(coefi)]%*%coefi_x0007__x0007_Lots of good stuff going on here, one of which b",1,0,0
"Things I've seen sometimes, and am wondering what experienced data analysts would make of them:_x0007__x0007_1. response Y is correlated positively with regressor A, and negatively with regressor B (regressors A and B are probably correlated) . Both Y-A and Y-B correlations seem strong. Should I interpret only the positive one? Both the positive and negative? Should I assume the model just doesn't know how to deal with the correlation between A and B? Is there any way I can change either the model or the fitting procedure to be more confident on my results?_x0007__x0007_2. response is correlated positively with a predictor (let's make the categorical, let's say response is larger in condition A than in condition B). However, if I take the response and use it as a regressor - together with the other regressors in my original model - to predict another response, the correlation between the initial response and the newly predicted response appears to be reduced in condition A vs condition B. Can I go ahead and interpret this, or might it be a statistical quirk (and if the second, what might be a more meaningful analysis of how the two responses are correlated differently in two different conditions)? _x0007__x0007_Thank you for suggestions.",1,1,1
"Yea, because the 2nd equation you use is the correct one.",1,0,1
"Hello all,_x0007_Suppose, I need my predictions (on new data) to lie within a certain bounds (or interval, say (0,1) or (0, 100)). As it is, linear regression won't give bounded response. _x0007__x0007_How do we go about solving such a problem ?_x0007__x0007_Assume that I trained my regression model on a training data, which also has a response that is bounded (say within an interval (0,100). _x0007__x0007_Anybody has any thoughts on this.",1,1,1
"Hitters has 20 columns, including the response variable \Salary\"". Code:_x0007__x0007_x=model.matrix(Salary~.-1,data=Hitters) _x0007__x0007_From the R session (part 4) generates a matrix with 20 columns, instead of a predictor matrix with 19 columns._x0007__x0007_The main problem is that a factor \""League\"", with level \""A\"" and \""N\"", will break into two columns, instead of one. Please check._x0007__x0007_I believe what can be used instead is:_x0007_x=model.matrix(Salary~.,data=Hitters)[,-1]""",1,0,0
For the coefficient of X1 I see 0.145xx. Still see 0.025xx for the standard error of X1...just do not see what I'm missing here.,1,1,1
"I am on Ubuntu 12.04 and have no problem with matrix. Though I did notice I have two versions of Matrix, and can't remove them.",1,0,1
"What is the name of the dataset, i mean if we want to fit a linear model to it:  lm(y~x, data = ?)",1,1,1
"I got it , we don't actually need to specify the data , just the formula is sufficient",1,0,0
"I don't understand your question. What is it that is not working as expected? _x0007__x0007_Library(ISLR)_x0007__x0007_attach(College)_x0007__x0007_str(College)_x0007__x0007_That should display the structure of the College data set, telling you that it is a data frame and what is in each column. If you typed: _x0007__x0007_data(\College\"") _x0007__x0007_Then it would appear in the list of objects from ls()_x0007__x0007_attach(\""College\"") doesn't cause the data set to appear in the list ls() it just puts College in the first location of the search order so you don't have to specify the name of the data frame to work with the columns. _x0007__x0007_attach(College)_x0007__x0007_students <- Enroll + Accept_x0007__x0007_Without attach you would need to type:_x0007__x0007_students <- College$Enroll + College$Accept_x0007__x0007_Anyway, if you can better explain what isn't working maybe I can help clear up the problem.""",1,0,1
Thanks Boethian!,1,0,1
"I was also thinking about confidence intervals...sometimes questions are a bit confusing, or students overthink too much :)",1,0,1
How would you determine the complexity of a 'smoothing spline' model?  This seems necessary to make the comparison required by the question._x0007__x0007_Thanks,1,1,1
"Dear Prof Tibshirani and Prof. Hastie_x0007__x0007_I will thank you any piece of advice on a problem that I currently have with some data. I am analysing hourly time series for temperature generated by automatic weather systems. As you mention, the condition of independence is violated in these data sets, one can notice that when generating a autocorrelation plot. I am trying to impute missing data using a principal components analysis approach on this hourly data. In order to do that, I generated a crossed-correlation matrix to figure out whether the correlation is high enough to take neighbouring stations into account. My question is if you may recommend me a way to figure out the significance of this correlation, since I  have heard that my crossed correlation matrix may be somehow inflated due to the high temporal autocorrelation of my data. I will really thank any piece of advice or any clue. I would really thank a suggestion on what methods you may recommend._x0007__x0007_Best regards and thank you for this wonderful course.",1,1,1
"By the way, if a fellow student may cast me some light on this question of mine, I will also be most thankful._x0007__x0007_Best regards.",1,0,1
"It depends what you're trying to do. Here's a toy example using the `caret` package on the `Auto` data. The example is about trying to predict whether a car is 8 cylinders or not using the miles per gallon variable. _x0007__x0007_    require(ISLR)_x0007_    require(caret)  # install.packages('caret') if needed_x0007_    Auto$cyl8<- factor(Auto$cylinders==8)_x0007_    boxplot(mpg ~ cyl8, data=Auto, xlab=\mpg\"", ylab=\""8 cylinder\"")_x0007_    ctrl <- trainControl(method=\""repeatedcv\"", number=5, repeats=10)_x0007_    train(cyl8 ~ mpg, data=Auto, method=\""glm\"", family=\""binomial\"", trControl=ctrl)_x0007_    # No pre-processing_x0007_    # Resampling: Cross-Validated (5 fold, repeated 10 times) _x0007_    # _x0007_    # Summary of sample sizes: 313, 314, 313, 315, 313, 315, ... _x0007_    # _x0007_    # Resampling results_x0007_    # _x0007_    # Accuracy  Kappa  Accuracy SD  Kappa SD_x0007_    # 0.918     0.786  0.029        0.0739""",1,0,0
It is almost always unknown in practice. See text ISLR 10ed. p. 19.,1,0,1
"I don't think it *must* be a U-shaped plot. For example, I was testing this on my own as well and went back to the Smarket data set (where we were trying to predict the direction of the market - Up vs. Down). When I run through a similar set of code using the glmnet tutorial, I get the following - which was intuitive to me. It's also possible that since this is using logistic regression rather than (linear?) regression, the issue lies there - though I don't think so._x0007__x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>0<phoneRedac>9.jpg",1,1,1
"pred <- predict(myfit, newdata=mynewdata)_x0007_    pred[pred < 0] <- 0_x0007_    pred[pred > 1] <- 1",1,0,1
"Also, depending on if you've been using R a bit before you loaded, clearing your work space before you load could help determine the variables the data set provides. Use the *rm(list=ls())* command.",1,0,0
Try rounding off to 3 places after decimal,1,0,1
"It means to generate completely random variables that have no relationship to the class labels. For example, here are 100 random normal predictors that have nothing to do with the binary class label. _x0007__x0007_    data.frame(y=sample(0:1, 50, replace=TRUE), replicate(100, rnorm(50)))_x0007__x0007_The procedure he is describing is taking a huge number of variables, selecting the most correlated ones using the class labels, and using those to predict. It is only mentioned in the lecture.",1,0,1
"Lisbon, Portugal",1,0,1
"Yes, do not add the error value dear.",1,0,1
"Is your \why?\"" a question? =)_x0007_If that's so, then the answer probably would be: because we'd like to find an intersection for these two lines. By assumption right in between two formulas 4.13 and 4.14 in the book, we have K = 2. By solving it, we find the border line, which separates our classes.""",1,0,0
"Kolkata, India",1,0,1
"Hi! I am a postgraduate student in Statistics, and dealing with data and computations for the last 5 years have taught me the sheer wonder that R is! I have had my run-ins with Minitab, Matlab, SPSS and of course the Big Brother SAS, but for daily Statistical usage, R scores over all these any day with it's amazing flexibility, easy language structure and user-friendly interface, not to mention the awesome graphics. So, go ahead, have a wonderful time while you familiarize yourself with R._x0007_#Happy #R-ing",1,0,1
"While you might/might not have found the answer to your question already, let me suggest something which will help you in dealing with R immensely. If you can develop the habit of creating projects, you wouldn't have to specify the entire path of the file you want to read. It also has many additional benefits you can read up on. So, in short, \Create A Project\""._x0007_#Happy #R-ing""",1,0,1
"hi,_x0007_I think the amswer is yes. The CV will give us a pretty good idea of the variance of the model, i.e. if the model depends heavily on the points selected the CV error wil be high. Also the bias part is estimated for each CV run._x0007__x0007_If we used a separate test set imho the variance estimate would be much poorer and we would pick a model baded on less information then in the CV approach._x0007__x0007_regards_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"R2 measures how well data fits the model, but that is only the sample data, not the actual population. An extreme example is if we make p=n then the function will hit every point exactly for the sample data. Imagine you measured the height and weight of one 11 year old and one 12 year old, then wrote an equation to relate height and wt to age. Your equation would give your results exactly, but would be silly for anyone else in the population. when p=n error is zero and R2 is one.",1,0,1
"A small aside._x0007__x0007_Every time most people with degrees in engineering or physical chemistry see Marlow's Cp, they are likely to think of heat capacity._x0007__x0007_Except for electrical engineering, most people with degrees in engineering or physical chemistry had to study thermodynamics.  In that subject, Cp stands for heat capacity at constant pressure and that quantity is widely used in thermodynamics.  _x0007__x0007_In a way, this fact is a small aside and in another way it isn't.  Had Marlow known about this widely used abbreviation for heat capacity, he would probably have chosen a different name for his technique, just as most people would probably be advised not to name their children Elvis or Madonna._x0007__x0007_There is nothing wrong with Elvis, Madonna, or Marlow's Cp technique, but when some of us see these words, our minds are flooded with other associations, not the ones intended.",1,0,1
library(ISLR)_x0007_data(foo)_x0007_View(foo),1,0,1
"You can also try this command_x0007_install.packages(\ISLR\"", dependencies = TRUE)""",1,0,1
"A fit with cubic splines may have many more degrees of freedom than one with a polynomial if the number of knots is high, that is #1 wrong._x0007_Polynomials are $C^{infty}$, so they are in fact more continuous than splines, that is #2 wrong.",1,0,1
"Notice that the question isn't necessarily asking which statements are *true*. It's asking which statements explain the big advantage of natural cubic splines._x0007__x0007_In other words, it's entirely possible for a statement to be true but still not be the reason that natural cubic splines are preferred over polynomials of degree d._x0007__x0007_(As a further hint, only one statement will need to be checked.)",1,0,1
"hi, I use the same method as you are. When I set l=1, I got <zipRedac>.<zipRedac>2595 and the answer pass the grader, but when I set l=1<zipRedac><zipRedac>, my answer didn't pass the grader. Could anyone help us?",1,1,1
"I understand that k-fold cross validation is used to get an estimate of test error, but I do not see how it helps in selecting the best model. In particular, if you're doing k-fold cross validation then overall you're estimating k models (potentially with different parameters) and running k validation tests. But how do you know which of the k models estimated with the training data is best, if ultimately you're just calculating a weighted average of all the test errors?",1,1,1
"Cross-validation chooses the model size associated with the algorithm (forward, backward, hybrid stepwise), not the actual model. _x0007__x0007_For example, let's say you're choosing between model size 2 and 3 (each of which were built using forward stepwise regression). And say cross validation chose model size 3 based on the lowest CV MSE. You would then use all the data to find the best model with 3 variables using forward stepwise regression. Your final model may or may not contain all the variables in your K models.",1,0,0
"Thanks, from this I just understand what is question about...",1,0,0
I agree. I would add that R is great tool for prototyping and python is great to deploy. Right now python still doesn't  caught up R's equivalents in all aspects. _x0007__x0007_I think one should learn python because its a great general tool that you can use to ML as well to other tasks as glue language.,1,0,1
"Let's clarify a few terms:_x0007__x0007_ 1. training set $R^2$_x0007_ 2. training set adjusted $R_{adj}^2$_x0007_ 3. test set $R^2$_x0007__x0007_GayleUWA points out that using training set $R^2$ to choose your model is a bad idea, because this criteria will always choose the model with the most number of variables (overfitting)._x0007__x0007_Metrics such as $C_p, AIC, BIC, R_{adj}^2$ attempt to adjust the training set $R^2$ by penalizing for having too many variables._x0007__x0007_What anonymous is proposing is to use a validation set approach. Rather than adjusting the training set $R^2$, use a test set to choose your model. This is a perfectly valid approach, and an approach that is mentioned in the book and lectures. However, the book and the lecture videos use the term \lowest test set $MSE$\"" rather than \""highest test set $R^2$\"". These are actually equivalent concepts,_x0007__x0007_$R^2 = frac{MSE_{null}-MSE_{model}}{MSE_{null}}$_x0007__x0007_To say that you want to minimize test set MSE is the same as saying you want to maximize test set $R^2$.""",1,0,0
$$R^2 = frac{MSE_{null}-MSE_{model}}{MSE_{null}}$$,1,0,1
"when you have a logit model (logistic regression), spit out your point estimate of logit (your prediction of logit)._x0007__x0007_Calculate the confidence intervals using your logit estimate_x0007__x0007_logit estimate +/- 2 * SE(logit estimate)_x0007__x0007_this will give you two numbers: lower-bound logit and upper-bound logit._x0007__x0007_Convert the two bounds into probabilities: low-bound probability and upper-bound probability._x0007__x0007_\Logit\"" is defined as $log(frac{p}{1-p})$, where p is the probability of the event. Sometimes \""logit\"" is called \""log odds\"".""",1,0,0
"Not exactly. If you follow the code, x=model.matrix(Salary~.-1,data=Hitters), look inside dimnames(x)[[2]]. The problem is with \LeagueA\"" and \""LeagueN\"". They are actually just one same factor variable with two levels. This can't be right. And further, use coef(), it will show you the problem. _x0007__x0007_glmnet() will compute betas for intercept+19 predictors. This initial setting splits one variable into two, if you look inside the object \""x\"".""",1,0,0
"The question (7.1.R1) asks which of_x0007__x0007_* spline terms_x0007_* polynomial terms_x0007_* interactions_x0007_* arbitrary linear combinations of variables_x0007_* step functions_x0007__x0007_can be added to a linear model to capture non-linear effects?_x0007__x0007_My initial thought was splines, polynomials, and step functions, but that is being marked as incorrect. My thinking was that an arbitrary linear combination is still a linear surface, as that is what a linear model is. And with the interaction, the linear effect of one variable has influence on the linear effect of the other variable, but this is still linear (in the sense that \linearity\"" means the effect of $x$ on $y$ is the same, no matter the value of $x$, *when all other variables in the model are held constant*)._x0007__x0007_Where am I going wrong?""",1,1,1
interactions are non-linear. for example:_x0007__x0007_$y = _x0008_eta_0 + _x0008_eta_1X_1 + _x0008_eta_2X_2 + _x0008_eta_3(X_1*X_2) + epsilon$_x0007__x0007_The effect of $X_1$ on $y$ is non-constant because the effect depends on $X_2$._x0007__x0007_$y = _x0008_eta_0 + (_x0008_eta_1 + _x0008_eta_3X_2)X_1 + _x0008_eta_2X_2  + epsilon$,1,0,0
"Thank you for the NCSS document. I continue to be curious why this method is called Ridge Regression. Also curious why Lasso is called as such. Only a curiosity on my part. _x0007__x0007_In the document section, *Sources of Multicollinearity*, the third source is that of an over-defined model and states, \Here, there are more variables than observations\"". It also states, \""This situation should be avoided\"". From our course material I understand that we need to protect against over-fitting our data. However, the issue of more variables than observations seems to be a valid big data issue that really cannot be avoided._x0007__x0007_It seems to me big data means we no longer have control over the sampling plan, or the space over which our data are collected, or the method by which it is collected. The genome we have is what we have. This in turn causes statisticians to look beyond traditional controlled environments to deal with the new level of, what I will term as, uncontrolled data. Meaning we did not control external sources of variation, nor did we control correlation between variables, nor did we control the range over which the variables will vary. I have witnessed this even in manufacturing environments. The concern spilling from this observation is validity of analytical results. This seems to me why validation, cross-validation, and test error estimation is so important. We need a way to see our analytical results are valid/usable/convincing/legitimate.""",1,1,1
The models presented in chapter 6 use least squares fit either directly or with constraint. Least squares fit assumes the error in the data are independent and identically distributed as Gaussian. It also assumes the X variables are fixed and do not vary. I think we have held to these assumptions so far in our studies. Am I correct in thinking this?,1,1,1
"Seville, Spain",1,0,1
"But then again, to test all possible models with cross-validation, assuming one subset of variables per model, you need (2^p) folds.",1,0,1
"Thanks <redacted> for leaving your answer here. But I am still not sure. May be it is the artifact of the model used, as you said. I would like to read one of the teaching members comment or the TAs comments. Someone with a lot more experience than me.",1,1,1
"Hi all,_x0007__x0007_In the ch4.R file there is a part on linear discriminant analysis and another on k-nearest neighbours that is not touched upon in the 10.14 min video called \Classification in R\""._x0007_I haven't found another video tackling these last few lines of codes. Does anyone know if such a video exists ?_x0007__x0007_Thanks,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"Interesting question! I have not made the computations, but following the bias/ variance tradeoff principle I would guess the extra variance incurred in adjusting the bias would cancel to some extent the benefits of shrinking.",1,0,1
"Hmmm...something seems wrong with this question.  I had the same initial response as Gavin (splines, polynomials, and step functions) and got it wrong.  I then added 'arbitrary linear combinations' (I originally didn't like the word 'arbitrary') and got it wrong too.  Since William has pointed out that interactions are not linear, and the book says splines, polynomials, and step functions are linear, it seems that the answer has to be either A/B/D/E or A/B/E but both are marked incorrect.  Is there something silly that I am missing?",1,1,1
From the equation that gives the Smoothing spline expression it seems very similar to the Hoddrick-Prescott filter very much used on economics._x0007_Is the Smoothing spline used also for detrending time-series? Can smooth.spline() be used as a kind of filter for time-series?_x0007__x0007_Thanks for your insight!,1,1,1
The following URL might help: https://stat.ethz.ch/pipermail/r-help/2008-February/<zipRedac>54274.html_x0007__x0007_See also:_x0007__x0007_- http://statistic-on-air.blogspot.com/2009/09/polynomial-regression-techniques.html,1,0,1
"I think I know the answer (permutations instead of combinations + adjusting for sampling with replacement), but the answer format does not allow factorals. _x0007__x0007_Any hints on the format/notation would be welcome!",1,0,1
"Ok, so interactions are also for no-linear functions. i think somebody has already mentioned this, thanks!",1,0,1
"Take the variables from your final lasso and fit a least squares model. For example, lasso chooses x1, x10, and x15 out of 100 potential variables. Fit a least squares model (or logit if your response is binary) with only those 3 variables.",1,0,1
Additional detail is provided on page 288 of the textbook for this course.,1,0,1
"Not Found_x0007__x0007_The requested URL \/~tibs/ElemStatLearn/datasets/cvfolds.txt\"" was not found on this server._x0007_Does this link from the first data set work for anyone ? this is the last link on the data set. ( the 14 types of cancers)""",1,1,1
"Ok, so why couldn't they have explained it this well in the book or lectures?  Thanks!",1,1,1
"The point is to model the mechanism that generated the data, so I'd suggest trying lots of things and seeing what works best.  So, beta regression or nonlinear regression are good things to try.  But, personally, I'd probably start with something simpler than either of those.  It's hard to say without looking at the data and knowing the intended goals of the model, but in general I'd probably suggest tranforming the response and using standard models like linear regression, splines, lasso, whatever, on the transformed response.  So, for example, if it really is bounded on an open interval, you can shift and scale the response until it's bounded on (0,1) and the do a logit transform. The transformed response will then be unbounded and the standard models may apply.  Of course, if those models don't fit well (e.g., examine the residuals), then you'll need to try something else.  For example, you may want to look at folded power transforms for a series of transforms that may work better than logit.  If that doesn't work, then try more complicated approaches._x0007__x0007_Hope that helps.",1,0,0
"There is a wonderful article by Deidre McCloskey and Stephen Ziliak  - \The Standard Error of Regression\"". I think it gives valuable insight into your question.  Check it out: http://www.deirdremccloskey.com/docs/pdf/Article_189.pdf""",1,0,1
"How can I become familiar with the way that the Lasso function is optimized and write my own program instead of using GLM package? _x0007_I read the related papers from Friedman, Hastie and Tibshirani and other authors but they are too mathy. Can anyone introduce me an easier reference?",1,1,1
"Not at all. There's no typo. It means that after y has been fitted by x1, use x2 to fit the residuals.",1,0,1
"@scigeek:_x0007__x0007_> Great answer ahlf. Thanks a lot. I will try out your_x0007_> suggestion on my data._x0007_> _x0007_> I also did a search and found the reference to Beta_x0007_> Regression, where the response variable is restricted within_x0007_> (0,1)._x0007_> _x0007_> I am wondering what is your th",1,0,1
EXCELLENTissime MooMoo!,1,0,1
"Hi Kikaka,_x0007__x0007_Not sure if this explanation helps but I thought I'd give it a go:_x0007__x0007_If the inclusion probability is indeed 1 - 1/e it must be so that the exclusion probability is its opposite, i.e. 1/e. So you have a 1/e likelihood that a data point will not be drawn from the original training set. The likelihood never changes because we have replacement (every data point that gets picked out once can be picked out again). Remember that the result of 1 - 1/e is contingent on the fact that the bootstrap sample and the training data are of the same size (k), which makes the result independent of sample size. I'm not sure what 'B' represents in your equation but if B >> 1 the probability converges to 0 and for 0 < B < 1 the probability would converge to 1. Neither alternative is likely with replacement of data points.",1,0,0
"Hi all,_x0007__x0007_You've probably all moved on from this but I think there's a small point to clarify. I do not think amw5g is correct in restating the question (and hence nor is brucehoff in arguing so). The question DOES focus on a single bootstrap sample and you get the probability of p = (1-1/n)^n as pointed out by boethian. If however the question would indeed refer to \any bootstrap sample\"" one would have to multiply the probability that it does not happen in 1 sample with the number of bootstrap samples (say k), leading to (1-1/n)^kn._x0007_E.g. Take two bootstrap sample, the first occurs with probability p and the second occurs with the same probablity p. The chance they both occur is p*p thus p^2""",1,0,1
"I think in an advanced course I would really like to see some more theory -not exclusively theory- but some more depth. For example, I have no idea why repeating examples in a training set when using Bootstrap is beneficial at all.. I would have liked some more discussion of that.",0,1,1
"Another thing to mindful of is the different sub-fields within statistics. I was trained as an econometrician, where we take into account economic significance. In econometrics, we are more concerned with pre-defined models based on economic theory, so we don't add or drop variables to make the model \fit better\"". In some econometric papers, they are ecstatic with an R^2 of <0.2.  _x0007__x0007_I think an econometrics course would be better fit to your needs; not that this course will hurt any! You just have to mindful of the goals of each sub-field.""",1,0,1
"Hi everyone,_x0007_I have developed a model to predict the probability of failure by counting the number of the properties failed in predefined classifications. Now, to validate my model, I have thought of K-fold cross validation. By dividing the data-set in 10 folds. Train with 9 folds every time and finish the process. Then take the average of all folds and compare it with each fold and calculate the error of the model. Do you think this is right?",1,1,1
Coordinate gradient descent is the optimization method.  _x0007_https://www.cs.cmu.edu/~ggordon/<zipRedac>0725-F<zipRedac>2/slides/25-coord-desc.pdf,1,0,1
"Hi William,_x0007_Thanks for your reply. I'm not sure whether you are actually saying something different than what we are apparently not allowed to say:_x0007_Compare _x0007_- 95% sure that the CI contains the true value (what you wrote)  _x0007_with_x0007_- The C.I. contains the true value of beta with probability (1- alpha) (what I wrote) _x0007_To me both are the same and both are the same as the alleged false answer (there is a 95% probability that the true parameter is between 0.4 and 0.5)_x0007__x0007_Am I missing something here or is this really a matter of semantics?",1,1,1
"The best starting point would be the **nlme** package which comes with R. Its successor, **lme4** is still under development but improves the computational algorithms for certain types of crossed or nested random effects. The latter package also handles GLMs containing random effects, so called GLMMs, whereas **nlme** is for linear mixed models and non-linear ones (where non-linear refers to non-linearity in the parameters, *not* the non-linearity we've been looking at one this course. An early version of a book that Doug Bates (until recently lead author of **lme4** and **nlme**) is available alongside the **lme4** sources on R-Forge: http://lme4.r-forge.r-project.org/lMMwR/",1,0,0
Thanks Dan. I came to know a lot while analyzing this data set.,1,0,0
"agreed. Although, there is a good thread going on for this question._x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/53<zipRedac>8f<phoneRedac>16edfd<zipRedac><zipRedac><zipRedac><zipRedac>25",1,0,1
"setwd('C:/Users/<nameRedac_<anon_screen_name_redacted>>/Documents')_x0007__x0007_load(\7.R.RData\"")_x0007__x0007_plot(x,y)""",1,0,1
Hi from Ukraine. _x0007_My name is <nameRedac_<anon_screen_name_redacted>> and I am working mostly with web development and data science projects these days._x0007_Please feel free to connect with me via LinkedIn - ua.linkedin.com/in/<nameRedac_<anon_screen_name_redacted>>/_x0007_Cheers!,1,0,1
I tried this and got a wrong result (0.032)?,1,1,1
If you have a spare $1450 laying around you could take this course:_x0007__x0007_http://www.stanford.edu/~hastie/SLDM/sldmpaIII.html,1,0,1
"Actually there are K + d + 1 base functions, since 1 is also must be used as a base function, as in quiz problem 7.2.R2",1,0,0
"The hardcover discount does not seem to be recognized by Springer. When I accessed the web site using the discounted ISLR link, by the time I got to checkout I got a message that the product does not have enough stock. After contacting Springer customer service they offered to sell me the book which is in stock but they do not know of any discount. This is how they replied to the link from this course page:_x0007_\Dear Sir,_x0007_Thank you for your e-mail. Please note that there is a misunderstanding: We do not offer any discount for this title.\""""",1,1,1
Check out chapter 11 from the book. Sorry I clicked submit too quickly above.,1,0,1
Is it legit to use K-fold cross-validation to select your tuning parameters **and** to estimate your test set error? I suspect some over fitting here.,1,1,1
"Thank you very much, scigeek!",1,0,1
"There will be some bias in the estimate of the MSE if you are both tuning the model *and* estimating the MSE at the same time._x0007__x0007_Double CV is often used when you need to tune *and* estimate the model. Essentially you have an outer CV loop, say a $k$-fold with $k = 5$, and then within each fold you do an entire CV (another $k$-fold for example or a bootstrap CV). The inner fold is to choose the values of the parameters you want to tune (number of components etc) and then you fit the tuned model and use it to predict for the fold left out in the outer loop. Repeat for all folds in the outer loop._x0007__x0007_That said, it is my understanding that the bias in tuning *and* estimating the error at the same time is not that large in many cases. Whether that bias is larger or smaller than the bias introduced by fitting CV models to even smaller subsets of the training data is something I can't answer.",1,0,0
"Sounds like you got someone in customer service who is a bit, um, misinformed._x0007__x0007_The do indeed offer a discount. I got it and have the invoice to prove it._x0007__x0007_Sheesh._x0007__x0007__x0007_Maybe it would help if you sent them a screenshot of the \How to access the course textbook\"" page here (which mentions the discount), and a screenshot of the page on their own site (which *still* shows the discount).""",1,0,1
I took an econometrics course way back in the day (late 70s?).  The problem is that you run into so many economists :),1,0,1
"Why is there no discussion about the pitfalls of step-wise regression?   We have known for some time now that using the data to both define our model and estimate its parameters is prone to bias.  Chatfield (1995) discusses these matters in his paper _x0007_**Model Uncertainty, Data Mining and Statistical Inference**_x0007__x0007_There are many other good papers that have discussed the matter.  I don't expect your opinion to be the same as mine, but the absence of any discussion of this matter is, to me, rather disconcerting.  This text is implying that the acceptance of stepwise regression techniques is universally accepted.  I know the writers of the text must know better.  _x0007__x0007_http://www.ncbi.nlm.nih.gov/pubmed/1<phoneRedac>_x0007__x0007_Just because you stir ingredients in a pot (apply a statistical technique to data)  doesn't give you the right to call it soup (an actionable inference)._x0007__x0007_I urge you to include a discussion of these concerns in future versions of this course.",1,1,1
"I hope I've understood your example correctly. I think it is important to ask as \who is in the skill group vs the random group?\"". The p-value of .20 suggests the historical high-performer has a 20% chance of being in the random group. I suppose it is kind of arbitrary and depends on the context; 20% chance may be small enough to have confidence he or she is in the special-skill group to trust with an investment. But then in science articles where they talk about research at the Swiss atom smasher, they look for events that are 6 sigmas out, I guess the physicists want to be very confident it's in the special group.     _x0007_But for physicists, I suppose they are looking at something in the moment, with fund managers, it is looking at expected performance in the future. Did you have a validation set in your experiment? I find the chapter 5 concepts on training vs test error to be very useful in trying to distinguish between groups, when there is a time difference between observations and response.    _x0007_I hope I didn't miss the point of high p-value adding economic significance -- a p=.20 could add a lot to a simulation, but the perspective I'm looking at is how confident you can be in the p=.20 to perform that well going forward.""",1,1,1
"Shaanxi, China",1,0,1
"I found that quiz formulas don't show appropriately. I didn't notice this problem until a recent firefox update (now I use firefox v20) _x0007__x0007_I tried IE, but it says it doesn't support the frame or something at all._x0007__x0007_Can anyone help? Thanks_x0007__x0007_Attached:_x0007_![formulas don't work][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>12.png",0,1,1
"Sure. I have f_i, but could you walk me through an example?_x0007__x0007_Let's go back to the equation: Bias is E[f_i - E(f_hat_i)]^2_x0007__x0007_now, to obtain E(f_hat_i), how many times to I fit the model? Given a data point represented by a specific i, I will have only one value of f_hat_i. so is E(f_hat_i) = f_hat_i ??? Without multiple f_hat_i for the ith data (meaning multiple f_hat_i for the same X_i), how does the E (expected value of) part make sense?",1,1,1
"Yeah, there are four videos along the top there.",1,0,1
Hi all_x0007__x0007_i'm wondering how general additiv models handle with missing values._x0007__x0007_is it possible to use multiple imputation with gam? (e.g. first impute 5 times your missing values with package mice and then use gam?)_x0007_and what about pooling the estimates? are the rubin rules for pooling the estimates also valid in the context of gam models? if yes are there any tools to perform analyses and combine the results from multiple data sets?_x0007__x0007_thanks! greg,1,1,1
"Hi Simon_x0007__x0007_In \Classification in R\"" there are three videos in the line where you choose between vidieo and tasks - number two is about lda and the last one is about Nearest neighbors._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,0,1
"Does K (e.g. 5 or 10) fold and Bootstrapping apply to time series data (e.g. Stock market or Forex)?_x0007__x0007_For example, wouldn't a K fold cross validation need to take an ORDERED sample to build the model? For example, use stock data from 2001 to 2002 as the sample to build the model. BUT the way I understand it, the sample used to build model is RANDOM (e.g. use random data between 2001 and 2014.) And using a random sample (as with k folds and bootstrapping) for time series data would not make sense._x0007__x0007_For example, in the book in ch 4, years 2001-2004 are used to build the model and then 2005 is used to test the model. Is there a reason the book doesn't use random data throughout 2001-2005 to build the model? If the reason is that the data must be ordered, then **IS there a way to use bootstrapping and k fold in an ORDERED way such as what would be required with stock or forex data?**",1,1,1
"Thanks all for the valuable discussions above. I also did not get clear idea about \block bootstrapping\"" from the R-session videos._x0007_Although I did not get \""the zero std.error problem\"", may be because for \""ordinary programming-style\"" it is like an intrinsic property - to use only the arguments inside the function body that declared in the header.""",0,1,1
"Problem solved--I hadn't noticed the clickable link to download the 7.R.RData in the 7.R question.  Once I clicked that link and downloaded the data, I was able to open R and get a solution to the questions.",1,0,0
This is helpful. Thank you.,1,0,0
Hi John:_x0007__x0007_I really appreciate your response. Would you be kind enough to illustrate this with a very small data set?,1,1,1
"Hi Team,_x0007__x0007_In my opinion, to best way to learn the ML learning is to work with an actual data. I need to go through one example from beginning to an end to pick a model for a particular model._x0007__x0007_Given a data set, how would you go about creating a model and picking the model? Can some one walk me through that?_x0007__x0007_For example, let's say I have a data frame that has the actual observations:_x0007__x0007_1. I need to create a truing model, using linear, bayes, random forest, svm etc._x0007_2. I need to test this model against the test data set, right? Where do I get the test data set? is it a subset of the actual data frame?_x0007_3. Once I have run all the different models agains my original data frame, how do I compare the results? What command I can issue in R to do that?_x0007__x0007_I really appreciate any insight._x0007__x0007_Best,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Thank you iman2546 & timsetsfire. I have been wondering about this as well.,1,0,1
Have you tried Chrome?,1,0,1
"There are references to things being stable in our text. For example, LDA is more stable than Logistic Regression when classes are well separated and if n is small and the distribution of the predictors are approximately normal for each class (pg.138). The Natural Spline is more stable than a Regression Spline due to the addition of the linear boundary constraint (pg.274). Regression Splines are more stable than Polynomial Regression as it reduces the need for high degree transformations of X by increasing the number of knots (pg.276)_x0007__x0007_I believe 'stable', in all cases, refers to the coefficient estimates and the fact that the standard error of these estimates is kept as small as possible. Am I correct and is there more to it?",1,1,1
"On slide 11 of the non-linear notes, Hastie shows the basis expansion for a cubic spline. However, he does not show the basis expansion for a natural cubic spline. What is the basis expansion for a *natural* cubic spline?_x0007__x0007_In particular, I want to be able to write the natural cubic spline inside the lm() function without calling the splines library. For example, in the case of a cubic spline, I could write:_x0007__x0007_reg.fit <- lm(y ~ x + I(x^2) + I(x^3) + I((x > 100)*(x - 100)), data)_x0007__x0007_How would I do this for a natural cubic spline?",1,1,1
"For, say, 4 knots, try $y$ ~ $ x + (d_1 - d_3) + (d_2 - d_3)$,_x0007__x0007_where_x0007__x0007_$d_1 = [(x - t_1)^3 * (x > t_1) - (x - t_4)^3 * (x > t_4)] / (t_1 - t_4)$_x0007__x0007_$d_2 = [(x - t_2)^3 * (x > t_2) - (x - t_4)^3 * (x > t_4)] / (t_2 - t_4)$ _x0007__x0007_$d_3 = [(x - t_3)^3 * (x > t_3) - (x - t_4)^3 * (x > t_4)] / (t_3 - t_4)$ _x0007__x0007_1) Using square brackets for clarity. Replace with parentheses._x0007__x0007_2) The expression for the regression on y has **three** terms. You probably have to use $I(.)$ to avoid interpretation.",1,0,0
"Hi,_x0007__x0007_with respect to trading system validation the walkforward process as described by Dr. Howard Bandy and also by David Aronson is a special case of k fold cv that addresses the concern you have observed._x0007__x0007_With respect to risk and position sizing bootstrapping can be used in the context of  Monte Carlo simulations.  Here the out of sample trade set resulting from the walkforward process serves as the original data set of trades from which the bootstrap samples are extracted to assess risk and determine position size.  See also Bandy._x0007__x0007_Best,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Are $t_1$ and $t_4$ the boundary knots (where the linear slope constraints kick-in for \extreme\"" x values)?""",1,1,1
"In the text book, model 7.9, p 273, shows the cubic spline model as:   _x0007_$y_i = B_0 + B_1b_1(x_i) + B_2b_2(x_i) + ... + B_K+_3b_K+_3 (x_i) + error $   _x0007__x0007_In the previous section, I can follow the point that you will have K + 1 formulas, because you will have a formula for each knot interval, plus the first interval before the first knot. But in the above model for cubic spline, they go up to K + 3 formulas, I'm confused where they get the extra two? In other words, why doesn't it stop at B_K b_K (x_i)? Do K+1, K+2, and K+3 formulas throw out some of the terms?    _x0007__x0007_Or if there is a link to an example, where they show the actual numbers being computed, that might help me as well, sometimes notations and formulas are abstract and hard to see what the formula is actually doing.",1,1,1
Right,1,0,1
"When p > n, least squares cannot be fitted.",1,0,1
"I'm having a hard time getting the red line (natural spline coded using ns) and green line (natural spline coded manually) to match._x0007__x0007_    library(ISLR)_x0007_    _x0007_    salary <- Wage_x0007_    _x0007_    quantile(salary$age, c(0.2, 0.4, 0.6, 0.8))_x0007_    _x0007_    # linear spline with 4 knots_x0007_    _x0007_    reg.lin.spline <- lm(wage ~ age _x0007_                         + I((age > 32) * (age - 32)) _x0007_                         + I((age > 39) * (age - 39))_x0007_                         + I((age > 46) * (age - 46))_x0007_                         + I((age > 53) * (age - 53))_x0007_                         , salary)_x0007_    _x0007_    summary(reg.lin.spline)_x0007_    _x0007_    # plot actual and fitted values_x0007_    _x0007_    fit <- data.frame(x=salary$age, y=reg.lin.spline$fitted.values)_x0007_    fit <- unique(fit)_x0007_    fit <- fit[order(fit$x),]_x0007_    _x0007_    plot(salary$age, salary$wage, col=\darkgrey\"")_x0007_    lines(fit$x, fit$y, lwd=2,col=\""blue\"")_x0007_    abline(v=c(32, 39, 46, 53), lty=2, col=\""black\"")_x0007_    _x0007_    # natural spline with df=4_x0007_    _x0007_    library(splines)_x0007_    _x0007_    reg.ns <- lm(wage ~ ns(age, df=4), salary)_x0007_    _x0007_    summary(reg.ns)_x0007_    _x0007_    # plot actual and fitted values_x0007_    _x0007_    fit <- data.frame(x=salary$age, y=reg.ns$fitted.values)_x0007_    fit <- unique(fit)_x0007_    fit <- fit[order(fit$x),]_x0007_    _x0007_    lines(fit$x, fit$y, lwd=2,col=\""red\"")_x0007_    _x0007_    # natural spline (manual with 4 knots)_x0007_    _x0007_    t1 <- quantile(salary$age, c(0.2, 0.4, 0.6, 0.8), type=7)[1]_x0007_    t2 <- quantile(salary$age, c(0.2, 0.4, 0.6, 0.8), type=7)[2]_x0007_    t3 <- quantile(salary$age, c(0.2, 0.4, 0.6, 0.8), type=7)[3]_x0007_    t4 <- quantile(salary$age, c(0.2, 0.4, 0.6, 0.8), type=7)[4]_x0007_    _x0007_    salary$d1 <- ((salary$age > t1)*(salary$age - t1)^3 - (salary$age > t4)*(salary$age - t4)^3) / (t1 - t4)_x0007_    salary$d2 <- ((salary$age > t2)*(salary$age - t2)^3 - (salary$age > t4)*(salary$age - t4)^3) / (t2 - t4)_x0007_    salary$d3 <- ((salary$age > t3)*(salary$age - t3)^3 - (salary$age > t4)*(salary$age - t4)^3) / (t3 - t4)_x0007_    _x0007_    reg.ns.man <- lm(wage ~ age + I(d1 - d3) +I(d2 - d3), salary)_x0007_    _x0007_    summary(reg.ns.man)_x0007_    _x0007_    # plot actual and fitted values_x0007_    _x0007_    fit <- data.frame(x=salary$age, y=reg.ns.man$fitted.values)_x0007_    fit <- unique(fit)_x0007_    fit <- fit[order(fit$x),]_x0007_    _x0007_    lines(fit$x, fit$y, lwd=2,col=\""green\"")_x0007_    _x0007_    _x0007_    # legend_x0007_    _x0007_    legend(\""topright\"", # places a legend at the appropriate place _x0007_           c(\""Linear Spline\"", \""Natural Spline (df=4)\"", \""Manual NS (K=4)\""), # puts text in the legend _x0007_           lty=c(1,1,1), # gives the legend appropriate symbols (lines)_x0007_           lwd=c(2.5,""",1,1,1
"If the training sets in Leave One Out Cross Validation (LOOCV) are highly correlated by only differing in one observation, why does this result in a high variance of the average of errors?",1,1,1
"In the first video, there are no significant coefficients (3:30), how is it possible to try to predict values with that? Or is it considering like an example for using of R?",1,1,1
"After I read your explanation, I understood it.  Thanks for sharing.",1,0,0
"The base has $K+d+1$ functions for $K$ knots and a degree $d$ spline. _x0007__x0007_So for a linear spline you can take (with knots $t_1, t_2, ..., t_K$):_x0007__x0007_ $y_i = B_0+B_1.x_i+B_2(x_i-t_1)_+ + B_3(x_i-t_2)_+ + ... +B_K+_1(x_i-t_K)_+$ _x0007__x0007_and for cubic:_x0007__x0007_$y_i = B_0+B_1.x_i+B_2.x_i^2+B_3.x_i^3+B_4(x_i-t_1)^3_+ + B_5(x_i-t_2)^3_+ + ... $_x0007_$ ...+B_K+_3(x_i-t_K)^3_+$",1,0,0
"Perth, Australia",1,0,1
"From that great book reference!_x0007_If we have 2 knots. so with y=a, y=b, y=c in the three regions we describe 3 horizontal lines as our best fit._x0007_Next we allow slope so we add another 3 parameters (d, e and f) describing that slope in each region. BUT when we introduce continuity we lose b and c because a + xd will describe b, and adding xe will describe d._x0007_Then we are asked in the question to describe the x constraints on these four parameters, a, c, d and e._x0007_However the question only has one knot, so we just need a, c, and d.",1,0,0
"Thinking of doing this for my PhD. Have written lots of R code, loops, functions etc also used metlab, and keen to explore this......_x0007_advice welcome",1,0,1
"Yes, I am not talking about the components of model, just about significance. So do we look at p-values just sometimes if we want? Why authors even consider them in some cases if it is totally not important? Or big p-values just a signal that predictors could be correlated?_x0007_Thanks for the article, will read.",1,1,1
After I red the explanation it was much more clearer what was being asked._x0007_Thank you!,1,0,0
Why it cannot be fitted? I don't get it,1,1,1
"The best sentence of the paper:_x0007__x0007_\To sum up, (small) p values help us learn nothing about the data, the model and the importance of the result.\""""",1,0,1
Hi teranceee_x0007__x0007_First download the workspace 7.R.RData as suggested by jamesgehrmann_x0007_above._x0007__x0007_For Mac:_x0007__x0007_In R: Workspace/Load Workspace File _x0007__x0007_In RStudio: Session/Load Workspace_x0007__x0007_to open a dialog box and choose 7.R.RData._x0007__x0007_Note that .RData cannot be opened in text editor.,1,0,1
"Hi,_x0007_At first you will need some additional tools: 'devtools' in R, MikTeX for documentation and for version control use Git or SVN (http://www.rstudio.com/ide/docs/version_control/overview?version=0.97.312&mode=desktop)_x0007__x0007_Then You may create your packages in RStudio by creating new project with type \package\"". That wizard is really helpful!_x0007__x0007_Also I did my first steps in packaging R code by looking at simple \""Hello World\"" package_x0007_\""http://notepad.patheticcockroach.com/1342/a-hello-world-r-package-or-a-quick-start-into-writing-r-extensions-with-some-c-inside/\""""",1,0,0
for time series bootstrapping the tsboot function of the boot library should work. It's been discussed elsewhere in this forum. Just search 'block bootstrap' or 'tsboot',1,0,1
"this is the code let me pass the R3 question:_x0007__x0007_    boot.fn = function(data,index) return(coef(lm(y~X1 + X2, data = data, subset=index)))_x0007_    boot(Xy, boot.fn, 1000)_x0007__x0007_but when I pluged that in to the tsboot, I got the error:_x0007__x0007_    tsboot(Xy, boot.fn, R=1000, sim=\fixed\"", l=100)_x0007__x0007_     Error in eval(expr, envir, enclos) : _x0007_      argument \""index\"" is missing, with no default _x0007__x0007_So, like others, I searched the forum, looked some threads, finally, I changed the code into:_x0007__x0007_    boot.fn = function(data,index) return(coef(lm(y~X1 + X2, data = data[index,])))_x0007_I got the right answer for R4 and R3. _x0007__x0007_The thing is I cann't figure out why the one(boot.fn) above works fine in boot, what's wrong_x0007_when used in tsboot??? Please, anyone can direct me to the right document?thanks a lot!!!""",1,1,1
"Chao, I got stuck in block bootstrap also._x0007__x0007_Look through the topic \help on \""block bootstrap\""\"" 16 days ago - it explains everything!!!_x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-quiz-review/threads/52fa4<zipRedac>2c<phoneRedac>71f<zipRedac><zipRedac><zipRedac><zipRedac>27_x0007__x0007__x0007_Briefly, modify your call of 'tsboot' and try_x0007_tsboot(Xy, boot.fn, R=1<zipRedac><zipRedac><zipRedac>, sim=\""fixed\"", l=1<zipRedac><zipRedac>, index=1:1<zipRedac><zipRedac><zipRedac>)""",1,0,0
I think this is only machine learning suite: http://en.wikipedia.org/wiki/Weka_%28machine_learning%29,1,0,1
"Sorry that I don't see how can I edit my comment, here are the courses:_x0007_https://weka.waikato.ac.nz/explorer",1,0,1
"Thanks a lot! imag, it seems like it has something to do with the ... argument I should pass along with the boot.fn, i.e.(statistics). _x0007_I think I was misguided by the example in the book in p.196 where no extra arguments are explicitly passed to the statistics function. _x0007_Thanks for your fast response.",1,0,0
"Hi,_x0007__x0007_Not sure if this fix works for everyone but I got the idea from Lisa:_x0007__x0007_run the following_x0007_library(ISLR)_x0007_data(Carseats)_x0007_fix(Carseats)_x0007__x0007_This worked for me, while initially I had the same problem as everyone here...",1,0,0
Thanks Heidi!_x0007__x0007_I had figured it out yesterday when I had the same problem for chapter 5 :),1,0,0
"Great idea, request sent.",1,0,1
I would like to know if the videos will be available after the course in some form.  The downloads are of grainy quality and I would prefer to have access to the originals if possible.  _x0007__x0007_Thanks.,1,1,1
"If you access YouTube you can download HD versions of the videos directly from there with your favorite method for doing so.  If you are on a Mac, try ClipGrab http://clipgrab.org/?.",1,0,1
firefox add-on https://addons.mozilla.org/en-US/firefox/addon/download-youtube/,1,0,1
"the load() function does not output R objects. Therefore, you cannot assign the output of load() to an object \hw\""._x0007__x0007_Just run the load() command by itself, like this:_x0007__x0007_    setwd(\""C:/wheremyfileis\"")  # set your work directory_x0007_    load(\""5.R.Rdata\"") # load your data file_x0007_    ls()  # show what is in your workspace_x0007_    summary(Xy)""",1,0,0
Why do you take df=4? Shouldn't that be 5?,1,1,1
we are using machine learning algorithms for classify things._x0007_there are different type of algorithms available which gives different error rate on different input and no one is better then other all time._x0007_Do we have some algorithm that selects between different models efficiently without initiating by user._x0007_I mean some type of research is done related to optimization of model selection or anything somebody knows about please comment and share links.,1,0,1
"I've tried ns(age, df=4) and ns(age, df=5). Neither match what I coded manually.",1,1,1
"I know confidence interval is for the average, and prediction interval is for the prediction for one data. But it's really hard to say what's the difference.",1,1,1
Excellent Reference :) Thanks!,1,0,1
"Just type \betas\"", and you will see the summary of se.""",1,0,0
what have you done so far?,1,0,1
Hi - Has anyone tried to reproduce lab 6? I'm getting different results for the mean.cv.errors in the cross-validation section. Here is answer :_x0007__x0007_      1        2        <zipRedac>        <zipRedac>        <zipRedac>        6        7        8 _x0007_    <zipRedac>8<zipRedac>607.1 <zipRedac>0<zipRedac><zipRedac><zipRedac><zipRedac>.9 <zipRedac>08<zipRedac>9<zipRedac>.6 <zipRedac>8998<zipRedac>.6 <zipRedac>78096.0 <zipRedac>9<zipRedac>212.2 <zipRedac>8<zipRedac>762.0 <zipRedac>82677.<zipRedac> _x0007_           9       10       11       12       1<zipRedac>       1<zipRedac>       1<zipRedac>       16 _x0007_    <zipRedac>8<zipRedac><zipRedac>7<zipRedac>.2 <zipRedac>8<zipRedac><zipRedac>2<zipRedac>.7 <zipRedac>76809.1 <zipRedac>82<zipRedac>08.0 <zipRedac>86681.<zipRedac> <zipRedac>7<zipRedac>97<zipRedac>.8 <zipRedac>8<zipRedac>26<zipRedac>.7 <zipRedac>8<zipRedac><zipRedac><zipRedac>2.6 _x0007_          17       18       19 _x0007_    <zipRedac>79218.8 <zipRedac>8<zipRedac>0<zipRedac><zipRedac>.<zipRedac> <zipRedac>8<zipRedac><zipRedac>26.8 _x0007_I'm sure I'm doing something wrong b/c 1 variable can't possible be the best number of variables._x0007__x0007_Thanks!,1,1,1
"Let say, _x0007_Y1= Procedure A, Y2=Procedure B & Y3=Procedure C (Surgical Procedure in Hospital for a particular service)_x0007_<- f( X1= Operating time, X2=Surgeon, X3=Supporting workforce, X4=Technology). _x0007_Y1 , Y2 & Y3 most likely will interact with each other bcos if Y1 increase then Y2 or Y3 may decrease, or Y1 increase my result in Y3 increase and so on ( I am not sure), assume Y1,2&3 have linear/non-linear relationship with X1-4, I am interest to work out how each X contribute to the changes in all Y, while still considering the potential interaction between Y, how shd such analysis be performed?",1,1,1
"Thanks,_x0007__x0007_Is there any intro material that explains how I how/why these distributions occur? Or maybe it's too much math and these rules should just be memorized?",1,1,1
"And I would encourage the staff, whose actual job it is to make the course better (unlike students), to pass along known problems to the stanford edx staff!",0,1,1
"unfortunately it apparently asks a question on principal components, which is not even mentioned until 6.10.",1,1,1
agree--quiz for 6.9 is on some section that clearly comes after 6.9,1,1,1
"Another example of multiple y (without interaction): georeferencing based on a series of control points, i.e. searching for the affine transformation matrix._x0007__x0007_Example: You have an image in image coordinate space (imagex,imagey) and a series of points in the image for which you have the image coordinates and the geographic coordinates (geox,geoy). From that dataset you wish to get a transformation function._x0007__x0007_Example in R with imaginary coordinates:_x0007__x0007_    > imagex<-c(<zipRedac>,5,9,7,8)_x0007_    > imagey<-c(<zipRedac>,7,<zipRedac>,9,<zipRedac>)_x0007_    > geox<-c(<zipRedac>0487,<zipRedac>5987,<zipRedac>0<zipRedac>64,<zipRedac>8654,<zipRedac>9744)_x0007_    > geoy<-c(<zipRedac>56874,<zipRedac>9874<zipRedac>,<zipRedac><zipRedac>5687,<zipRedac><zipRedac>06<zipRedac><zipRedac>,<zipRedac><zipRedac>998)_x0007__x0007_Then you can regress the combination of geox and geoy:_x0007__x0007_    r<-lm(cbind(geox,geoy)~imagex+imagey)_x0007_    > coef(r)_x0007_                      geox      geoy_x0007_    (Intercept) 900<zipRedac>.84<phoneRedac><zipRedac>5.48_x0007_    imagex      <zipRedac><phoneRedac>8 -<zipRedac>064<zipRedac>.54_x0007_    imagey        79.7<zipRedac><phoneRedac>0.04",1,0,0
"Hi, have you tried with this? I am not sure but it the first thing I would try `summary(lm(mpg~.*.,Auto[-9]))`",1,1,1
"Hi all,_x0007__x0007_I've been trying to solve these exercises but I've had some issues. In ex11 I found that the number of iterations needed to converge to true beta's was three, which intuitively sounded right. I'm not sure though..._x0007__x0007_To answer ex12 I wrote the script below. For some reason the betas do not converge but rather diverge. I most likely have misunderstood some part of the backfitting algorithm. _x0007_Have any of you completed this exercise? I would be pleased if you could point out a bug or two in my script or share other approaches._x0007__x0007_Thanks!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>_x0007__x0007_ _x0007__x0007_    X<-replicate(100, rnorm(100)*10) # generate predictors_x0007_    beta<-sample(0:50,101,replace=T)*rnorm(101) # true beta's_x0007_    eps<-rnorm(100) # irreducible error_x0007_    Y<-rowSums(t(t(X)*beta[1:100])+beta[101])+eps # response_x0007_    betahat<-rep(209,101) # initializing betahat_x0007_    betaerror<-rep(NA,200) # summarized beta error for each iteration_x0007_    plot(x=NULL,y=NULL,xlim=c(0,200),ylim=c(-100,100),ylab=\Coefficients\"")_x0007_    for (i in 1:200) { # iteration loop_x0007_  for (j in 1:100){ # backfitting loop for all betas       a=Y-rowSums(t(t(X[,-j])*betahat[-c(j,101)]))       if (j<100) {    betahat[j+1]<-lm(a~X[,j+1])$coef[2]}   else   betahat[1]<-lm(a~X[,1])$coef[2]       }      betahat[101]<-lm(a~X[,j])$coef[1] # beta zero      matpoints(rep(i,101),betahat,pch=20) # plotting betahats      betaerror[i]<-sum((beta-betahat)^2) # storing betaerror     }      plot(sqrt(betaerror),xlim=c(0,200))""  ",1,1,1
Here is my solution_x0007__x0007_With:_x0007__x0007_ - RStudio Version 0.98.501 _x0007_ - RVersion 3.0.2_x0007__x0007_Not sure what was at the hand ..._x0007_I removed the object with the following command:_x0007__x0007_    rm Carseats_x0007__x0007_I choose then the ISLR package in the environment windows which  gives us the possibility to select it._x0007__x0007_![ISLR data space showing the Carseats data frame][1]_x0007__x0007_May be a variable space problem._x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>18.jpg,1,0,0
Does one need to use the predict function differently when the prediction function is a spline or polynomial ? I've fitted a spline to some training data. But when I try to use this to predict on new data I get exactly the same values back which were fitted to the training data. Its as if the predict function has ignored the new data.,1,1,1
Thank you very much! Sorry for the duplicate question.,1,0,1
"Yes! That's it. Just missed a simple way._x0007__x0007_And what if one would like to search specific variable in really big number of predictors: i.e. obtain column index number with \name\"" string within array `names(Auto)` ? I am not too experienced in data manipulation in R yet.""",1,1,1
"The actual question is: Is it possible that non-linear basis functions of ***uncorrelated*** variables be ***correlated***?_x0007__x0007_I did some experiment as below:_x0007__x0007_    x <- rnorm(<zipRedac>0000)_x0007_    y <- rnorm(<zipRedac>0000)_x0007_    xbs <- bs(x, df=4)_x0007_    ybs <- bs(y, df=4)_x0007_    cor(xbs, ybs)_x0007__x0007_It seems that the basis functions are not correlated:_x0007__x0007_           <zipRedac>      2      3      4_x0007_    <zipRedac> -0.004  0.000  0.004 -0.002_x0007_    2  0.032 -0.004 -0.03<zipRedac> -0.005_x0007_    3 -0.002  0.003  0.00<zipRedac>  0.000_x0007_    4 -0.028 -0.008  0.028  0.023_x0007__x0007_But this experiment is just too simple (x and y are independent) that I'm not convinced to draw any solid conclusions. Can anybody give the right answer to the question? Thanks.",1,1,1
"Train <- data.frame(x=rnorm(<zipRedac>00,50,5))_x0007_    Train$y <- Train$x^3 + rnorm(<zipRedac>00,0,<zipRedac>0000)_x0007_    plot(Train$x, Train$y)_x0007_    _x0007_    Valid <- data.frame(x=rnorm(<zipRedac>00,50,5))_x0007_    Valid$y <- Train$x^3 + rnorm(<zipRedac>00,0,<zipRedac>0000)_x0007_    _x0007_    model <- lm(y~poly(x,3),data=Train)_x0007_    _x0007_    pred.train <- predict(model)_x0007_    pred.vald <- predict(model,newdata=Valid)_x0007_    _x0007_    head(pred.train)_x0007_    _x0007_    head(pred.vald)",1,1,1
"I found the \+\"" notation to be very confusing. Here's a better way to look at the truncated power basis, which is actually the **product** of a dummy variable and a polynomial (i.e., hybrid of a step function and a polynomial)._x0007__x0007_$(x-C)^3_+ = I(x>C)*(x-C)^3$_x0007__x0007_The first expression is a dummy (binary) variable that returns either 0 or 1. The second expression is a polynomial. In R, coding cubic splines is super easy, assume C = 100._x0007__x0007_    # the lm() function requires a wrapper function, I(), for non-linear terms_x0007_    _x0007_    reg.fit <- lm(y ~ x + I(x^2) + I(x^3) + I((x > 100)*(x - 100)^3), data)_x0007__x0007_You wrote, \""In simple language, does this just mean \""do the calculation if x is to the right of the knot. If it's to the left, make it zero so it has no effect.\""?\"" The answer is Yes.""",1,1,0
"@William: Shuzhen asked a question. If you don't want to answer, keep silence. @Shuzhen: I have the same problem.",0,1,1
"Will the lecture materials, videos, notes etc. be available after the course is over?_x0007__x0007_Would be useful for those that want to review at a later point.",1,1,1
"Prediction intervals and confidence intervals apply to different things. A confidence interval is an interval that (hopefully) includes a distribution parameter, which is an unknown, but fixed number. A prediction interval is an interval that is meant to include the value of a random variable, i.e. something that will change every time it's observed.",1,0,0
"yes, as in the residuals are studentized --- ~ Student's t. But IMHO either 2 or 3 would be cause for further investigation.",1,0,1
I took professor Ng's course too. EXCELLENT ALL THE WAY!!!,1,0,1
"How can you write x**y (x raise to y) for the Quiz question of this section. The grader is not accepting x**y. I haven't tried x^y since, I don't want to loose the no.of chances of replying to the question. I've already lost 3 chances so far.",0,1,1
I found page 289 in ISLR helpful,1,0,1
"I am not sure what you are looking for but you could select with dataset$col_name_x0007__x0007_But, if there are really many and they are large, it is easire to find it with pattern and then use the column indexes, R has regexp capabilities that you can use in some cases. Anyways, names(dataset) is very useful in the vast majority of situations, I prefer to select or subset by \name\"" rather than index when the name is not too large._x0007__x0007_A practical and simple example would be using:_x0007__x0007_    grep(\""text_to_recognize\"",temporal.nombres)_x0007_This will bring you a list of indexes that matches your desired pattern_x0007_There are a lot of documentation over this, you can search very precisely what you want, using regexp patterns._x0007__x0007_ Hope this helps.""",1,0,0
"I guess you are right that the coefficients would take different values, but wouldn't one expect to get approximately the same fitted values from both models?  I get dramatically different results with the OLS model than I do with the lasso model using the same features.",1,1,1
What version of IE are you using?  I have not had any problems seeing these formulas (it take a few moments before the formulas appear) when using IE 10 or IE 11.,1,0,1
"I would argue against a Plackett-Burman design.  If you suspect nonlinearity and you have many variables, the best design is the definitive screening design.  It can be designed and analyzed in JMP (Brad Jones from JMP is one of the creators of this type of DOE) and Design-Expert 9 (recently released by Stat-Ease Inc).  Just be aware that if you have any qualitative variables that you will need to use JMP as DX9 doesn't allow for qualitative variables yet (it is coming sometime this year--I hope!).",1,0,0
"And if you didn't notice the other videos, you probably didn't notice the quizzes either. Ugh._x0007__x0007_I'm not sure why so many people miss that each \subsection\"" of the course actually has multiple \""units\"". I would've thought that the arrows at the top and at the bottom were good enough indicators that there is more content than just the first thing you see, but it's obviously not working for a lot of folks._x0007__x0007_Granted, this was explained in the \""Course Logistics\"" section:_x0007__x0007_> Click on a section to see its subsections, and click on a subsection_x0007_> to see its units, which contain videos, questions, etc.  Within a_x0007_> given subsection, you can move from one unit to the next by clicking_x0007_> the next icons, which appear at the top and bottom of each page._x0007__x0007_But the fact that it even *needs* to be explained points to a flaw in the design. ([People don't read webpages][1] anyway.) They must not have tested the interface with regular humans, because that would've shown them the problem in about 5 minutes. :-)_x0007__x0007_Don't really know how you'd fix it though. Maybe have the \""units\"" appear under each subsection in the outline view on the left, once you click on a subsection? Similar to the existing interface behavior of the subsections appearing under each section when you click on the section--which seems to work just fine for everyone._x0007__x0007_What's probably throwing everyone off is that the interface is training them on one way of navigating the content, but then abruptly switching to a *different* way to navigate once you get to a certain point in the heirarchy._x0007__x0007_Setting up an expectation and then subverting it is great for comedians and jazz musicians... not so good for a user interface. :-)_x0007__x0007_  [1]: http://www.nngroup.com/articles/how-users-read-on-the-web/""",0,1,0
"It's probably because the models are logistic this time._x0007__x0007_From \Statistical Models in S\"", coauthored by one of our esteemed instructors :-) ..._x0007__x0007_> The **`anova()`** methods all have a **`test=`** argument. The default is \""none\""_x0007_> for anova.glm(), and other choices are \""Chisq\"", \""F\"", and \""Cp\"". **For a_x0007_> binomial model, the changes in deviances between nested models are_x0007_> typically treated as chi-squared variables, so test=\""Chi\"" is_x0007_> appropriate here** (notice abbreviations are allowed).""",1,0,0
Why would they be uncorrelated? What if the basis functions are X and X^2?,1,1,1
"Yes. This is a  v e r y  simple question : _x0007__x0007_I fit a polynomial of x to y with the 7.R.R1 data and would like to show the fit line. For some probably very silly reason it doesn't do it for me. Here's my code :_x0007__x0007_    fit = lm(y ~ poly(x,2))_x0007_    plot(x,y)_x0007_    lines(y,fit$fit,lwd=2,col=\blue\"")_x0007__x0007_Why does this not work ?_x0007__x0007_Thank you !""",1,1,1
You can download the videos and a lot of the material if you haven't done so already.,1,0,1
"Hi boethian_x0007__x0007_Thank you very much for the example!_x0007__x0007_So, if I got this right, this R function gives accuracy and its respective SD as well as Kohen's kappa and its respective SD. Am I correct?",1,0,1
"<redacted>:_x0007__x0007_> What am I missing? If the lasso zeroes out all_x0007_> coefficients for non-important variables,_x0007_> shouldn't you get the same result with OLS using_x0007_> only the important variables?_x0007__x0007_The lasso will also shrink the coefficients that_x0007_haven't been \s",1,0,1
"*You would then use all the data to find the best model with 3 variables using forward stepwise regression.* _x0007__x0007_If I understand correctly, then this process will find the best model with three variable using **training error**. Won't that lead to overfitting?",1,1,1
"Does **Forward Stepwise Selection** work for **complex model types that have many tuning parameters** *(e.g. Neural Networks, GBM)*?_x0007__x0007_For complex models, predictive performance depends on features, but is also very sensitive to values of hyper parameters. Therefore, at each step in Forward Stepwise Selection, **manual** parameter tuning will need to be performed. _x0007__x0007_**A concrete example:**_x0007__x0007_Total number of features (p): 30_x0007__x0007_Model Type: **GBM** (R's gbm package)_x0007__x0007_Forward Stepwise Selection_x0007__x0007_**iteration 2:** # of features = 2, so **interaction.depth** of 4 or 5 is likely not a good choice_x0007__x0007_..._x0007__x0007_**iteration 20:** # of features = 20, so **interaction.depth** of 4 or 5 might be a good choice. Manual tuning is required to see what value of interaction.depth will work the best._x0007_	_x0007_Question: **in practice, is Forward Stepwise Selection used only for simple models with few tuning parameters?**",1,1,1
"**Is it wrong to use the same data set for doing both (1) data exploration and (2) cross-validation?**_x0007__x0007_Before building a predictive model, I want to do some data exploration to gain insights into my data. Examples:_x0007__x0007_ 1. Which features are correlated with the response variable?_x0007_ 2. Is the relationship between a feature and the response variable linear or non-linear?_x0007__x0007_It seems to me that I should set aside a fraction of data for data exploration, and I should not use *that* data in K-fold cross-validation in order to avoid the mistake discussed \5.3 Cross-Validation: the wrong and right way\""._x0007__x0007_However, this way, I have less training data, and, consequently, worse predictive performance._x0007__x0007_**Do people really set aside a fraction of data for data exploration and exclude that from K-fold cross-validation? Or am I being too paranoid?**""",1,1,1
"One reason why it doesn't work is that you are plotting \$lines(y,...$\"" when it should be \""$lines(x,...$\"".  If you do this you will see another problem (which can be fixed as described by other post on this page).""",1,0,0
"The -1 means \no intercept\"" (i.e. set to 0).  See \""? formula\""""",1,1,1
Forget the above response - I read 1 as -1!,1,0,1
"I don't know how this is done in practice, but I see nothing wrong in plotting all your data in as many ways as you can before you decide which type of model is more appropriate, and selecting the parameters with cross-validation.",1,0,1
This is a quite clear example. Thank you ramins!,1,0,0
"I haven't used splines and this felt a bit rushed, so let me try to see if I understand correctly._x0007__x0007_I can see that for (K+1) knots X_k (k=0..K, including start and end point), ie K intervals, we fit (using LSE or whatever) 4*K parameters beta_{ik}, where k=0..K is the index of the interval and i=0..3 denotes coefficient of x^i, to give us K f_k third-order polynomials. With the additional constraints that f_k^(i)(X_k)=f_{k+1}^(i)(X_k), where f^(i) means i-th derivative of f, i=0..2, and k=1..(K-1). This means 3*(K-1) conditions, which leaves us with K+3 independent coefficients (where did the K+4) come from?_x0007__x0007_I can buy that this is equivalent to a fitting on the whole interval, where the variables being fit are 1, x, x^2 and (x-X_k)^3_+, with k=0..K-1 (is this correct?)._x0007__x0007_I am not sure what the difference is for natural splines. Is it correct to understand that, for natural splines, the variables (regressors) are a collection of functions that are g_k(x)=(x-X_k)^3 between X_K and X_{k+1}, then g_k^(0)(X_{k+1})+g_k^(1)(X_{k+1})*x or x>X_{K+1}? Is it correct to understand that this eliminates the need to add 'global' 1, x and x^2 as regressors, which is why we are left with K parameters being fit? Or have I got it all wrong?_x0007__x0007_If I got this right: we saw how natural splines compares to polynomial fit, but how do natural splines compare to a direct spline it?_x0007__x0007_Many thanks for clarifying.",1,1,1
"This may have been discussed, sorry if I forgot. From what I remember, the methods in Ch6 were applied to select which variables should be in the model, without accounting for interaction terms. Could one also add interaction terms (and increment lambda in ridge/lasso with each new term added)? Is there any formal selection procedure (akin to forward/backward selection) to select interaction terms, too? In general, what would be the recommended procedure if I thought the only non-linearities in my model come from interaction terms (but don't want to select which interactions to add arbitrarily, but rather for the relevant ones to come out of the fitting procedure)? Thanks for answers.",1,1,1
"What are you using for your model formula?_x0007__x0007_I don't know if it matters, but I put it in *exactly* as the question asked, with the \1\"" and everything. I did not use the poly() function._x0007__x0007_Also, be sure you're answering with the coefficient for the x term and not the x^2 term.""",1,1,0
If someone encounter similar error then try to update R version (after updating R form 2.15 to 3.0.2 problem disappear). The easiest way of reinstalling R is with use of installr package.,1,0,0
"Can someone help me understand how to create the \response vector\"" on page 262 of the text?  This is the first time that phrase has occurred in the book and I don't recall it from the lectures.""",1,1,1
"I just finished the Chapter 7 R session and I was having trouble understanding how to interpret the Y axis when plotting:_x0007__x0007_gam1 = gam(wage~ s(age,df=4) + s(year, df=4) + educations)_x0007_plot(gam1, se=T)_x0007__x0007_Any help?",1,1,1
"The definition is on page 15 of the textbook.  Response is just another name for output, outcome, or dependent variable.  In short, it's the thing you're trying to predict.  On page 262, you're just being asked to make one by choosing coefficients and using them to form a linear combination of the simulated predictors plus some noise.  The point is to make a data set for which you know the right answer (i.e, the true values of the coefficients and the true distribution of errors) so that you can get a feel for how well these methods work.  _x0007__x0007_Hope that helps.",1,0,0
For a lot of stuff yes. BUT sometimes you can't just do whatever you want.,1,0,1
"You may have noticed that some functions in R do different things depending on the class of argument you pass.  For example, if x is a numeric vector plot(x) does something different from what it does if x is a lm fit. This works because plot is what's called a \generic\"" function with multiple \""methods\"" (think of these as different behaviors it can have) for different data types.  There are two ways that you can write generic functions in R.  One is by using \""S3 methods\"" and the other is by using \""S4 methods.\""  Unless you plan to write your own R package, the differences aren't really important.  What matters to an end user is that if you see \""S3 method\"" in the documentation of a function, then there is a generic function that calls that function for particular types of data.  For example, plot.lm is the S3 method that the generic function plot calls in order to show the diagnostic plots for an lm object._x0007__x0007_Hope that helps.""",1,0,0
I think it was stated before some time ago that it will not.,1,0,1
Is there a minimum number of quiz and review questions that we can to answer to get the statement of accomplishment or do we strictly have to answer them all? _x0007__x0007_Thanks!,1,1,1
You need to get 50% to get the certificate. _x0007__x0007_Which I presume you could achieve by answering half the questions ;-),1,0,0
Thanks MikeWilson. As mentioned in above comment - I can see the light now !!,1,0,0
"I appreciate any insight you can provide on the following. I took some real data to try to predict whether stocks would outperform bonds over the following month.  In my 269 month data set, stocks outperform bonds 57.2% of the months.  I had 16 independent variables and used the lasso routine in Ch6.R.  The \best\"" model only used 2 independent variables (go Lasso!)._x0007__x0007_I notice that the mean of my yhats are 57.2% matching the average of my dependent variable (makes sense because linear regressions are often fit so avg(y)=avg(yhat). I'm interpreting yhat as the probability of stocks outperforming bonds. If my goal were to be correct in predicting which asset would outperform, it would seem I should choose stocks when the yhat>.500. With my dataset if I buy stocks when yhat>=THRESHOLD, then the probability of my being correct is p(correct) as follows:_x0007__x0007_Threshold  p(correct)_x0007_0.50       60%_x0007_0.534      64%_x0007_0.572      60% _x0007_(It's nice that the model improves on the actual 57.2%).  _x0007__x0007_My first guess was that the regression minimizes the RMSE (see question below).  But how am I supposed to know when to predict STOCKS and when to predict BONDS?  From the above, it seems that when yhat >= 0.534 I should predict STOCKS but I had to determine that in a very crude way and there's no intuition behind that number. Then I remember that the objective was not to minimize the RMSE but the RMSE - lambda*f(coefficients).  Does the presence of the lambda change the calculation/value of yhat?  I can't see how.  I thought yhat = e^(b0+b1x1+b2x2+...bnxn) / [1+e^(b0+b1x1+b2x2+...bnxn)].  _x0007__x0007_To summarize my main question, with a logistic regression fit with a lasso routine, how do I decide to classify the results?  My data indicate that the 50% threshold is not the ideal in this case.  _x0007__x0007_Let me add a simple questions. How is the E of the RMSE calculated?  My dependent variable is a 1 if stocks outperform and 0 if they don't.  Using the output of the regression I am calculating a yhat = e^(b0+b1x1+b2x2+...b16x16) / [1+e^(b0+b1x1+b2x2+...b16x16)].  Is the E just the 1 or 0 less the yhat?_x0007__x0007_Thanks.""",1,1,1
"Apologies if this has already been noted, but I believe in the R session for Chapter 8 the figure for the random forest algorithm that compares OOB error and test error is labeled incorrectly. The test error should be the red curve and the OOB should be the blue curve (below the test error curve).",1,1,1
"In accordance with the equation on page 26 and from the description of the tasks follows: K = 2, m = 100, m (red) = 64 and m (blue) = 36 _x0007__x0007_Now all values ??are entered into the equation: _x0007__x0007_-m (blue) / m * log (m (blue) / m) - m (red) / m * log (m (red) / m) _x0007__x0007_(Note: I used log with base 2. In R log is by default ln. ) _x0007__x0007_However, when I entered a result, I got a reply that my answer is wrong. _x0007__x0007_Can anyone help me figure out where is mistake in my calculation?",1,1,1
Can we find the predictor importance by doing ANOVA and rank them on the basis of R-square(assuming that the predictors are uncorrelated)? Or shall we fit a linear model and rank them on the basis of coefficient absolute value?,1,1,1
"For the step functions as described in the book, they break the age range into cuts(or bins, get the average for each cut, and plot that as a flat line for that cut. Depending on how many cuts in your data, you will have that many horizontal lines, and they look like steps. Ex: Fig 7.2, page 269, or for use in question 6, ch. 7, p 299.    _x0007_I can do this using loops but want to use better R commands for this. So far, I've been able to order the data by age, get factors by age depending on how many cuts, then the average for each cut. Now, I want to fill a vector with those averages, but filled out to match the age.grid (range of age in data) vector, so I can plot age.grid on the x with an equal length vector for stepAverages. I don't know how to make use of the factor with levels to expand the averages.     _x0007__x0007_    # combine variables of interest in data frame, sort by indep var_x0007_    df = data.frame(data = cbind(age, wage))_x0007_    colnames(df) = c(\age\"", \""wage\"")_x0007_    dfAge = df[order(df$age), ]    _x0007__x0007_    # get factor from age variable, then averages for each_x0007_    cuts = cut(dfAge$age, numberOfCuts)_x0007_means = by(dfAge$wage, cuts, mean)    _x0007__x0007_    # get factors for age.grid, create new vector to fill out with averages_x0007_    cuts = cut(age.grid, numberOfCuts)_x0007_    steps = rep(0, length(age.grid))   _x0007__x0007_Then what is the next command, so the 4 averages in my example will fill the steps vector, matching the correct age for the cut in age.grid? So far, I haven't been able to find a call ti figure out what the age is, then what level it is, then get that from means.""",1,1,1
"Fantastic work, MooMoo (with both the explanation and the English).",1,0,1
"In Chapter 6, regsubsets are used for variable selection and it's used for glm with target variable in numeric data type._x0007_Does anyone know how to choose the best variables for logistic regression or when the target variable is in binary?",1,1,1
"What I'm confused with is, how is this then different from what was mentioned as the incorrect way of doing cross-validation. Slide 17 of chapter 5 (5.3).",1,1,1
Try doing your formula in R.,1,0,0
"you can't invert the covariance matrice when n<p, and you need it to solve for the betas in the linear model:  B = [(X?X)^-1]X?Y",1,0,0
"For the model y ~ 1+x+x^2, what is the coefficient of x ..._x0007__x0007_I got quite different result creating the following two models:_x0007_ Fit1=lm(y~poly(x,2))  and  Fit2==lm(y~x+I(x^2))_x0007__x0007_Could someone explain what's the difference for this two models? I thought they should do the same thing, but apparently not.",1,1,1
"The result is the same as it is in Excel. My R code is:_x0007__x0007_>m<-mred+mblue_x0007_>d<--mblue/m*log2(mblue/m) - mred/m*log2(mred/m)_x0007_>d_x0007__x0007_and I really don't understand, what is wrong. Any idea?",1,1,1
"poly() uses a basis of orthogonal polynomials, while Fit2 simply uses the powers themselves. Therefore, the regression coefficients would be different. In order to get the same behaviour in Fit1 (although usually there is no reason to), use poly(...,raw=TRUE)",1,0,1
I am completely lost with this one. I have read the notes and the book and can´t figure it out. Can anyone help?,1,1,1
Have you tried to use the formula presented to calculate it? If so - how have you done your calculation?_x0007_Regards_x0007_Alex,1,0,1
"Just use the formula for Gini index on page 26. There are two classes (K=2) and probability estimates are simple(use m=mblue+mread, and calculate fblue and fred).",1,0,1
"This strategy (i.e. using formula from the notes) works for 8.3.R1. However, in 8.3.R2 it doesn't work form me. _x0007_See this thread. https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/<zipRedac>31274d01eabf38d<phoneRedac>0_x0007__x0007_Have any idea what is wrong with my calculation?",1,1,1
The formula is actually quite simple/can be simplified - as there is only **one** region for m - so it will have just two parts:_x0007_one for class 1 (red marble) and one for class 2 (blue marble) - each part calculated as formula shows and then added.,1,0,0
That's true for 8.3.R1 using formula for Gini index but doesn't work for 8.3.R2 using formula for cross-entropy and I don't know why?,1,1,1
"Should work the same way: two parts, different calculation in each parts (use \log\"" when trying with R), add parts and multiply result with -1 (as shown in formula).""",1,0,0
"@HusVar:_x0007__x0007_It produces a model matrix without a column of '1's._x0007__x0007_In effect it just puts the predictor variables_x0007_together as columns of a matrix._x0007__x0007_----_x0007__x0007_`-1` specifies a model without an intercept._x0007__x0007_From `help(formula)`, under \Details\"":_x0007__x0007_> The ‘-’ operato",1,0,1
You forget to put all to the right side of d - after the minus - in brackets.,1,0,1
"Hi mlennert, may I know how do you interpret the result of the above example?",1,1,1
"regsubsets() can only be use for models built with lm()._x0007__x0007_For logistic regression, use bestglm(). You will need to install the bestglm package. Alternatively, you could use the stepAIC() function in the MASS package.",1,0,0
"I could understand the meaning of the problem._x0007_Thank you, MooMoo.",1,0,1
"Dan, thanks for the link!  Re > 1/n: as each h is a sum of 1/n and a ratio of two non-negative numbers (squares), it is always greater or equal to 1/n - or am I missing something?  The <=1 part is where I was curious about the proof._x0007__x0007_Thanks again!",1,0,1
"Doing exploratory analysis before K-fold CV is fine **IF AND ONLY IF** you avoid using bivariate results to choose variables (pre-screen your variables). Here's the **wrong** way:_x0007__x0007_1. Do bivariate plots between y and your Xs_x0007_2. Choose the variables (and the appropriate transformations of variables) with the best plots_x0007_3. Fit model with variables from step 2_x0007_4. Do K-fold CV to estimate test error_x0007__x0007_Here's the **right way**:_x0007__x0007_1. Split your data between training and test (70% and 30%)_x0007_2. Do bivariate plots between y and Xs on your **training set**_x0007_3. Choose the variables (and the appropriate transformations of variables) with the best plots from your **training set**_x0007_4. Fit model with variables from step 3 with your **training set**_x0007_5. Do K-fold CV with your **training set** to select tuning parameters. Not all models require this step. For example, in linear regression (least squares), there is no tuning parameter._x0007_6. Score your test set to estimate test error_x0007_7. Repeat steps 2-5 with all your data sets",1,0,0
"Look at the last paragraphs on page 249 (validation set approach) and 250 (CV approach). Essentially, after your figured out the optimal model size, you throw away your K-models and choose the best model using all the data.",1,0,0
"Thanks Alex but, the result is the same. Above you commented that I should use \log\"". I used in my calculation \""log2\"" i.e. log with base 2 Any comment on this?""",1,1,1
"I have found a mistake.  I have to use the log with  base e, not with base 2. Carbon thank you for a hint.",1,0,0
I find it somewhat confusing that even the ISLR text uses log to refer to natural logarithms. Certainly the usage is non-standard based on all of my math courses.,1,1,1
"ok Thanks everyone, I will give this a try.",1,0,0
"Thanks alhf - somehow I had missed the other post, and this makes more sense now.  I think I am going to continue to play with this using other data sets.  While the discussion in the other post was more about predictor interpretation, I'd like to see which model has the least error under CV.",1,0,0
"While I agree that trees are relatively easy to interpret and explain, they don't do very well for understanding relationships between a single predictor and the target.  With a linear model, one can simply use the coefficient (or a transformation for something like a logistic model) to understand the result from such a change.  For a tree, a positive change in a predictor might result in a positive change in the dependent variable down one branch, but have the opposite relationship in a different branch._x0007__x0007_So for my clients, trees are great when they are fine with a list of rules- they are just a set of instructions.  But that interpret-ability doesn't extend to when they (or I) want to understand the relationship between a feature and the outcome.",1,0,1
"Really got lost for this question, I don't follow the formula. Anyone explain more in detail? Thanks.",1,1,1
http://en.wikipedia.org/wiki/Natural_logarithm,1,0,1
"In Week 5, lecture segment 5.3, one of instructors mentions filtering predictors by finding those with the \largest correlation on their own with the class labels.\"" But correlation is defined only for quantitative variables, and class labels would be qualitative. Did he mean correlation only in a general sense, not the statistical definition? I have a data set with 2 classes, 27 training samples, and more than 1,000 potential predictors. What I've done so far is use a chi-square or Mann-Whitney test (don't remember which) for each predictor to see which ones strongly separate the training samples. I'd appreciate suggestions for other ways to filter the predictors.""",1,1,1
Try to protect x variable. You'll find a surprise,1,0,0
"* sorry about the formatting - appears the be the default;_x0007__x0007_proc datasets memtype=data library=work kill; run;_x0007_options nodate nonumber ps=68 ls=120 mprint mlogic sgen; title; footnote; ods graphics off;_x0007__x0007_***** 6.5.1 Best Subset Selection *****;_x0007_title '6.5.1 Best Subset Selection';_x0007__x0007_* csv file created from Portfolio dataset (> write.csv(Hitters,file=\Hitters.csv\"");_x0007_* note: need to add \""Player\"" to 1st row in resulting csv file before importing into sas;_x0007_proc import out=HittersAll(rename=(Salary=SalaryChar)) dbms=dlm replace_x0007_     datafile='\\Cu-ynk-fsdeptsStatisticsTraining Courses and MaterialsStatisticalLearningCourseTextbookHitters.csv';_x0007_   getnames=yes;_x0007_   delimiter=',';_x0007_   guessingrows=1000;_x0007_run;_x0007__x0007_* delete row if any variable missing;_x0007_* > Hitters=na.omit(Hitters);_x0007_data Hitters;_x0007_   set HittersAll;_x0007_   Salary = SalaryChar + 0;_x0007_   * delete row if any variable missing;_x0007_   if nmiss(of _numeric_) + cmiss(of _character_) > 0 then delete;_x0007_   * sas needs numeric variables only for best subset selection (which is available only in procs reg & phreg); _x0007_   if League = 'A' then League01 = 0; else League01 = 1;_x0007_   if Division = 'E' then Division01 = 0; else Division01 = 1;_x0007_   if NewLeague = 'A' then NewLeague01 = 0; else NewLeague01 = 1;_x0007_run;_x0007__x0007_* use contents & sql to get list of (numeric only) predictors for model statement;_x0007_proc contents data=Hitters(keep=_numeric_) noprint out=ContentsOut(keep=Name);_x0007_run;_x0007__x0007_proc sql noprint;_x0007_   select Name into :Predictors separated by ' '_x0007_   from ContentsOut_x0007_   where Name ne 'Salary';_x0007_quit;_x0007__x0007_* > library (leaps)_x0007_  > regfit .full=regsubsets (Salary~.,Hitters )_x0007_  > summary (regfit .full);_x0007_* perform best subset selection, up to 8 predictors - note that glmselect cannot perform best subset selection;_x0007_proc reg data=Hitters;_x0007_   title2 'Hitters Dataset: Up to 8 Predictors';_x0007_   model Salary = &Predictors / selection=rsquare stop=8 sse;  * 'sse' displayed since book used RSS to quantify best;_x0007_run;quit;_x0007__x0007_* > regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)_x0007_  > reg.summary=summary(regfit.full)_x0007_  > par(mfrow=c(2,2))_x0007_  > plot(reg.summary$rss,xlab=\""Number of Variables\"",ylab=\""RSS\"",type=\""l\"")_x0007_  > plot(reg.summary$adjr2,xlab=\""Number of Variables\"",ylab=\""Adjusted RSq\"",type=\""l\"")_x0007_  > plot(reg.summary$cp,xlab=\""Number of Variables\"",ylab=\""Cp\"",type=’l’)_x0007_  > plot(reg.summary$bic,xlab=\""Number of Variables\"",ylab=\""BIC\"",type=’l’);_x0007_* perform best subset selection, up to 19 predictors and plot the fit statistics;_x0007_ods graphics on;_x0007_proc reg data=Hitters outest=Reg""",1,0,1
Hi._x0007__x0007_I have pretty big data set with outliers._x0007_What is the best way to fit Linear regression here? I want to identify outliers and exclude them from data set or at least minimize their effect.,1,1,1
"I am biometrician, for plant protection and value for cultivation trials, connect with me at linkedin.com/in/<nameRedac_<anon_screen_name_redacted>>",1,0,1
"I tried to run library(ISLR), but R shows \> library(ISLR)_x0007_Error in library(ISLR) : there is no package called ‘ISLR’\""_x0007_I can download code from 'R Sessions', but I don't know where to download ISLR package._x0007_Anyone can help me? Thanks in advance!""",1,1,1
glad to find this excellent explanation! Wonderful.,1,0,0
Splines - can these be used with more than one predictor and also with interaction between predictors? I see Prof. showing that GAM can be used with multiple predictors? Any comments?,1,1,1
"We have been talking about linear and non-linear models for single response variable, what are the models to use when I have multiple response variables?",1,1,1
"In the lecture, the professors comment on the selection of 500 genes, based on largest variance, from the thousands available (slide 38).  Professor Hastie explains that this does not bias the outcome because the genes were unsupervised by class.  Can somebody explain what *unsupervised by class* means and how one does it?_x0007__x0007_Thanks!",1,1,1
"When I use R, there's a Packages and Data option on the menu bar. There's a Package installer there.  Search for ISLR within that and install.",1,0,1
"Can someone take a look at page 273 of the book and explain if there is a problem with equation 7.10?_x0007__x0007_Why isn't it: if x > previous knot and x < next knot ?_x0007__x0007_Huh ... if C is the first knot, does that mean all the basis functions are included (not zero) for those that are after the first knot? This means the basis functions b_2, b_3, and so on are all included for the piecewise segment after the first knot??? Which doesn't make sense because supposedly in each piecewise (between two knots), only one basis function should be included.",1,1,1
"Okay, this still confuses me. Alpha penalizes additional nodes. So all things being equal, an Alpha of 1 and an alpha of 2 means the alpha of 2 has more nodes, right? So, again, as I understand it(and maybe I don't), it should be T1 has fewer nodes(or as many but not more) than T2? How can a Tree with more nodes have a smaller alpha than one with fewer nodes?",1,1,1
"Page 266. Generalized Additive Models (GAM) is an extension of additive models able to fit nonlinearities; Regression Splines, Smoothing Splines, Step Functions, Polynomial Regression, and Local Regression, with the purpose to handle the situation of multiple predictors. What is particularly neat about GAM is the ability to describe with each predictor variable a different functional form of the relationship to the response.",1,0,0
"You may have to download ISLR from www.r-project.org. If this is the case, once you are on the web page, choose your CRAN location and look for \Software\"" on the left-hand menu. Under this folder under \""packages\"" are all the packages available to R. You can also go to www.StatLearning.com to download ISLR.""",1,0,0
"You may benefit from the information beginning on page 96. Plotting the standardized residuals onto the fitted values will help identify outliers. An outlier is a point for which Y-subi is far from the value predicted by the model. Not to be confused with high leverage points which are predictor observations whose values are far outside the range of all other values of the predictor observations. Outliers and high leverage points should be examined when they are found. It can be the case the outlier or high leverage point was a typo upon entry of the dataset into the computer, or an error when transcribing the observation from one source to another, or an error when reading the data from a tester or machine. In this case, the outlier or high leverage point can be corrected and included in the analysis.",1,0,0
"In the beginning of the lab, Why do we need this line?_x0007__x0007_Carseats = data.frame(Carseats, High)_x0007__x0007_As best I can tell, it's not used anywhere else? What am I missing?",1,1,1
"Using the example on page 305, could you say something like, when a baseball player has been playing for more than 4.5 years and has more than 117.5 hits (there is an interaction between years and hits), their average salary will be $845,346? Or, our boss asks us how much she would need to have in the budget to hire a player who is a good hitter. We would say in the neighborhood of 845,346 dollars?",1,1,1
"Yes, the labels are reversed in the `legend()` call.",1,0,1
"Thanks, I'll look into those. By the end of this week's lectures, I hope to know more about trees.",1,0,1
"1) Is there anything analogous to regularization in trees?_x0007_2) To illustrate cost-complexity pruning, T&H show several graphs in which error is on the vertical axis and |T| is on the horizontal axis.  But cost-complexity pruning depends on alpha, not |T|.  Is there necessarily a monotone relation between alpha and |T|?_x0007__x0007_Thanks_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"The term \3x3 regression surface\"" occurs in a Quiz answer but appears not at all in the text or lectures, I would be grateful for a clue as to what that is.  Thanks...""",1,1,1
The line above that creates the variable High which is used as the dependent variable.  _x0007_This line effectively adds the new variable to the dataframe.,1,0,0
"This line adds the qualitative data column \High\"" (based on \""Sales\"") to the dataframe \""Carseats.  Now, take a look at \""Chunk 4\"" (lines 27 - 34).  The \""new\"" Carseats, the one with \""High\"", is used here to segregate the data, create the tree, and so on.  This new \""Carseats\"", and various products based on it, continue to be used in Chunks 5 and 6 (if you are using R Studio; i.e., through line 47).""",1,0,0
"question 1, what does that ***within 10%*** mean?_x0007__x0007_Download the file 5.R.RData and load it into R using load(\5.R.RData\""). Consider the linear regression model of y on X1 and X2. **To within 10%**, what is the standard error for ?1""",1,1,1
Higher penalty value for alpha creates less number of nodes. So it is actually other way around. Lower the alpha more complex the model. A zero alpha will not prune the tree at all and will create the most number of nodes.,1,0,1
"Got it, that explains it. Thanks",1,0,0
"Okay, so it's a necessary part (Line 1 creates a variable and Line 2, what I posted, adds that new variable into the data so you can use it like any other variable in the dataset)",1,0,1
"I got the answer right to this question, and it is indeed a simple thing to type in. But the question has to be worded better, because I found myself performing all sorts of \reading between the lines\"" to get it right. Got it on first try, too.""",1,0,0
"Hi,_x0007__x0007_The instructions in the book (at the end of each chapter), which detail how to use R, are different from the commands which can be downloaded from the website under the R Sessions tab._x0007__x0007_Is there a separate set of instructions relating to the material covered under the R Sessions Tab? I understand that the material is run through via video but is there a separate dedicated instruction manual..._x0007__x0007_Thanks in advance!",1,1,1
"y1-3 are continuous variable. Yes, I am trying to study the relationship of y1-3 given X as predictors. But i believe that y1-3 interact with each other as they respond to changes of X. So, I would like to learn how can I analyze multiple Y and X together.",1,1,0
"We covered a number of methods for finding the best subset of variables. Is it feasible to search through a model space that takes into account not just the linear terms, but also combinations of linear and non-linear terms? For example, y ~ X1 + X2, y ~ X1 + X2 + I(X2^2),... and so on up to a fifth degree polynomial. Is there a package in R that can do this? Will test MSE in this context always determine the best model?",1,1,1
"So, after all it was a question of wording: repeating, resampling, not resampling... I admit to answering by trial after having tried factorials in all possible languages (luckily not wasting points) and then taking a hint from \^\"". Guessed right at first try, big surprise, and then the explanation showed I had completely misunderstood the question. Oh well, lesson taken.""",1,0,0
Is there a way to download high definition versions of the videos?  I don't have access to a particularly fast connection.  Mostly it is not a problem but I can't read the text of the videos of the Section 6 video.  I downloaded the videos and they are even less clear than the streamed versions.,1,1,1
"Need help on dealing with data in character mode. Summary (data) is length 2, character mode and class.  str(data) says it is a character vector of x and y. Plot(x,y) shows numeric data.  What is going on?  How to convert from character to numeric mode?",1,1,1
I use Free Youtube Downloader and the quality is very good...,1,0,0
I have installed the knitr package and called library(knitr) but I do not see the icons/tabs in the upper left panel of RStudio that appear in the 6R session.  I must have left out a step but am clueless as to what to do next.,1,1,1
"Hello,_x0007__x0007_I think Prof. Hastie has a bug in the demo. _x0007_In the second video, round about 5 minutes, Random forests are shown. _x0007_He has the following two lines of R code:_x0007__x0007_    matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c(\red\"", \""blue\""), type=\""b\"", ylab=\""Mean Squared Error\"")_x0007_    legend(\""topright\"", legend=c(\""OOB\"", \""Test\""), pch=19, col=c(\""red\"", \""blue\"")_x0007__x0007_It seems like the colours are swapped in the legend. _x0007_I assume it should be: _x0007__x0007_    legend(\""topright\"", legend=c(\""OOB\"", \""Test\""), pch=19, col=c(\""blue\"", \""red\""))_x0007__x0007_If am wrong, please let me know._x0007_TIA.""",1,1,1
"If I recall correctly, that question had to do with a model in which there were two qualitative predictors, each of which had three levels.  So, in that case, the possible fitted values of the model can be summarized in a 3 by 3 table, which they're calling a regression surface in analogy to what would obtain if the two predictors were quantitative._x0007__x0007_Hope that helps.",1,0,0
"Not really sure what your \data\"" variable is referring to._x0007__x0007_Once you load in 7.R.RData, the `x` and `y` vectors should be available to you directly. You shouldn't have to use anything else, and your call to `lm` (or `glm`, or whatever...) will not need to specify a \""data\"" parameter at all._x0007__x0007_To answer your question, you can covert from character to numeric using the `as.numeric()` function. But you shouldn't have to do that here.""",1,0,0
"There is a one-to-one relationship between a given alpha value and |T| belonging to the subtree that minimizes the objective function. Finally, recall that as alpha increases, |T| decreases.",1,0,0
how do I get markdown into rstudio.  when I browse from the install packages option when I browse and it goes to my own local directory.  So that leads me to think there is a download I should download.  Is that true?  basically can you give me the steps to add markdown to my rstudio application.  Thanks.,1,1,1
"Hi,_x0007__x0007_the book (particularly chaps. 4.6.2 and 4.6.6) as well as the lectures (exp. end of Lec. 4.7, ROC curve) often refer to the classification error rate that one would obtain in case of random guessing, as a reference for the error rate of our classifiers. Consider for example the Caravan Insurance data set in the R package ISLR: 6% of individuals purchased a caravan insurance. It is stated on on page 166 that random guessing would give an error rate of 6%. However, it seems to me that error rate in case of random guessing should always be 50%! This would be coherent with what's written on page 160, where the error rate of random guessing (for a different data set) is said to be 50%._x0007__x0007_My reasoning is as follows: let $Y=1$ correspond to an individual that buys insurance, and $Y=0$ to an individual that doesn't. I throw a fair coin, and I predict that Joe will buy the insurance if the coin comes out head ($X=1$), otherwise he won't ($X=0$). Clearly $X$ and $Y$ are independent, and $P(X=0)=P(X=1)=0.5$ (fair coin). Now,  _x0007__x0007_$P(Error)=P(Y=1|X=0)P(X=0)+P(Y=0|X=1)P(X=1)$                                        _x0007__x0007_Y and X are independent, so_x0007__x0007_$P(Error)=P(Y=1)P(X=0)+P(Y=0)P(X=1)$_x0007__x0007_Also, since the coin is fair, _x0007__x0007_$P(Error)=.5P(Y=1)+.5P(Y=0)=.5*1=.5$_x0007__x0007_So the error rate is 50%! Where am I wrong?",1,1,1
"You need the package \knitr\"" with its dependicies.  Google \""kntir in r\"".  Be nice if that were mentioned in the text.""",1,1,1
"I have not used the smoothing spline, so take my answer with a grain of salt. Usually, when there is a tuning parameter (like complexity), cross-validation is used to find the \best\"" parameter value.""",1,0,0
www.clipconverter.cc,1,0,1
"Some of the questions make reference to \page X of the 'Notes.'\"" What are the notes? The textbook? The slide PDFs?_x0007__x0007_Yes, I know it's dumb...._x0007__x0007_Thanks in advance for your assistance.""",1,1,1
"Hi,_x0007_I think you are correct._x0007_as _x0007_test.err_x0007_ [<zipRedac>] <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007_oob.err_x0007_ [<zipRedac>] <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac> <zipRedac><zipRedac>.<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>_x0007__x0007_shows that the test errors are higher as indicated by the red curve.",1,0,1
"in the exercises after the \resampling in R\"" session, the answer of Q3 is <zipRedac><zipRedac>274; however, my answer is <zipRedac>.<zipRedac>247 and it is also seen as right answer. I am a little confused. does it mean that the answer is not unique?_x0007__x0007_In addition, can I use tsboot() function to do the block bootstrap? Or is there any other methods? And I found the answer is not equal to <zipRedac>.212 that is provided as the standard answer.""",1,1,1
"I came across this and it confused me a bit... I'll admit to some laziness/lack of time trying to read a whole paper to understand it. Anyone can tell us in a nutshell what generalized cross-validation is? many thanks,",1,1,1
"ight]$_x0007__x0007_In the context of splines and additive models, $mathbf{S}$ is the smoothing matrix and the trace of that matrix was used as the effective degrees of freedom for the smoother/spline model, but this could be generalised I presume (reading ELS) as they talk about $mathrm{trace(mathbf{S})}$ as being a measure of the effective number of parameters in the model._x0007__x0007_The key point is that GCV approximates the LOO CV result and is useful when computing the \""hat-values\"" for individual observations is is more difficult than coming up with a value for the effective complexity of the model.""",1,0,1
"I've yet to read the answer, but this was my gut reaction too. Maybe I'm confusing concepts here but I know that the more terms/components in a model the FIT gets btter but not necessarily the prediction. Without reading the answer yet I think this question is a bit ambiguously worded... I'll retract this in a moment once I've read the answer ;-)",1,0,1
Agree!,1,0,1
"I have generally found that GCV is a poor approximation of test set error. If you want to estimate test set error, do the validation set approach or cross-validation approach.",1,0,0
Chapter 8 videos seem fine.,1,0,1
"Thanks, Dan.  So the 3rd row (or column?) is the interaction term?",1,0,1
"I'm not sure, but it looks to me like it uses each basis function incrementally as it goes. Ex, knots 1 - 4 use the intercept and the first 3 basis functions. Then knot 5 uses those, and adds another basis function. Knot 6 uses those previous, then adds its own. At the end, the last knot uses all the basis functions, and adds its own.",1,0,0
Is there something that the SO answer does not address for you?,1,0,1
"yes, by default for classification, for regression it will give RMSE and R^2 plus their SD, but for both it can be configured (i.e., you can have it compute your own metrics).",1,0,1
"@alhf_x0007__x0007_> The following could even happen (*)_x0007__x0007_> - Overall mean Balance for females is lower than the overall mean Balance for males_x0007__x0007_> - but for any particular level of income, females of that income have higher Balance than males of that income. _x0007__x0007_> (*) ",1,0,1
"Most likely, you are viewing a .R file. You need to create a .Rmd file by File > New File > R Markdown, or using the \+\"" icon in the top left then R Markdown.""",1,0,0
"Beijing, China --> Atlanta, GA --> Portland, ME --> San Jose, CA",1,0,1
"I think they analytically worked those out. It suffices to answer Question 2.2 R2 to work it out in that case, and there are forum threads that help with that, too:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-quiz-review/threads/52de20d9033fb<phoneRedac>002b",1,0,0
Here natural log (ln) is used. It does matter.,1,0,0
I have done all the quizs. when I see the performance it says 100% but over al score is 9%. Can somebody explain em what is going on? To get the credit do we have to some assignments or just quiz after each lecture.,0,1,1
"With rlm I get and error - _x0007__x0007_> Error in rlm.default(x, y, weights, method = method, wt.method = wt.method,  : _x0007_  'x' is singular: singular fits are not implemented in 'rlm'",1,1,1
See http://scott.fortmann-roe.com/docs/BiasVariance.html,1,0,1
"Entropy is used to decide which feature to split-on. As long as one is consistent in using the same metric, how does it matter? This is not clear to me;",1,1,1
"In statistics, log is almost always base e.",1,0,0
"there are questions after each video lecture. click on the \Progress\"" link._x0007__x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>5.png""",1,0,1
"You \protect\"" a variable with the I() function._x0007__x0007_    reg.fit <- lm(y~ x + I(x^2), data)""",1,0,0
look at David_L's response below,1,0,1
"Hi,_x0007__x0007_some weeks ago I posted the link to a proof of the bias-variance decomposition that, while nice, didn't show all of the details. I found a great one in this book:_x0007__x0007_Bishop, C.M., \Pattern Recognition and Machine Learning\"", Springer, 2007_x0007__x0007_It's in paragraph 3.2. I really liked the book! It's very rigorous and full of clear examples. What's more, all the advanced mathematical tools he uses are explained in the Appendixes. The book content isn't really the same as this course's, so it doesn't really fit as a textbook for the course. However, we don't need a textbook for this course since we already have two excellent ones :) But for anyone interested in extra references on specific topics, this may just cut it.""",1,0,1
"I get the results that appear in the video.  You will likely figure this out pretty quickly.  Apologies if you have already tried some of these simple tests, but perhaps it would help to look at the results step by step, and maybe examine some of the variables._x0007__x0007_  + Run the code 1 line at a time (not in chunks)_x0007__x0007_  + Do you see a proper histogram plot? (line 10)_x0007__x0007_  + what is `length(High)`  (should = 400)_x0007__x0007_  +  how about this_x0007_     _x0007__x0007_>  head(High)_x0007__x0007_[1] \Yes\"" \""Yes\"" \""Yes\"" \""No\""  \""No\""  \""Yes\""_x0007__x0007_  + The only other thing I can think of is that perhaps your copy of ch8.Rmd was modified somehow.  Maybe reloading it from the course site?_x0007__x0007_Good luck.""",1,0,0
"Thank you very much! Yes, I must have modified it unknowingly. After reloading the file and restarting R, it runs well. But knowing you got correct results running the same codes really helped. Greatly appreciated.",1,0,0
Thx. Must be on my end then.,1,0,0
"Yes, I agree. It would be very helpful to have transcripts for the R Sessions, which I have found very enlightening.",1,0,1
"If you wish to fit a model like a0 + a1 x + a2 x^2 + ... + an x^n, you should define all the 2-degree and more predictors in I().",1,0,0
"Hi, I was confused, because in some moment I though that the value was $p^2$ approx, but the real value is completely different, so what is the different with $p^2$?_x0007__x0007_Best regards,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"There has been a video change of some sort. Before, I could slow down or speed up the video. No more. :(",0,1,1
It's easy to download the videos but it would also be nice to download an archive of the discussions.,1,1,1
"Good Evening_x0007__x0007_May be it is too late for me, but i have spent more than 15 min trying to understand what the quizz is really answer. I suppose the ten values comes into a single tree but i cannot figure how it can be done. _x0007__x0007_¿Any idea?",1,1,1
"When using several analysis methods (from ISLR and ESL) including the shrinkage methods, principal components, partial least squares, forward step regression, kernel smoothing, structured kernels, neural networks, and nearest neighbors, it is important to standardize the inputs.  Standardization seems not to be required for the tree-based methods owing to input transformation to conditionals at splits.  Does it ever make sense to standardize inputs for tree methods?",1,1,1
"Hi ZhouyunZ.  Many of the numerical answers allow some slack due to automatic grading.  In the case of resampling problems, we NEED slack because the results are random.  tsboot() will work, but you need to do the blocking properly!  If you are getting a low answer you probably messed up the blocking.",1,0,0
If you don't like the approach offered in the SO answer you might choose to skip the HTML rendering altogether.  With slight adjustments to the markdown you can use [Knitr][1] in RStudio together with R and LaTeX to produce PDF directly._x0007__x0007__x0007_  [1]: http://yihui.name/knitr/,1,0,0
"Do you know of any online courses you could please recommend to everyone, for those of us that have enjoyed it? Personally I want Bayesian analysis, but others probably will want other things._x0007_Also I love your teaching for it's unexpected but exceptionally welcome emphasis on usefulness and guided preference choice. Two presenters is good as well._x0007_Thanks again, <nameRedac_<anon_screen_name_redacted>>.",1,1,1
"Dallas, Texas",1,0,1
"I notice a lot of emphasis on this method (validation set) in chapter 8, used I assume to test we have the correct analysis. But, I assume after we did this we would in practice rerun it using the whole data-set to ensure our model was the best it could be?",1,1,1
Why don't we have to decide at what level to stop averaging the trees? I think this is a parameter we have to choose. For example at the lowest level only 40% of the trees had chosen the same answer (out of 5 choices). Isn't the choice whether to include that node in the final tree important?,1,1,1
"tree planning splits each variable into those that predict y=a and those that predict y=b. When building the tree it must decide at each node if the split in variable X1 is more useful than the split in X2. Therefore usefulness in predicting y is the criteria, so values of X are unimportant. The short answer is No.",1,0,0
Slides shown in Chapter 8 are extremely poor resolved. Does anyone have the same problem?,1,1,1
"Very informative answer. With the **right way**, the cross-validation error will be a **biased** (optimistic) estimate of the test error because of step (3). Right?",1,1,1
"Hi Matt,_x0007__x0007_I'm not sure I correctly understand your question, but here goes:_x0007__x0007_If I wanted to model the type of scenario you seem to be describing I would consider using a vector ARIMA (VARIMA) model. In such a case, you can account for correlated errors and multiple predictors._x0007__x0007_Although completely out of left field, another approach you might consider is that of multidimensional wavelets. If I recall correctly, wavelets have some nice properties that help one circumvent correlated error terms. Perhaps someone with more exposure to this topic could step in here and advise?_x0007__x0007_Best of luck,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Should we pick the **mtry** parameter using **cross-validation** or **OOB error estimate**? I think cross-validation estimate will be more reliable because OOB error uses only 1/3 of the trees for each observation. [fewer trees => more variance => worse estimate],1,1,1
"I am thinking about 5-fold cross-validation vs 10-fold cross-validation. One point in favor of 10-fold CV is that it has lower standard error compared to 5-fold CV [*sample size: 10 vs 5*]._x0007_Therefore, assuming enough compute capacity, we should prefer 10-fold cross-validation to 5-fold cross-validation. Does that sound like a good rule of thumb? Are there other considerations that I have overlooked?",1,1,1
"In \7.R Nonlinear Functions in R\"", anova() is used to compare a sequence of nested models. Why not just use cross-validation to compare these models (*pick the model with the lowest cross-validation error*)? Any guidelines on when to prefer anova() to cross-validation?""",1,1,1
"In \8.R Tree-Based Methods in R\"", at time 8:05, Prof Hastie says, \""*randomForests reduce the variance of the_x0007_trees by averaging._x0007_So it grows big bushy trees, and then gets rid of the_x0007_variance by averaging._x0007_Boosting, on the other hand, is really going off to bias._x0007_And so **boosting grows smaller, stubbier trees_x0007_and goes at the bias**.*\""_x0007_https://www.youtube.com/watch?v=IY7oWGXb77o#t=485_x0007__x0007_I am **struggling to grasp how exactly Boosting is going at the bias**? Can someone please elaborate?""",1,1,1
"I am working on trying to use CVlm to do multiple regression using 6 exploratory variables. I have <zipRedac><zipRedac><zipRedac>000 lines of data and when I run _x0007__x0007_cvfit <- CVlm(df=cleandata, fit5, m=5, printit=TRUE)_x0007__x0007_it completely fills the screen with residual error? I want to know how to get the fit coefficients out, anyone know how to do this?",1,1,1
"*When you use all the data to fit a model, you cannot trust the training error. Your CV error is an estimate of the test error.*_x0007__x0007_But when we use all the data to fit a model, we do not have CV error anymore._x0007__x0007_We cannot trust the training error. Then how do we know if the new model is any good?",1,1,1
"In a Regression demo (single & multivariant regression) nowhere the training and test dataset has been used by the instructor. The R program is directly choosing predictive variables for regression model. So, given test and training dataset how to model the regression problem?",1,1,1
"What is the logic behind when they say:_x0007__x0007_Rearranging gives ?6+.05?h+1?3.5=0 or h=50_x0007__x0007_I think it shoud be _x0007__x0007_?6+.05?h+1?3.5=0.5 because .5 is the total P(x)_x0007_my result equals 60 hours because the equation above gives 3/0.05_x0007__x0007_Can someone give me a clue on this, I got very frustrated not getting this result, since I missed the other one in the section too with the exponential functions....",0,1,1
"cvfit <- CVlm(df=cleandata, fit5, m=5, printit=F)",1,0,1
"Hi All._x0007__x0007_Sometimes it is useful to hand pick some variables to split the tree for first n layers and then run the tree algorithm on terminal node of the former tree. This kind of interactive tree can be build using data mining softwares like clementine._x0007__x0007_My current approach is to create two trees seperately (one using selected variables, the other using left ones) in R. This results in hard time interpreting nodes and plotting._x0007__x0007_Anyone got fresh ideas?_x0007_Thx.",1,1,1
"<redacted>:_x0007__x0007_> I suppose the ten values comes into a single tree but i_x0007_> cannot figure how it can be done._x0007_> _x0007_> Any idea?_x0007__x0007_To summarize the situation:_x0007__x0007_- There are ten bootstrap samples_x0007__x0007_- For each sample, a tree is constructed_x0007_  (so ten trees)_x0007__x0007_- We us",1,1,1
"The question asks about the **total** number of nodes, but per the notes, the alpha penalty is applied only to the **terminal** nodes. So it seems to me that in theory, a T2 model might be chosen that has fewer terminal nodes but still more total nodes because it has more internal ones. Thus I answered that there is not enough information to decide._x0007__x0007_I am curious if there is some logic about the possible numbers of terminal vs total nodes that means in fact you cannot end up with more total nodes from higher alpha value or not?",1,1,1
"I simply select, copy and paste",1,0,1
"I had the same problem: the tree had a depth of 1. I cleared the environment and started the code again without reloading anything, and it solved the issue.",1,0,0
Am I missing something -- why would you use tsboot (bootstrapping for time series) on these data?,1,1,1
"Check out the first response in _x0007_[this post.](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/5302bd339ad87c<phoneRedac>)_x0007__x0007_It explains why a zero coefficient would be a very special_x0007_case in ridge regression, but is not such a special case_x0007_with lasso.",1,0,0
"Good point! I just viewed that video. It seems Prof Hastie forgot that X-axis is NOT a number of trees, but it is `mtry` value._x0007__x0007_So it should be \the performance of random forest of 400 trees with a single random variable at each split\"", but I also will appreciate the response from course staff""",1,1,0
"Hi,_x0007__x0007_I started taking this course not only to learn more on the topic, but to hopefully solve a real regression problem I'm faced with. It's slightly more complex than this, but the crux of it as follows:_x0007__x0007_http://stats.stackexchange.com/questions/<zipRedac>3493/failing-at-linear-regression-prediction-on-a-real-data-set_x0007__x0007_I've since gained a slightly better understanding of the issue and written attempts both in R and Python to solve it, but still failing. _x0007__x0007_It seems as the linear model is not accounting for hidden skill variables (e.g., unskilled operators use robot A much more often, while robot B is used only by skilled operators), and possibly the collinearity of the model is not handled properly._x0007__x0007_Any ideas what could be done better here?",1,1,1
Good suggestion thanks!,1,0,1
"Hi,_x0007__x0007_How is it that when the flexibility of a model increases, the bias decreases. I'm able to understand how variance increases in this scenario but, can someone throw some light on the impact wrt bias ?",1,1,1
"I think so. If I have a lot of data, I want to have a test data set to make sure I didn't \cheat\"" cross-validation.""",1,0,1
"Because additional trees are fitted to the *residuals* of the current fit. When the first tree is grown it is trying to fit the response as the response values are copied to the residuals vector. Then, a simple tree is fitted and a shrunken version of the fitted values is taken as the contribution of that tree to the fit. The residuals are reduced by the same amount, ready for the next tree._x0007__x0007_When the second tree is grown, because each tree is fitted to the *residuals* (**not** the observed values of the response), the fit is improved some more and with it the bias is reduced a little bit; the two trees more closely follow the data than the first tree alone._x0007__x0007_As you keep adding trees, those trees explain aspects of the data not well fitted by the set of previously fitted trees. If you were to keep on fitting, eventually you'd explain all the variance in the data: the residuals would be reduced to zero, and hence there'd be *no* bias [but high variance at that point]. Achieving this takes a long time with boosted trees because of the shrinkage applied to the fitted values of each tree; hence boosted trees learn slowly._x0007__x0007_Contrast this with randomForests; there we have the same ensemble of trees, but each tree is fitted to the same response vector of data/observations. The stochasticity introduced via bagging and the random selection of `mtry` parameters for each split-selection reduces models that can be averaged over, to reduce variance._x0007__x0007_I can't quite think of a good way to explain what is happening to the bias in a randomForest; you are fitting a (potentially) low bias, high variance tree to a bootstrap sample of the data because you grow each tree large, but as you only consider random subsets of the available variables to form each split in those trees, they could still be biased. How the two sources of bias contribute to the actual bias of the tree and hence to the ensemble of trees I'm not sure about. Perhaps someone will comment?",1,1,1
"Around 7.09 in the video we have the Matplot OOB and Test Errors from the random forest. Looking at the code it appears the labeling is the wrong way around._x0007__x0007_    matplot(1:mtry,cbind(test.err, oob.err),pch=19,col=c(\red\"",\""blue\""),type=\""b\"",ylab=\""Mean Square Error\"")_x0007__x0007_The code will assign \""red\"" to test.err and \""blue\"" to oob.err_x0007_I believe he meant to write..._x0007__x0007_    matplot(1:mtry,cbind(oob.err, test.err),pch=19,col=c(\""red\"",\""blue\""),type=\""b\"",ylab=\""Mean Square Error\"")_x0007__x0007_Then the legend script will be correct._x0007__x0007_    legend(\""topright\"", legend=c(\""OOB\"", \""Test\""), pch=19, col=c(\""red\"", \""blue\""))_x0007__x0007_Can you let me know if I misunderstood anything here from the video...""",1,1,1
"In the lectures the number of terminal nodes selected for each tree is at the min of the cross validation errors. However the error bars in the cross validation are sometime large. Does that mean that I could also select a tree that is smaller than the one selected? In fact isn't that \better\"" (selecting a smaller tree that is withing error bars) ?""",1,1,1
"> We create a new object, \reg.summary\"", with which to save \""regfit.full\""._x0007__x0007_Ah, but you *don't* actually save `regfit.full` in the variable `reg.summary`._x0007__x0007_Notice the call to `summary()` that's included in that line. What you're *really* assigning to `regfit.summary` is the output of the summary function._x0007__x0007_That is, in one line, you're telling R to run `summary(regfit.full)` and, instead of displaying the results on the screen, *store* those results in `regfit.summary`.""",1,0,0
"for random forests, since each tree is bushy (large), there is low bias in each tree.",1,0,1
"Hi Gayle,_x0007__x0007_Unfortunately I don't understand your question.  What do you mean by \level\""?  We average entire trees, not just part of them.  I also don't understand what you mean by \""For example... choices).\""  Maybe if you clarify I (or someone else) will be able to reply.""",1,0,1
"OOB is fine.  I don't know why you identify OOB error as \uses only 1/3 of the trees\"" and that being a bad thing...  OOB error uses approx 1/3 of the trees, but that is to estimate test error (where more trees is better, and 1/3 is a lot).  If you want to be concerned about something, it should be the training only based on 2/3 of the data (with some extra replication).  That said, I would not be too concerned as CV also fits models with less data.""",1,0,0
"Hi <redacted>,_x0007__x0007_I think you'll be able to get a better answer to your question if you flesh it out a little.  First, Multivariant regression is not a thing (multivariate? multiple?).  Second, which regression demo do you mean?",1,0,1
"Hi LopezRuiz,_x0007__x0007_$P((h, 3.5)) = frac{e^{-6 + .05*h + 1*3.5}}{1 + e^{-6 + .05*h + 1*3.5}} = .5$_x0007__x0007_This implies that $e^{-6 + .05*h + 1*3.5} = 1/2(1 + e^{-6 + .05*h + 1*3.5})$_x0007__x0007_This implies that $frac{1}{2}e^{-6 + .05*h + 1*3.5} = frac{1}{2}$_x0007__x0007_implies that $e^{-6 + .05*h + 1*3.5} = 1$._x0007__x0007_Then you take logs of each side to get to the rest of the staff explanation.  Hope that helps!",1,0,0
I was completely lost... But i found myself. Thanks to both for your help.,1,0,0
"Formula for 6.3R2 is given in the lecture as P!/2!(P-2)!_x0007_If you use P as the number of feature plus the 0 features, (using 20 as the example to not give the answer) P would actually = 21 (20 features plus 1 y-intercept only)_x0007_Given that, this formula in R works:_x0007_> a <- factorial(21)_x0007__x0007_> b <- factorial(2)_x0007__x0007_> c <- factorial(21-2)_x0007__x0007_> a/(b*c)_x0007__x0007_But I must admit, I thought that formula was for the Subset method, not the Forward Selection method (as I understood the video) <shrug>",1,0,0
Keep in mind that you can progress with the course at your own pace.  Feel free to finish up chpt 6 this week!,1,0,1
The error about singular fits probably means that you are using too many predictors (n < p) or have some collinearity in your predictors that you didn't expect.,1,0,0
"<redacted>:_x0007__x0007_> How is it that when the flexibility of a model increases,_x0007_> the bias decreases. I'm able to understand how variance_x0007_> increases in this scenario but, can someone throw some light_x0007_> on the impact wrt bias ?_x0007__x0007_See also the discussion in_x0007_[th",1,1,1
"Hi, since I can not find any mention to cvtools package in the book, I'd want to know if there is good choice, cause it seems at first glance to provide shortcuts to some steps in cross validation. I found DMwR too, and both seem to have a nice lattice interface to make the plots._x0007__x0007_Bye and thanks in advance for your time.",1,1,1
I spent too much time on this one. Thanks acflippo and imperd01 (from 'Bad wording in 5.4.R1).,1,0,0
Thanks.  That makes sense.,1,0,1
"Hi_x0007_I never got such errors_x0007__x0007_Which OS are you operating on?_x0007__x0007_It seems to be tedious, but you can try to compile and build it from source code - http://www.rstudio.com/ide/download/desktop",1,0,1
"Hi_x0007__x0007_I think the number of folds selection could be based on the consideration of sufficiency of the test sample size to predict the response within the tolerated (subjective!) error %. You can look at 'sample size determination', which is also called 'power analysis' as far as I know.",1,0,1
"From my understanding, using a test set is the gold standard for estimating out-of-sample performance of predictive models. On p. 36 of the slides, OOB error estimation is introduced as \a very straightforward way to estimate the test error of a bagged model\"", and that it is \""essentially the LOO cross-validation error for bagging\""._x0007__x0007_What I don't understand is why the curve for OOB: Bagging on p. 34 of the slides is so much lower than the Test: Bagging curve on the same slide (suggesting optimistic bias in the OOB estimate). _x0007__x0007_The only explanation I can think of is a substantial difference between the training set (from which the OOB estimate is derived) and the test set. Or is there another explanation that I'm not aware of?""",1,1,1
"MooMoo, thank you for explaining it so well! This part of the course isn't explained very clearly, but your description is perfect. I think the course team should bring this explanation into the material itself. (There's no poor English in your post by the way :-)",0,0,1
"Hi,_x0007__x0007_If I work with a very small sample size (n=22 european countries)._x0007__x0007_Is it a good idea or better is it necessary, when we have such a few data (n=22 european countries) to use the bootstrap to improve estimation and confidence intervals for the regression coefficients ?_x0007__x0007_Thanks for your help,_x0007_Best,",1,1,1
I think it looks like Prof. Hastie got the order of the labels in the call to `label` reversed.,1,0,1
"Hi everybody,_x0007__x0007_I do understand that high dimensionality is a problem for least squares as it works with a distance measure and that becomes less useful with more dimensions. But in the lecture it is claimed, that the least squares model is even undefined for p>N. And the book (Intro to Stat Learning, 6.1.2, page 208) alternatively claims that least squares of any model with p>=N will not yield a unique solution. I've done a fair share of online searching and browsed through the Elements of Stat Learning but don't seem to get it. Could anybody explain this or provide a pointer somewhere or to some useful search terms?_x0007__x0007_Thanks in advance,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
@JuilaneA you handle the log1 right. Just check the equation in slide 6 and rearrange it to be equal to X and you'll get the right number. You can simply round you don't have to have fractions.,1,0,1
Boils down to linear algebra. The matrix $(X^TX)$ cannot be inverted when p > n._x0007__x0007_The OLS solution is: _x0007__x0007_$hat_x0008_eta = (X^TX)^{-1}X^TY$,1,0,1
"I was curious too, also since I was looking to a meaningful translation in Italian. I found most literature in Italian use the original names, ridge and lasso regression. Just one math student at Politechnic of Milan used the Italian for L2-penalized regression and L1-penalized regression (in an application to medical data)._x0007__x0007_I am always curious of the origin of words, so I gather that in ridge, there must be a ridge somewhere, i.e. \2. any long narrow raised strip or elevation, as on a fabric or in ploughed land\"". Where is the ridge here? Thanks for pointing to a waywayabovemyhead math page, but I saw a figure similar to what is in the slides, and did not get any closer to the ridge._x0007__x0007_And how about the lasso? \""a long rope or thong with a running noose at one end, used (esp in America) for roping horses, cattle, etc;\"". In Italy, that was called \""lazo\"" in all comic books of cowboys that were popular in my youth, so translation is easy. But where is the lasso?""",1,1,1
"ightarrow 0^+} D(p_{mk}) = 0$_x0007__x0007_My observation is that in both a mathematical and programming sense $D$ is currently only defined when each $p_{mk} in (0,1]$. It would be much more convenient if it were defined on the closed interval $[0, 1]$. To do this we would need to add a case statement to each term in the sum. But that would really clutter up the simple equation :)""",1,0,1
"I've been trying out decision trees and really like the rattle routine to plot them.  Details are available at http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html_x0007__x0007_Here is an example:_x0007_![https://www.dropbox.com/s/638nyjpa8v8chh2/Gender_tree.png][1]_x0007__x0007_Rattle is a fairly heavy package.  If you just want this function the source code is in:_x0007_fancyRpartPlot  _x0007_and it works by itself after  _x0007_library(rpart.plot)_x0007__x0007_If anyone has alternatives they prefer please share._x0007__x0007_Edit: This version is almost as nice (mainly lacking color):_x0007__x0007_    prp(model, type=2, extra=104, nn=TRUE, fallen.leaves=TRUE,_x0007_        faclen=0, varlen=0, shadow.col=\grey\"", branch.lty=3)_x0007__x0007_For (much) more on this see:_x0007_[One pager on Decision Trees][2]_x0007__x0007__x0007_  [1]: https://dl.dropboxusercontent.com/s/638nyjpa8v8chh2/Gender_tree.png_x0007_  [2]: http://onepager.togaware.com/DTreesR.pdf""",1,0,1
"Hi Olmo,_x0007__x0007_Try them out and see if you like them.  The package system is one of R's biggest pros.",1,0,1
"When introducing local regression, the book notes that it is a memory-based procedure because you need all the training data for each prediction (like with nearest-neighbors).  This would seem to rule out this technique for any large data set; i.e., larger than can fit into memory on one machine._x0007__x0007_But do splines have the same issue?  If you use bs() in a linear model, you end up with coefficients for the # of knots plus degrees of the spline.  But is there a portable equation for calculating those terms for a new value of <i>x</i>?  The bs() function creates a basis matrix with K+3 columns and rows of length equal to the number of training observations.  But I can't recreate any of the values in that matrix using formulas 7.9 and 7.10 in the book.  And you don't want to carry around K+3 times the amount of training data just to make a prediction._x0007__x0007_I realize you can just use the predict() method with the returned linear model object, but I am looking for ways to implement the prediction separately from the training implementation.  For example, you might use R for training and model creation but Java at run-time for real-time predictions.",1,1,1
"Why is the solution not unique? It depends. For instance, if $$X=matrix 1,1,-1,-1\\ 1,0,2,1\\ 0,1,-3,-2$$ (sorry about the alignment), and $Y=(1,-1,0)$, then I think  the (only) projection of $Y$ onto the column space of $X$ is  $Z=(1/3, -1/3, 2/3)$, since $ Y - Z$ is orthogonal to the column space. Also, the formula for beta above is not quite right.",1,0,1
"Where is this? Not textbooks?  Course material notes would be instructive and useful.  I should have asked sooner.   Thanks In Advance,  Charlie <nameRedac_<anon_screen_name_redacted>>  (Really enjoying the class)",1,1,1
"This response is just to add a little to the explantion in an earlier thread_x0007_([click here for earlier thread](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/5302bd339ad87c<phoneRedac>))_x0007_and to the earlier response in this thread._x0007__x0007__x0007_Here's a rough diagram showing that the lasso will not have_x0007_a corner solution (and one of the coefficients will be exactly zero) if the centre of the ellipses is in_x0007_certain zones — shaded red. [*] _x0007_If the centre is in the white area there will ba a corner solution._x0007_(And of course if the centre is in the blue diamond, there will_x0007_be no shrikage at all.)_x0007__x0007_![lasso does not always have corner solution][1]_x0007__x0007_As the constraint gets tighter (more shrinkage), the blue_x0007_diamond gets smaller, and the zones where there's not a_x0007_corner solution get smaller.  With enough shrinkage, one of_x0007_the coefficients will go to zero.[**]_x0007__x0007__x0007_----_x0007_[*] The ellipses are drawn with no correlation between the two_x0007_estimated slope coefficients, so the axes of symmetry are_x0007_exactly vertical and horizontal.  But the idea would be_x0007_similar if there was correlation between the estimated slope_x0007_coefficients, where the ellipses would be tilted._x0007__x0007__x0007_[**] _x0007_Except, I guess, in circumstances which are so particular that_x0007_they are quite unlikey._x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac>0<phoneRedac>7.png",1,0,0
"Can distribution of 500 points >0 (values can be fractional) with a mean =165 and s.d =195 not be right-skewed? A statement was made that because the values are >=0 and mean is less then S.d., the distribution must be right-skewed.  How can it be proven or a counter-example constructed?  I can easily construct a set of 4 or 5 points with all different values (flat histogram) and mean < s.d., but how to construct the one with 500 points and given mean and s.d.?",1,1,1
"The projection is indeed unique, meaning, the particular vector of fitted Y values that minimizes the sum of squares always exists. It is the vector of slope coefficients that is not unique (and in fact, is infinite when p>n), meaning there are infinite number of beta_hats that bring X*beta_hat to that one particular yhat._x0007__x0007_The reason comes from solving X'X beta = X'y. These are the normal equations, and when X'X is invertible the solution stated above is correct. When X'X is not of full rank, but has at least one solution, the system has infinite solutions, as does any system Ax=b . Any linear algebra text will have this discussion in more detail.",1,0,0
Thank you! I got it!,1,0,0
"Given the y=50% line on the Progress bar chart, and given that I seem to recall reading this requirement somewhere, I believe that it is true; however, I cannot find it written anywhere and am thus now unsure.  Can you tell me (us) where it is written?  Is it a Stanford MOOC standard written somewhere outside of the course materials?  Thanks.",1,1,1
WOW! There are many replies to my explanation. _x0007_I'm glad I was able to help you.,1,0,1
"@DerDavid:_x0007__x0007_[On a less technical note (than the comment above).]_x0007__x0007_Often starting with the simplest extremes can help with our_x0007_intuitive understanding._x0007__x0007_----_x0007__x0007_To start with, consider:_x0007__x0007_- p=1, one predictor _x0007_- N=1, just one observation_x0007__x0007_Note that $p=N$_x0007__x0007_To ",1,0,1
"@tjbec:_x0007__x0007_> Very good points all, and thank you for correcting my_x0007_> loose/sloppy language. I would add that it's true that when_x0007_> in 2-d the the red area in your graph seems like a lot, but_x0007_> in higher dimensions, it'll get swamped by the white_x0007_> area. And",1,0,1
"I am trying to download 5.R.RData, but I cannot opent it, can please let me know how I can open this file._x0007__x0007_With kind regards,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"If the response variable is 3-class, can I use lasso or ridge to solve it by logistic regression model? Thanks!",1,1,1
"May I raise that question again: why don't we ever check for plausibility of iid errors assumption? It turns that Least Squares is applied purely as mechanics to minimize MSE. But don't we ever care about CIs or significance of our coefficients estimates? Since we are dealing with statistical learning, we can hardly be satisfied with point estimates, but would rather infer confidence boundaries for the models' estimates, wouldn't we?",1,1,1
"Hi,_x0007_You can use glmnet() and cv.glmnet() with family= \multinomial\""._x0007_See here:_x0007_http://www.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin""",1,0,1
Great stuff from my part as well MooMoo. I did not even know what a basis was until I started reading your explanation!,1,0,0
"Thanks for all the different approaches in answering my question. I think I just about got it._x0007__x0007_I kind of did a test in between the abstract and the intuitive approach for myself: I worked through the abstract examples of (i) N=3 and p=2, (ii) N=2 and p=3, i.e. created the Matrices X with unique variables for each position, and figured out the determinants of $X^TX$. For the second example where there are more predictors than samples, all terms of the determinant cancel out, so the determinant of any such matrix will always be zero, meaning it is singular and not invertible. For example one where there are more samples than predictors, the terms don't generally cancel out. Well, a nice little practice in matrix algebra that could probably be generalised for any p>N through induction. But I'll work with the intuition for now and move on in the course!_x0007__x0007_Thanks again!",1,0,0
"Personally I watch the videos, read the book and only then do the quizzes. In a couple occasions I noticed that lectures weren't sufficient to answer quizzes, so I decided to read the book before attempting answers. Also, reading the book is great to fix the key points in my mind. I never read the pdfs, since I watched the videos already. If I have any spare time, I prefer to study the book or navigate the forum. Doing the book exercises would be a great idea, but I've got a full-time job and I I'd never find time for that. _x0007__x0007_<nameRedac_<anon_screen_name_redacted>>_x0007__x0007_ps I'm very curious, so I like to read extra references, such as ESL, and the book by Bishop I cited some time ago. The few times I do that, I regularly fall behind schedule (but I feel satisfied :)",1,0,1
"It doesn't works for me. I get_x0007__x0007_1 - StatLearning Statistical Learning -> Started_x0007_Enter Course Number: 1_x0007_Traceback (most recent call last):_x0007_  File \edx-dl.py\"", line 403, in <module>_x0007_    main()_x0007_  File \""edx-dl.py\"", line 286, in main_x0007_    {\""class\"": \""content-wrapper\""}).section.div.div.nav_x0007_AttributeError: 'NoneType' object has no attribute 'section'""",1,1,1
"When viewing the lecture (and reading the book), it would be nice to run the examples of the illustrations in R. Yes, we have the Credit dataset. Yes, we can easily do lm/glm on the data. However, the plots and control structure for gathering the data for the plots would be instructive.",1,1,1
Thank you.   I now see page numbers in each pdf I downloaded.,1,0,0
"Hello everyone!_x0007__x0007_I have some doubts about which is called the \right cv\"" and the \""wrong cv\"". I'm actually using cross validation at my bachelor project, and I got a big doubt, since seems I was using it the wrong way... _x0007__x0007_What I did is, I developed some linear regression models (which actually are quite complicated, with some predictors and interactions), BUT I updated them before using CV on them. I mean, I deleted already such variables/interactions that were non-significant for the models. And with my \""clean\"" model, then I apply cross validation, in order to select the \""best\"" models. But seems I did it wrong, or?? _x0007__x0007_I don't really understand which is the correct way to apply CV to my models. Should I leave EVERY predictor and interaction (which would be almost impossible, there would be hundreads of them) before applying CV? And them develope (update) the models with the best CV results? _x0007__x0007_Can anyone please explain me how does it work? That would be really helpful... I'm new at this topic, and I'm still pretty lost!!!_x0007__x0007_Thank you very much to every information :)""",1,1,1
"Thanks dkdog, that helps tremendously!",1,0,0
"The allocation of observations to one of the $k$ groups/folds is a (pseudo)random process. If you want to make the results repeatable, use `set.seed(x)`, where `x` is a number._x0007__x0007_You could run the CV a number of times and see if there is some consensus as to the size of the \optimal\"" tree. However, if the \""optimal\"" tree is so sensitive to the subset of data used, perhaps the tree isn't doing a very good job of approximating the relationship $Y sim X$ in the population, just in the rare sample from the population that you happened to collect.""",1,0,0
"The lecture notes (lecture 5, pages 17-21) are a good reference. *Elements of Statistical Learning* pages 245-247 are also good. See Ambroise and McLachlan (2002) for a detailed discussion of this issue.",1,0,1
I think he means that lots of companies are embracing data-driven decision making.,1,0,1
"This sounds like a rather alarming result concerning the CV method. Do you find it strange that textbooks never mention anything like \make sure you run your CV multiple times to make sure you get a stable answer\""?_x0007__x0007_The sampling issue seems to be based on how many different ways there are to choose the 10 blocks, which is what allows the corresponding result to be very different. For example, with a dataset of 100 observations, the number of possible folds is (100 choose 10)x(90 choose 10)x(80 choose 10) ... (10 choose 10), which is an extremely large number. Do you find this to be an unsettling concern about CV?""",0,1,1
"I don't see this as a problem with cross-validation, rather it is a problem with trees. Trees suffer from high variance (slight changes to the training set) would lead to drastically different trees.",1,0,1
Very helpful explanation. Thank you!,1,0,0
Thank you so much for this clarification. <redacted> <redacted> and <redacted> I had used the wrong log base and had no clue why the answer was wrong.,1,0,0
"Yes, this was a mistake. Well caught.",1,0,1
"At the risk of seeming ungrateful (asking for more when already given so much) I second <redacted> suggestion.  I actually feel quite grateful -- I've participated in several of these online courses and this is the best, by far:  having access to the text (2 texts, in fact), the use of R, and the hard-to-quantify but engaging performance of the Professors are the factors that have made StatLearning very rewarding.  I would go a little further than <redacted> and ask for even more.  This course would be significantly improved if more of the lecture time was spent going through meaningful, real-world examples.  The excellent texts make much of the material in the videos -- not all, but a lot of it -- somewhat redundant, in my opinion, especially since the worked examples themselves could be chosen to emphasize the topic being presented._x0007__x0007_[Oh yes, and the interviews are also a real treat:  many thanks!]",1,1,1
"Back in the day, I used to do some pretty sophisticated stuff in GUASS: Monte Carlos, Bordered Hessians constrained optimization, simple SGE modeling, etc._x0007_I don't think I'm stupid._x0007_I can't make heads or tails about R. The arguments are poorly described and operators appear to be randomly chosen. I guess I wouldn't have used a \!\"" _x0007_And then there is the loading of the data. I've tried several approaches to load the R session data. Can't do it.""",0,0,1
"I have a general question about categorical variables.  If a variable is categorical and has too many categories(zip codes for instance or counties), what is the proper way to bin them into classes so that they can be used as predictors._x0007__x0007_For instance if you wanted to predict price from zipcode and a bunch of other variables, what is the proper way to bin the zipcodes so they can be used as a predictor.",1,1,1
"This is a little mathematical, but here it goes: a non-regularized solution to an ordinary LS (OLS) is given by: inv(A'A)A'Y. ':Transpose, Y: observations. A weighted LS solution takes the form inv(A'WA)A'WY, where W is the weighting matrix. Now a regularized solution takes the form (without weighting to simplify the formula) inv(A'A + lambda*I)A'Y. So the Regularizer here is lambda*I where I is the identity matrix of appropriate size. So now here's my question: If I knew the correlation between the answers, how can I use that information to select a better Regularizer than lambda*I? _x0007__x0007_as an example, take the problem of fitting a 1D function to a set of observations, say 100 observations, and I want to evaluate the function at 1000 query points. If I knew or can compute the correlation between the function values at these query (unknown) locations, how can use this information to construct a better Regularizer?_x0007__x0007_hope you'll enjoy this one :)_x0007_many thanks,",1,0,1
"Under the link above for \Wiki\"", I think they want us to post our solutions. One link went to one student's attempts on github, but he only did up to Ch 6. I'll try to post some on the wiki page, hopefully get some discussion going.""",1,0,1
"Maybe he meant the change in the approach over the past few decades. As I've gathered from various lectures, previously, they had to do most work by hand, so it was better to use formulas, theories, and clever math to project how things would work. Today, with fast cheap computers, it's easier  to write some code for a simulation and observe what comes out, or put a link to \take a free survey for fun\"" and you get 100k responses very quickly.     _x0007_I was watching a probability lecture on the MIT site, talking about the Monty Hall problem. When it came out (1970's or 1980's?), there was great discourse citing theories and formulas and all. But then the lecturer said something like, \""well nowadays of course you just write some code and see what happens, no need to argue much when we have data and can actually look at it.\""""",1,0,1
"I've added my attempt for Ch 7, Ex 1, a and b. But if anyone knows a better solution, please feel free to edit.",1,0,1
"The answer (as it so often is) is that it depends. Of course, it would be good to incorporate geographic location into the binning.",1,0,1
"So I know that we are well off of unsupervised learning and might not even cover ICA in this course, but I'm implementing it for a project and figured someone here might be able to answer my questions._x0007__x0007_Basically, when I use the fastICA function in R (http://cran.r-project.org/web/packages/fastICA/fastICA.pdf) on a certain dataset, I get a completely different mixing and unmixing matrix each time I run the function. The dataset has not changed at all, I am using tolerance of 1e-10 and am extracting as many ICs as there are initial variables (in an attempt to try to make the unmixing matrix analogous to the PCA loadings matrix). I feel like the mixing and unmixing matrices are supposed to be stable (isn't this an assumption?), especially on the same dataset. _x0007__x0007_Am I looking at it wrong or missing something? Is the function flawed?",1,1,1
Deviance is not the same as misclassification rate. It is a measure that depends not only on predictions but also on the confidence of the predictions._x0007__x0007_http://en.wikipedia.org/wiki/Deviance_(statistics),1,0,1
"I think you slightly misunderstood my question. I would like to just take a categorical variable like zipcodes in which there could be over a thousand categories and group them into a small number of groups (between 5-10), so they could be used in a model (otherwise its useless for many algorithms).  _x0007__x0007_Can I use price (the quantity which is to be predicted) to somehow determine a grouping and if so how does this affect the test error when I create a model using this zipcode grouping and other predictors to predict price.  _x0007__x0007_It seems like this is a common problem, just wondering what the general stategies are for grouping a category with many elements into a small number of similar groups?",1,1,1
"<redacted>:_x0007__x0007_> Can distribution of 500 points >0 (values can be fractional) with a_x0007_> mean =165 and s.d =195 not be right-skewed? A statement was made that_x0007_> because the values are >=0 and mean is less then S.d., the_x0007_> distribution must be right-skewed. _x0007__x0007_S",1,0,1
"Has anybody used a package called bigmemory?  I wonder if it can be used with the stats learning packages?  I have not had a chance to check it out yet, but hopefully I can get to it next week.",1,1,1
"Thanks, that makes sense!",1,0,1
"In the lectures it is mentioned  that (given a large dataset) a hold-out test set is the preferable method if you have enough data. I have two questions about that:_x0007__x0007_1) Wouldn't it be better to eventually fit the model to the whole dataset and use that model instead of the model fitted on the trainingset only? For this model the test error (evaluated on the testset) probably gives a conservative estimate of the test error for the model fitted on the whole dataset._x0007_2) Isn't cross-validation always superior as this would give a more stable estimate of test error (not depending on one random split)? Even if you have a large dataset, as you are fitting more and more complex models, the effective size of the dataset decreases and it would be wasteful to set aside some part of the data; With crossvalidation you would be better able to tease out complex patterns in the data than with the hold-out method._x0007__x0007_Any opinions on this?",1,1,1
Concise and cut straight to the point! Thank you a lot!,1,0,1
"There are 4 reasons to always have a test data set (assuming you have a large data set with more than 1 MM observations):_x0007__x0007_1. You may be using CV the wrong way (and not know it). As explained in lecture, if you have 1,000 variables and pre-screen them using bi-variate correlations between each x and y, then your CV estimate of test set error may be overly optimistic. _x0007_2. The right way to use CV would be to include variable pre-screening at each fold. However, if you have a large data set, doing CV the right way might be computationally intensive. For example, if you do 10-fold CV with 1 MM observations, you're pre-screening the variables 10 times where **each** iteration would use 900,000 observations to pre-screen 1,000 variables. If you had a 70% training/30% test splilt, you would prescreen the variables once on 700,000 training observations._x0007_3. If you use CV to estimate tuning parameters **and** use CV to estimate test set error, your CV test set error may be overly optimistic. I often use CV to choose tuning parameters, but use a test set to estimate test set error._x0007_4. Comparing CV test set errors between two models (e.g., lasso vs. trees) may be problematic unless you correctly set the seed values. By default, 10-fold CV randomly assigns an observation to one fold. If you run 10-fold CV repeatedly, an observation might end up in different folds. For example, you run cv.glmnet(), observation 1 might be in fold 9. If you run cv.tree() observation 1 might be in fold 10. The CV test set errors are not comparable because the compositions of the folds are different! You can avoid this (I think) by setting the same seed value right before running each cv function.",1,0,0
"Hi,_x0007__x0007_I guess the main problem with being unable to load the right data is that your search directory in R is searching in the wrong place. _x0007_Before doing anything else, do the following:_x0007_1) getwd() # This will tell you in which directory R is searching. _x0007_2) Find the directory in which you have saved the focal file (if you use windows like me than just open the righ map and double click on the topline in the map_x0007_3) setwd(\C:UsersDropboxMethodsRStanford\"") # That is my directory (make sure to either use double \\ or to change them to forward slash /_x0007_4) load(\""7.R.RData\"") # Normaly not case sensITIve but take no chances :)  _x0007__x0007_That should do the trick""",1,0,1
"I tried to plot the fitted function in the quiz of 7.R but can't seem to get it to plot correctly. I'm obviously missing something about how R operates (R is still something of a mystery to me--much less intuitive than Matlab or even Stata). _x0007__x0007_Anyway,here are my commands:_x0007__x0007_fit2 = lm(y ~ x + I(x^2))_x0007__x0007_summary(fit2)_x0007__x0007_yhat = predict(fit2)_x0007__x0007_plot(x,y)_x0007__x0007_lines(x, yhat)_x0007__x0007_When I do that, the \fitted line\"" of the yhats are \""squiggly\"" in the x's. The problem is that the x's aren't sorted properly. If I do _x0007__x0007_plot(x, yhat) _x0007__x0007_you get the correct picture. (But plot(x,yhat,type='l') is again not correct)._x0007__x0007_What am I missing? _x0007__x0007_Thanks.""",1,1,1
"If you're trying to predict price, be careful about binning zip codes according to price. That's an example of the wrong way to pre-screen variables.",1,0,0
"To better understand the quantitative basis of the bias-variance trade-off, I'm working through Chapter 7 of Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/). Around page 220, the authors make a distinction between the *test error* $	ext{Err}_mathcal{T}$ (the expected prediction error for the specific training set $mathcal{T}$) and the *expected test error* $	ext{Err}$ (the expected prediction error averaged over all possible training sets)._x0007__x0007_In symbols, test error is expressed as_x0007__x0007__x0008_egin{equation}_x0007_	ext{Err}_mathcal{T}=L(Y, hat{f}(X)|mathcal{T})_x0007_end{equation}_x0007__x0007_and expected test error is given by_x0007__x0007__x0008_egin{equation}_x0007_	ext{Err}=L(Y, hat{f}(X))_x0007_end{equation}_x0007__x0007_where $L$ is a loss function to be minimized (e.g. squared error, negative log-likelihood)._x0007__x0007_So far so good. Looking at those two, I would (apparently naively) think that we would prefer to obtain a good estimate of $	ext{Err}$, the expected error across all training sets, than to have a good estimate of $	ext{Err}_mathcal{T}$, which 'only' gives us the expected error for the current training set, $mathcal{T}$. Indicating that I've somehow got this exactly backwards, the authors write, \estimation of $ext{Err}_mathcal{T}$ will be our goal, although we will see that $ext{Err}$ is more amenable to statistical analysis and most methods effectively estimate the expected error.\""  So my question is, why is estimating $ext{Err}_mathcal{T}$ the more desirable goal?   The only rationale I can think of is that full data set available to us (and upon which we'll fit our final model) *is* inevitably correlated with the training set, so maybe $ext{Err}_mathcal{T}$ should give us a better estimate of  the actual prediction error of the model that we eventually fit than would $ Is that the reason we should prefer $ ext{Err}_mathcal{T}$ (if we could get it), or am I missing the actual point?""",1,1,1
"Not having the Applied exercises solutions are greatly hindering my learning. To me, it's key to get the mechanics down in the code. Actual learning is 100 fold iteration between coding mechanics and theory. After a long search on Google, I couldn't find the applied solutions anywhere after Chapter 6.",0,1,1
"Or perhaps another problem was simply that when you saved the file, it wasn't saved with a .RData file extension?  ---Unlike other chapters (I think I recall), for Chapter 7 the file was named \7.R.RData\"", and depending on your system it might have been saved as \""7.R\"". RStudio would think that's a script file, not a workspace.""",1,0,1
"According to the instructions for Quiz 9.R we should use a normal distribution of_x0007_$% _x0007_N_{10}(0,I_{10}) _x0007_$._x0007_respectively_x0007_$% _x0007_N_{10}(mu,I_{10}) _x0007_$._x0007_I guess the subscript 10 indicates that there are 10 variables but I have no clue what the_x0007_$%_x0007_I_{10}_x0007_$_x0007_stands for._x0007__x0007_Any hints would be appreciated.",1,0,1
"\I\"" is the identity matrix_x0007__x0007_Your question actually helped me get this a bit better.  I see there are 10 \""x\"" variables.  I was thinking there was a count 10 of a single x variable""",1,0,0
"\I\"" is the identity matrix \""I\""_10 would be 10x10 identity matrix.  So our variance is 1""",1,0,1
"Found the following interesting looking references for group lasso, anyone know if this can be done within glmnet?_x0007__x0007_http://arxiv.org/pdf/1<phoneRedac>.pdf_x0007__x0007_http://cs229.stanford.edu/proj2012/ChoiParkSeo-LassoInCategoricalData.pdf",1,1,1
"Thanks, MooMoo!",1,0,1
"Hey guys,_x0007__x0007_As you might have heard, data science is the sexiest job these days. _x0007__x0007_But to become a Data Scientist is not easy - you need experience in a wide variety of skills and knowledge areas. Do you know what you need to know? _x0007__x0007_STAFF EDIT TO REMOVE ADVERTISEMENT:  Feel free to go google (or bing or dogpile) around for practice interview questions.  You will find many.  I'm sure this user's curated list is good as well, but please do not advertise products on this board.",1,0,1
I believe it stands for the variance/covariance matrix.  Perhaps the x's are independent so the off diagonal elements would be zero so you could specify the variance for the 10 x's with a 10x10 identity matrix.  Honestly I'm shootin in the dark a bit here.  I'm still confused on the setup of this problem,1,1,1
"Hi Will, to train the SVM I think you need 100 observations in each of the 50 samples. Each observation has 10 explanatory variables, half of the observation are 0 and half are 1.. _x0007__x0007_To test the model you will need another set (the large one they are asking).",1,0,1
"If really care about helping us becoming \data scientist\"", just give away such guide for free... in the same way that this course authors gave a downloadable options of their books._x0007_Or at least mention that is a resource to be purchased; otherwise please just don't bother spamming us.""",0,0,1
Thank you.  I'm still mucking through R but I think I might be getting close.  I'm using a test set of size 100 as well.,1,0,0
"What does the \E\"" and \""R\"" mean?""",1,1,1
It means that x_i is a 10-element vector of real numbers.,1,0,1
"Approximation methods that increase efficiency of this computation can be found also by querying Efficiently Indexing High Dimensional Data Spaces. Or even querying J.Hopcroft's lecture guides [here][1]. The base case for unit hypercube implied 1) computing boundary area 2) \slicing\"" the actual portion of the volume we're interested in. In other problems, side length of the n-cube would change as well or we may need to subtract its volume from that of an n-sphere first._x0007__x0007__x0007_  [1]: http://www.cs.cornell.edu/courses/cs683/2008sp/""",1,0,1
"<redacted>. Your basis expansion works!!! Thank you. Here's the updated code._x0007__x0007_    library(ISLR)_x0007_    _x0007_    salary <- Wage_x0007_    _x0007_    # b-spline with 2 knots_x0007_    _x0007_    reg.lin.spline <- lm(wage ~ bs(age, df=5), salary)_x0007_    _x0007_    attr(bs(salary$age, df=5), ",1,0,1
"Here you go.  You'll have to remove the blank lines yourself, as, for some reason, the only other with this web-based editor is to paste them in one continuous paragraph w/o any cr/lf's._x0007__x0007_https://www.youtube.com/watch?v=6l9V1sINzhE_x0007__x0007_https://www.youtube.com/watch?v=jk9S3RTAl38_x0007__x0007_https://www.youtube.com/watch?v=uQBnDGu6TYU_x0007__x0007_https://www.youtube.com/watch?v=DCn83aXXuHc_x0007__x0007_https://www.youtube.com/watch?v=jwBgGS_4RQA_x0007__x0007_https://www.youtube.com/watch?v=5ONFqIk3RFg_x0007__x0007_https://www.youtube.com/watch?v=6ENTbK3yQUQ_x0007__x0007_https://www.youtube.com/watch?v=GfPR7Xhdokc_x0007__x0007_https://www.youtube.com/watch?v=hPEJoITBbQ4_x0007__x0007_https://www.youtube.com/watch?v=lq_xzBRIWm4_x0007__x0007_https://www.youtube.com/watch?v=U3MdBNysk9w_x0007__x0007_https://www.youtube.com/watch?v=2cl7JiPzkBY_x0007__x0007_https://www.youtube.com/watch?v=9TVVF7CS3F4_x0007__x0007_https://www.youtube.com/watch?v=TxvEVc8YNlU_x0007__x0007_https://www.youtube.com/watch?v=6dSXlqHAoMk_x0007__x0007_https://www.youtube.com/watch?v=YVSmsWoBKnA_x0007__x0007_https://www.youtube.com/watch?v=3kwdDGnV8MM_x0007__x0007_https://www.youtube.com/watch?v=mv-vdysZIb4_x0007__x0007_https://www.youtube.com/watch?v=F8MMHCCoALU_x0007__x0007_https://www.youtube.com/watch?v=1REe3qSotx8_x0007__x0007_https://www.youtube.com/watch?v=1REe3qSotx8_x0007__x0007_https://www.youtube.com/watch?v=0wZUXtvAtDc_x0007__x0007_https://www.youtube.com/watch?v=IY7oWGXb77o_x0007__x0007_https://www.youtube.com/watch?v=QpbynqiTCsY_x0007__x0007_https://www.youtube.com/watch?v=xKsTsGE7KpI_x0007__x0007_https://www.youtube.com/watch?v=xKsTsGE7KpI_x0007__x0007_https://www.youtube.com/watch?v=dm32QvCW7wE_x0007__x0007_https://www.youtube.com/watch?v=mI18GD4_ysE_x0007__x0007_https://www.youtube.com/watch?v=2wLfFB_6SKI_x0007__x0007_https://www.youtube.com/watch?v=LvaTokhYnDw_x0007__x0007_https://www.youtube.com/watch?v=L3n2VF7yKkk_x0007__x0007_https://www.youtube.com/watch?v=L3n2VF7yKkk_x0007__x0007_https://www.youtube.com/watch?v=WjyuiK5taS8_x0007__x0007_https://www.youtube.com/watch?v=UvxHOkYQl8g_x0007__x0007_https://www.youtube.com/watch?v=VusKAosxxyk_x0007__x0007_https://www.youtube.com/watch?v=vVj2itVNku4_x0007__x0007_https://www.youtube.com/watch?v=PsE9UqoWtS4_x0007__x0007_https://www.youtube.com/watch?v=J6AdoiNUyWI_x0007__x0007_https://www.youtube.com/watch?v=1hbCJyM9ccs_x0007__x0007_https://www.youtube.com/watch?v=3T6RXmIHbJ4_x0007__x0007_https://www.youtube.com/watch?v=IFzVxLv0TKQ_x0007__x0007_https://www.youtube.com/watch?v=sqq21-VIa1c_x0007__x0007_https://www.youtube.com/watch?v=31Q5FGRnxt4_x0007__x0007_https://www.youtube.com/watch?v=MpX8rVv_u4E_x0007__x0007_https://www.youtube.com/watch?v=GavRXXEHGqU_x0007__x0007_https://www.youtube.com/watch?v=RfrGiG1Hm3M_x0007__x0007_https://www.youtube.com/watch?v=QG0pVJXT6EU_x0007__x0007_https://www.youtube.com/watch?v=X4VDZDp2vqw_x0007__x0007_https://www.youtube.com/watch?v=6FiNGTYAOAA_x0007__x0007_https://www.youtube.com/watch?v=_2ij6eaaSl0_x0007__x0007_https://www.youtube.com/watch?v=nZAM5OXrktY_x0007__x0007_https://www.youtu",1,0,0
"I basically ripped this from the Polynomials section of ch7.Rmd._x0007__x0007_    fit2 <- lm(y ~ x + I(x^2))_x0007_    x.grid <- seq(min(x), max(x), length.out=100)_x0007_    preds <- predict(fit2, newdata=list(x=x.grid), se=TRUE)_x0007_    se.bands <- cbind(preds$fit+2*preds$se, preds$fit-2*preds$se)_x0007_    plot(x, y, col=\darkgrey\"")_x0007_    lines(x.grid, preds$fit, lwd=2, col=\""blue\"")_x0007_    matlines(x.grid, se.bands, col=\""blue\"", lty=2)_x0007__x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>48.png""",1,0,1
"Much thanks to this excellent course, We have learnt a lot of statistical methods, some of which involve well-disigned mathematical formulae, while others rely on tree structure._x0007__x0007_I am just wondering, which methods can better deal with the Curse of Dimensionality, Taking into consideration the time and space complexity required to run these methods on massive and high-dimensional data ?_x0007__x0007_Thanks!",1,1,1
Hope this helps:_x0007_http://en.wikipedia.org/wiki/Real_coordinate_space,1,0,1
What is the grade requirement to earn a certificate of accomplishment?_x0007_Do I need to pass every single quiz?,1,1,1
"<redacted>:_x0007__x0007_> Lasso. However, as mentioned in the textbook, even the Lasso has_x0007_> problems when faced with a large number of variables._x0007__x0007_Could I ask, which part of the book was that ?_x0007_I got the impression that this was the sort of situation where lasso ",1,0,1
<redacted>_x0007__x0007_Thanks for the references._x0007_(Those slides from the talk were interesting)_x0007__x0007_Now I get what you mean.,1,0,0
I am glad that you took your precious time for making this available._x0007__x0007_Thanks,1,0,1
"ightarrow_x0007_%$_x0007_we are using 10 dimensions: _x0007_$%_x0007_x_i^{(1)},x_i^{(2)},x_i^{(3)},...,x_i^{(10)}_x0007_%$_x0007__x0007_If_x0007_$%_x0007_y=0_x0007_%$_x0007_then all of those have a normal distribution with _x0007_$%_x0007_mu=0_x0007_%$_x0007_and_x0007_$%_x0007_sigma=1_x0007_%$_x0007_$%_x0007_(N{(0,1)})_x0007_%$_x0007_._x0007__x0007_If _x0007_$%_x0007_y=1_x0007_%$_x0007_then _x0007_$%_x0007_x_i^{(1)},...,x_i^{(5)}_x0007_%$_x0007_have a normal distribution with $%_x0007_mu=1_x0007_%$_x0007_and_x0007_$%_x0007_sigma=1_x0007_%$_x0007_$%_x0007_(N{(1,1)})_x0007_%$_x0007_and _x0007_$%_x0007_x_i^{(6)},...,x_i^{(10)}_x0007_%$_x0007_have a normal distribution with $%_x0007_mu=0_x0007_%$_x0007_and_x0007_$%_x0007_sigma=1_x0007_%$_x0007_$%_x0007_(N{(0,1)})_x0007_%$_x0007_.""",1,0,1
It is not quite obvious explanation for me in answer section - why the standard error of vector of coefficients beta  decreases if one softens the margins and allows more error rate in classification._x0007__x0007_All valuable comments higly appreciated.,1,1,1
"Thanks, that has clarified it. I shall try 9.R.1, 9.R.2 and 9.R.3 accordingly.",1,0,0
"The '~\ sign used in the expressions confused me, especially because it is used in Review questions in segment on R (\""models\"" sign!)""",0,1,1
@imag:_x0007__x0007_> It is not quite obvious explanation for me in answer_x0007_> section - why the standard error of vector of coefficients_x0007_> beta decreases if one softens the margins and allows more_x0007_> error rate in classification._x0007__x0007_Maybe the title of your post should co,1,0,1
"That's certainly a fair suggestion, and tractable when you have only a few variables with a rather short tree.  Unfortunately, the best trees for my particular business problems tend to be quite tall with multi-way splits, which would result on a very long conditional probability statement of \when this and that and the other, and not this then....\"". _x0007_Nonetheless, you are right and I appreciate the reply.""",1,0,1
"Thanks, Dan. I realize more explanations are in order._x0007__x0007_The distinction highlighted by the Wikipedia quote exposes (I think) the difference between calculating the std error of the regression coefficients and fitted values._x0007__x0007_The std error of the regression coefficients can be directly calculated (in the sense of the first paragraph of the Wikipedia quote) from real data points for which $(X_i, Y_i)$ are known. The need for a bootstrap estimation in this case is not so pressing because a direct calculation with the actual errors can be done, almost assumption free._x0007__x0007_This is not possible for a single fitted point $(X_i,hat{Y_i})$, for which no error mean (neither RSS, etc) can be calculated, as we don't know the value $Y_i$, only $hat{Y_i}$._x0007__x0007_So, the std error for the fitted point is calculated as (2nd part of the Wikipedia quote) the standard deviation of what exactly?. The process of finding this \what exactly\"" and how the std error calculation is made, is what has resulted in the post above._x0007__x0007_I have found that the calculation depends heavily on the delta method and normality assumptions, so I think that using the bootstrap to directly calculate confidence intervals for the fitted value would be much more appropriate in some cases, better that relying on predict$.se, that might be meaningless._x0007__x0007_I hope is more clear now._x0007__x0007_Now, calculating the bootstrap estimation in the fitted point $X_i$ is perhaps not straightforward, as there is no initial set of observations $(X_i, Y_{i1}), (X_i, Y_{i2}), ... (X_i, Y_{im})$, from which to take the samples. We only have $(X_i,hat{Y_i})$._x0007__x0007_My thinking is that the procedure involves taking bootstrap samples from the training set; calculate the model for each bootstrap set; calculate $(X_i,hat{Y_i})$ for each model. The result would be a set of $(X_i,hat{Y_i})$ from which a confidence interval can be calculated.""",1,0,0
"R keeps the factor levels, so you have two options:_x0007__x0007_ - drop the unused factor levels after creating your subset: fb <- factor(fb)_x0007_ - don't factorize your columns when reading the csv: stringsAsFactors=FALSE",1,0,1
Thanks.,1,0,1
"Thank you for explaining the expression.  I am still having problem with this problem._x0007__x0007_I have generated a data frame with 100 points as described in the problem._x0007_I use the svm R function as explained in class without specifying any cost parameter._x0007__x0007_I am not quite sure what the next step is.  Am I supposed to generate a lot more test data with the same characteristics, use svmfit to predict and find the number of wrong predictions relative to the number of points and repeat the test 2-3 times.  How large is a large data set ?  Would generating 1000 points be considered \large data set\"" ?  I am not sure how other way could I find the expected error ?  _x0007__x0007_Would appreciate very much if someone could point me to the right direction. I already made 2 mistakes out of 5 trials.  Thanks""",1,1,1
How do you tell lm or glm to solve for a specific intercept?,1,1,1
Thanks McReyar but i still don't get the right answer. _x0007_is the error rate for radial and linear very close to each other?,1,1,1
"Hello,_x0007_As I understand, there are two tricks in the random forest method to reduce variance and ultimately to make better predictions on previously unseen data (compared to plain trees): bagging and random selection of features for each tree in the forest. _x0007__x0007_Can these 2 ideas be applied to other classifiers than trees ? Could it be transposed to logistic regression or even neural networks (assuming large computational power available) ???_x0007_If yes, would the gain in accuracy be comparable to the gain when going from plain tree to Random Forest ?_x0007_For Random Forest, it is said in the lecture that the trees should not be pruned, because thanks to the averaging, the low bias does not necessarily result in high variance. If we transposed the idea to logistic regression, would that mean we would carry out (hundreds of) logistic regressions **without penalization / regularization** on random features and then average them?_x0007__x0007_Thanks,_x0007__x0007_GL",1,1,1
"Thank you. But my another question is since when y=1, I want half of them has mean 0, half of them have mean 1. _x0007_> x=matrix(rnorm(50),5,2)_x0007_> y=rep(c(0,1),c(50,50))_x0007_> x[y=1,2]=x[y=1,2]+1_x0007_Is this the right code to interpret this question?",1,1,1
"Well, it seems wrong.",1,0,1
"I'm not sure why you think the standard errors for the coefficients are assumption-free?  They are based on an assumption of (at least approximate by an appeal to the CLT in large samples) normality.  The assumptions for the standard errors of the $hat{Y}$s is the same.  (EDIT: On a purely practical note, whenever you bother to find the standard error of a statistic, you probably should be assuming the sampling distribution of the statistic is approximately normal because otherwise the standard error is not a terribly useful summary of the shape of the distribution and hence not a good measure of the uncertainty in the statistic.)_x0007__x0007_Remember that in linear regression we are assuming that $hat{Y}=hat{f}(X)$ is a linear (in the parameters) function of $X$ (i.e., it is an estimate of $E[Y|X]$ under an assumption about the functional form: $E[Y|X]=f(X)$).  Once we have a set of data $(X_i,Y_i)$, we can estimate the parameters and hence the functional form $hat{f}(X)=hat{Y}$ as a function of $X$. Then, we can estimate the mean value of the response $Y$ on a new explanatory value $X_{new}$ as $hat{Y}_{new}=hat{f}(X_{new})$, based on the observed data $(X_i,Y_i)$.  Also based on the same observed data, we can estimate the standard error of $hat{Y}_{new}$ using the formula you gave in your first post above (which is based on the assumption of normality).  Alternately, we can estimate the standard error of $hat{Y}(X_{new})$ by taking lots (say $B$) of bootstrap samples of the observed data $(X_i,Y_i)$, fitting the model to each bootstrap sample, calculating $hat{Y}_{new,j}=hat{f}_j(X_{new})$ for $j=1cdots B$, and then finding the standard deviation of $hat{Y}_{new,j}$ for $j=1cdots B$.  _x0007__x0007_Note however that if the sampling distribution of $hat{Y}_{new}$ is not close to being normal (e.g., if the histogram of $hat{Y}_{new,j}$ is not well approximated by a normal distribution) , the standard error of $hat{Y}_{new}$ will not be useful as a summary of uncertainty (e.g., the CIs calculated from it will not have the nominal coverage).  In that case, it would be better to form a CI on $hat{Y}_{new}$ using quantiles of the sampling distribution.  On the other hand, if the sampling distribution is close to being normal, then (I suspect) the bootstrap standard errors and the calculated standard errors should be close to the same.  _x0007__x0007_Similar statements to the above apply to the standard errors and CIs for the coefficients as well.  That is, if the sampling distribution is close",1,0,0
"Hi,_x0007_I had no internet for a week, and tried to do R practices with downloaded videos. It is impossible to read the text. I have a Laptop with windows 7 and QuickTime Player 7.7.5 not professional. Downloaded videos are not useful. I don't know if classmates have also this problem.",0,1,1
Yes. See the randomGLM package.,1,0,1
"E means \belongs to\"" and R is the \""set of Real Numbers\"". So this could be read as xi belongs to the set of real numbers (in 10 dimensions, i.e. vector of length 10)""",1,0,0
"yeah I was trying it this way thinking we select the first 50 rows from multivariate normal (0,1)_x0007__x0007_    matrix(mvrnorm(500,rep(0,10),diag(10)),nrow=50,ncol=10)_x0007__x0007_and likewise for the other 50 rows(y=1) with multivariate normal(mu=(1,0),1)_x0007__x0007_    matrix(mvrnorm(500,c(rep(1,5),rep(0,5)),diag(10)),50,10)_x0007_then stacking them and combining with y vector of 50 0's and 50 1's_x0007__x0007_I fitted the model, predicted test error creating a test set similarly and averaged the result  but haven't got it right.",1,0,1
You get what you put into the course.  The content is rich in concepts and insights.  One can spend many hours per chapter to master the subject matter.  It's all up to the individual. _x0007__x0007_I really hope the site is available after the course is over.,1,0,1
"This is a great course that one can revisit time and time again to gain proficiency, particularly when combined with the ELements of Statistical Learning.  _x0007__x0007_Will the website remain open to students after the course is over?",1,1,1
"I've made this code to solve the 9.R.1 quiz but the value that's giving me it's wrong and I can't find where I messed up, does anybody know what's wrong here?_x0007__x0007_    svm_error <- function() {_x0007_      # 1) generate a random training sample to train on + fit_x0007_      _x0007_      # build training set_x0007_      x0 = mvrnorm(50,rep(0,10),diag(10))_x0007_      x1 = mvrnorm(50,rep(c(1,0),c(5,5)),diag(10))_x0007_      train = rbind(x0,x1)_x0007_      classes = rep(c(0,1),c(50,50))_x0007_      dat=data.frame(train,classes=as.factor(classes))_x0007__x0007_      # fit_x0007_      svmfit=svm(classes~.,data=dat,cost=10,scale=FALSE)_x0007_      _x0007_      # 2) evaluate the number of mistakes we make on a large test set = 1000 samples_x0007_      test_x0 = mvrnorm(500,rep(0,10),diag(10))_x0007_      test_x1 = mvrnorm(500,rep(c(1,0),c(5,5)),diag(10))_x0007_      test = rbind(test_x0,test_x1)_x0007_      test_classes = rep(c(0,1),c(500,500))_x0007_      test_dat = data.frame(test,test_classes=as.factor(test_classes))_x0007_      fit = predict(svmfit,test_dat)_x0007_      error = abs(500 - length(which(fit == 0)))/1000_x0007_      _x0007_      return(error)_x0007_    }_x0007_    _x0007_    # 3) repeat (1-2) many times and averaging the error rate for each trial_x0007_    errors = replicate(100, svm_error())_x0007_    _x0007_    print(errors)_x0007_    print(mean(errors))_x0007__x0007_Explanation:_x0007__x0007_ - The fit is made using 100 samples coming from X0 & X1 (each w/50 gaussian samples)_x0007_ - The test set is built using 1000 samples of the same distributions (500 of each class)_x0007_ - Error = (500 - #Y=0 predicted) / 1000 (by construction test set had 500 Y=0 + 500 Y=1)_x0007__x0007_Any feedback appreciated, thanks!",1,1,1
"Question 9 R1 starts \Use svm in the e1071 package with the default settings...\""_x0007__x0007_By default cost=1 !_x0007__x0007_I think that this line in your code is not correct:_x0007_svmfit=svm(classes~.,data=dat,cost=10,scale=FALSE)_x0007__x0007_try just _x0007__x0007_svmfit=svm(classes~.,data=dat)_x0007__x0007_Hope it will help. _x0007__x0007_Let me know whether it was the correct answers.""",1,0,1
Same here.  This is a real bummer!,0,0,1
"I've tried using the code in lecture and the book and here is what I have come up with:_x0007__x0007_       x=matrix(rnorm(1000),100,10)_x0007_       y=rep(c(0,1),c(50,50))_x0007_       x[y==1,1:5]=x[y==1,1:5]+1_x0007_    _x0007_       library(e1071)_x0007_       dat=data.frame(x,y=as.factor(y))_x0007_       svmfit=svm(y~.,data=dat,kernel=\radial\"",scale=FALSE)_x0007_    _x0007_       xtest=matrix(rnorm(1000), 100,10)_x0007_       ytest=sample(c(0,1), 100, rep=TRUE)_x0007_       xtest[ytest==1,1:5]=xtest[ytest==1,1:5] + 1_x0007_       testdat=data.frame(x=xtest, y=as.factor(ytest))_x0007_       ypred=predict(svmfit, testdat)_x0007_       table(predict=ypred, truth=testdat$y)_x0007__x0007_it seems to work until I get to the next to last line, the ypred statement.  R will spit out \""Error in eval(expr, envir, enclos) : object 'X1' not found\""  Is there any idea what might be wrong?  _x0007__x0007_Also, once I get this idea down, I will need to find the error and run it in a loop say 1000 times?_x0007__x0007_Thanks for any help!""",1,1,1
"Sorry I should have mentioned that the results I was describing are with the argument FUN=prune.misclass, which does return the sum of the number misclassed as the value of \dev\"" (and as is shown in the book example). _x0007__x0007_The discrepancy still remains and seems puzzling, any other ideas would be appreciated.""",1,0,1
"The data.frame testdat has not the same colnames as dat, so the svmfit doesn't recognize it when making predictions. _x0007__x0007_Check colnames(testdat), colnames(dat). You can simply use _x0007__x0007_    testdat=data.frame(xtest, y = as.factor(ytest))_x0007__x0007_or directly_x0007__x0007_    colnames(testdat) <- colnames(dat)",1,0,1
"Hi, thanks for the links! I will give caret a try",1,0,1
"You don't need to use multivariate normal, a multivariate normal with identity correlation is the same as 10 independent univariate normal, so you can sample it with rnorm directly",1,0,0
"you can just use compactly_x0007__x0007_mean(yhat != y)_x0007__x0007_to automatically calculate the fraction of \TRUE\"" over the whole logical vector.""",1,0,0
Thanks for your help. One of the subtleties that would have taken me a while to pick up.  I really appreciate it.,1,0,0
"Can I request Professor(s) or staff to provide the correct R-receipe when we click show answer?_x0007_There is lot of variation in the answers provided and our supplied answers, makes us wonder what went wrong, or what have we not understood correctly about the question._x0007__x0007_Thanking you in advance.._x0007_regards_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Here's an article about ensemble KNN classifiers. http://cs.gmu.edu/~carlotta/publications/NNensemble.pdf  Since KNN classifiers have high variance, you'd think that ensembles would be beneficial and could overcome their major weakness.",1,0,1
"<redacted>:_x0007__x0007_You could subtract the desired intercept from the response_x0007_then fit the model without an intercept (`-1` in the formula)_x0007_to get the slope._x0007__x0007_For example_x0007__x0007_    x <- seq(0.1,1,by=0.1)_x0007_    set.seed(10)_x0007_    y <- 2 + 3*x + 0.1*rnorm(x)_x0007_    data <- d",1,0,1
This was helpful. Thank you!,1,0,1
thanks I'll try this out,1,0,1
Can someone describe or provide examples to help understand what overfitting means in the context of classification problem?,1,1,1
"There is a terminology error at the very end of the intro video when they discuss Landsat classification. They incorrectly call \soil, cotton, stubble\"" land uses when they're really land covers. Land uses would be \""urban, agricultural, etc.\"" You can't actually determine land use directly from a satellite image.""",1,1,1
Much appreciated for your explanation,1,0,1
"How SVM behaves if the dataset has data in factor type?_x0007__x0007_Say, if I give SVM a dataset like this:_x0007__x0007_    x1 <- c(\MALE\"",\""FEMALE\"",\""FEMALE\"",\""FEMALE\"",\""FEMALE\"",\""MALE\"")_x0007_    x2 <- c(\""A\"",\""B\"",\""A\"",\""B\"",\""B\"",\""B\"")_x0007_    y  <- c(-1,1,-1,1,1,-1)  # y=1 for (x1=Female,x2=B)_x0007__x0007_    train <- data.frame(x1=as.factor(x1), x2=as.factor(x2), y=as.factor(y))_x0007__x0007_    fit <- svm(y~., data=train)_x0007__x0007_Does the function svm() translate the x1 and x2 columns into numbers? And if so, what numbers?_x0007__x0007_Thanks,""",1,1,1
Thanks guys. I learned more from you than from the lessons!!!,1,0,0
The whole AUC is 1x1=1_x0007_What are the probabilities of a completely random toss of a coin?,1,1,1
"What is the meaning of the *cost* parameter in the *svm()* function?_x0007__x0007_As it is described as the \cost of constraints violation\"", it sounds inversely related to the parameter *C* introduced in the lectures, the latter being the \""budget for the total amount of slack\""._x0007_Indeed, **increasing** *cost* seems to reduce the number of support vectors, which occurs also when *C* is **decreased**.""",1,1,1
**Ch01 Introduction**_x0007__x0007_1.1 Opening Remarks (https://www.youtube.com/watch?v=2wLfFB_6SKI)_x0007__x0007_1.2 Examples and Framework (https://www.youtube.com/watch?v=LvaTokhYnDw)_x0007__x0007__x0007_**Ch02 Overview of Statistical Learning**_x0007__x0007_Introduction to Regression Model (https://www.youtube.com/watch?v=WjyuiK5taS8)_x0007__x0007_2.2 Dimensionality and Structured Models (https://www.youtube.com/watch?v=UvxHOkYQl8g)_x0007__x0007_2.3 Model Selection and Bias-Variance Tradeoff (https://www.youtube.com/watch?v=VusKAosxxyk)_x0007__x0007_2.4 Classification (https://www.youtube.com/watch?v=vVj2itVNku4)_x0007__x0007_2.R Introduction to R (https://www.youtube.com/watch?v=jwBgGS_4RQA)_x0007_Interview with John Chambers (https://www.youtube.com/watch?v=jk9S3RTAl38)_x0007__x0007__x0007_**Ch03 Linear Regression**_x0007__x0007_3.1 Simple Linear Regression (https://www.youtube.com/watch?v=PsE9UqoWtS4)_x0007__x0007_3.2 Hypothesis Testing and Interval Confidence (https://www.youtube.com/watch?v=J6AdoiNUyWI)_x0007__x0007_3.3 Multiple Linear Regression (https://www.youtube.com/watch?v=1hbCJyM9ccs)_x0007__x0007_3.4 Some Important Questions (https://www.youtube.com/watch?v=3T6RXmIHbJ4)_x0007__x0007_3.5 Extensions of the linear models (https://www.youtube.com/watch?v=IFzVxLv0TKQ)_x0007__x0007_3.R Linear Regression in R (https://www.youtube.com/watch?v=5ONFqIk3RFg)_x0007__x0007__x0007_**Ch04 Classification**_x0007__x0007_4.1 Introduction to Classification Problems (https://www.youtube.com/watch?v=sqq21-VIa1c)_x0007__x0007_4.2 Logistic Regression (https://www.youtube.com/watch?v=31Q5FGRnxt4)_x0007__x0007_4.3 Multivariate Logistic Regression (https://www.youtube.com/watch?v=MpX8rVv_u4E)_x0007__x0007_4.4 Logistic Regression - Case Control Sampling and Multiclass (https://www.youtube.com/watch?v=GavRXXEHGqU)_x0007__x0007_4.5 Discriminant Analysis (https://www.youtube.com/watch?v=RfrGiG1Hm3M)_x0007__x0007_4.6 Gaussian Discriminant Analysis - One Variable (https://www.youtube.com/watch?v=QG0pVJXT6EU)_x0007__x0007_4.7 Gaussian Discriminant Analysis - Many Variable (https://www.youtube.com/watch?v=X4VDZDp2vqw)_x0007__x0007_4.8 Quadratic Discriminant Analysis and Naive Bayes (https://www.youtube.com/watch?v=6FiNGTYAOAA)_x0007__x0007_4.R Classification in R (https://www.youtube.com/watch?v=TxvEVc8YNlU)_x0007__x0007__x0007_**Ch05 Resampling Methods**_x0007__x0007_Interview with Bradley Efron (https://www.youtube.com/watch?v=6l9V1sINzhE)_x0007__x0007_5.1 Cross-Validation (https://www.youtube.com/watch?v=_2ij6eaaSl0)_x0007__x0007_5.2 K-fold Cross-Validation (https://www.youtube.com/watch?v=nZAM5OXrktY)_x0007__x0007_5.3 Cross-Validation: the wong and right way (https://www.youtube.com/watch?v=S06JpVoNaA0)_x0007__x0007_5.4 The Bootstrap (https://www.youtube.com/watch?v=p4BYWX7PTBM)_x0007__x0007_5.5 More on the Bootstrap (https://www.youtube.com/watch?v=BzHz0J9a6k0)_x0007__x0007_5.R Resampl,1,0,1
"You don't need to consider a multivariate normal, as the identity correlation matrix implies they are independent. Works also with multivariate normal...",1,0,1
"I am a bit confused about the relationship between \C\"" and \""margin\"" for the support vector classifier._x0007_Quoting below some statements from the book. They appear contradicting as far as the relationship between C and margin is concerned._x0007__x0007_**Section 9.2.2**: As the budget C increases, we become more tolerant of_x0007_violations to the margin, and so the margin will widen. Conversely, as C_x0007_decreases, we become less tolerant of violations to the margin and so the_x0007_margin narrows._x0007__x0007_**Section 9.6.1**: Now that a smaller value of the cost parameter is being used, we obtain a_x0007_larger number of support vectors, because the margin is now wider.""",1,1,1
Thank you all for your tips!,1,0,1
"Thanks, MooMoo, that was very clear.",1,0,1
Thanks,1,0,1
"Hi,_x0007_I have found slides about \Pre-validation\"" and \""Permutation tests\"" at the very end of Chapter #5 slides, however there are no such materials in video lectures. Am I missing something?""",1,1,1
"My take-away from this conversation so far is when inference is the reason for the analysis, we should be aware of our model assumptions. The end of Chapter 3 provides a discussion on the various issues that arise when using the linear model for inference. CIs and significance of parameter estimates are definitely desired here. However, in the prediction setting, we are willing to relax and bias our parameter estimates in favor of a more accurate predictor model.",1,0,0
now I can. Thanks a lot,1,0,1
"It seems a simple rule of thumb is never use the response variable as a way to prescreen predictors or to categorize predictors. Use something that makes sense based on the subject matter or practicality as it relates to the predictor. For example, we are prescreening as p is large and the opportunity for correlation between the predictors is high. Which predictors are correlated can be found and presented to SME for their input. It could be the case that between two predictors that are correlated, one costs less to produce or is easier obtainable. Another way to prescreen is based on statistics such as was exemplified by our professors.",1,0,0
"exactly my point - *C* and *cost* are not the same, rather they have opposite effects.",1,0,1
"I got lost at \Monte Carlo'..._x0007_It's a matrix too far for me.""",1,0,1
"About chosing the polynomial degree, there's an example at the R video at the 5th Unit. They show you how to use CV for chosing the polynomial degree.",1,0,1
"if you are using R for analysing your data, you can also use the gam() function (at the gam package). That function gives you an idea about the polynomic degree of each variable.",1,0,0
"Lets try dissecting the question_x0007__x0007_\In this problem, you will use simulation to evaluate (by Monte Carlo) the expected misclassification error rate given a particular generating model.\""_x0007__x0007_*<zipRedac>) We first identify that the error of interest is the misclassification error. That is; number of wrong classification over total number of classification*_x0007__x0007__x0007_\""Let yi be equally divided between classes 0 and <zipRedac>, and let xi?R<zipRedac>0 be normally distributed.\""_x0007__x0007_*2) This says that we have 2 different kinds of responses, 0 and <zipRedac>. Each of these responses can be explained by <zipRedac>0 independent/explanatory variables.*_x0007__x0007_\""Given yi=0, xi?N<zipRedac>0(0,I<zipRedac>0).  Given yi=<zipRedac>, xi?N<zipRedac>0(?,I<zipRedac>0) with ?=(<zipRedac>,<zipRedac>,<zipRedac>,<zipRedac>,<zipRedac>,0,0,0,0,0). \""_x0007__x0007_*3) This tells us the distribution of the <zipRedac>0 independent/explanatory variables. Corresponding to the responses y=<zipRedac>, we have that the x's are distributed according to a multivariate normal (mvn) distribution with mean being a <zipRedac>0 by <zipRedac> vector ? and variance equal to the <zipRedac>0 by <zipRedac>0 identity matrix. Similarly, corresponding to the responses y = 0, we have that the x's are distributed according to a mvn distribution with mean being a <zipRedac>0 by <zipRedac> vector of zeros and variance equal to the <zipRedac>0 by <zipRedac>0 identity matrix.*_x0007__x0007_\""Now, we would like to know the expected test error rate if we fit an SVM to a sample of 50 random training points from class <zipRedac> and 50 more from class 0.  We can calculate this to high precision by <zipRedac>) generating a random training sample to train on, 2) evaluating the number of mistakes we make on a large test set, and then 3) repeating (<zipRedac>-2) many times and averaging the error rate for each trial.\""_x0007__x0007_4) _x0007_This describes how a typical training data should look like. The training data should contain <zipRedac>00 observations:_x0007__x0007_50 observations with responses yi = 0, and <zipRedac>0 xi?N<zipRedac>0(0,I<zipRedac>0)_x0007__x0007_50 observations with responses yi = <zipRedac>, and <zipRedac>0 xi?N<zipRedac>0(?,I<zipRedac>0)_x0007__x0007_(train data should have <zipRedac>00 rows and <zipRedac><zipRedac> columns)_x0007__x0007_(hint: use mvrnorm in MASS package)_x0007__x0007_We can generate one test data with say a large enough observation <zipRedac>0000 such that_x0007__x0007_5000 observations with responses yi = 0, and <zipRedac>0 xi?N<zipRedac>0(0,I<zipRedac>0)_x0007__x0007_5000 observations with responses yi = <zipRedac>, a""",1,0,0
"Hi!_x0007__x0007_just a small question... every time that I am answering a question in a quiz, after saving my answer I have this message \Your answers have been saved but not graded. Click 'Check' to grade them.\"". But I fon't know how to \""check\"" my answer. Can you please help me? is it really important for the evaluation of the work?_x0007__x0007_Thank you!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"So, does this mean that if we have a 50% on the \total\"" score, the column on the right extreme in the progress chart, then we have passed the course?  Is there another place, other than just this one line on the progress, which explains the grading policy?""",1,1,1
"If you click \submit\"" they will immediately tell you if the answer is right or wrong. You can generally have up to 5 attempts. And once you have gotten the right answer or exhausted your attempts, you can click a button that explains the answer.""",1,0,1
"Thanks for the lengthy explanation.  I understood what was conceptualy required, what is missing is how to build the training data set._x0007__x0007_It appears that I should have been aware of a package in MASS (has it come up previously?) that would help solve the problem.  Which makes my point rather nicely that there was too big a conceptual leap here without a more structured example in the videos on how to create this kind of data set.",0,1,1
"Dear all,_x0007__x0007_At present, I am struggling with the final question of quiz 9.R on how to apply logistic regression in order to calculate the error rate within <zipRedac>0%._x0007__x0007_Below is the script, which I have managed to set up. _x0007_However, I am unable to obtain the correct answer._x0007_Your feedback is appreciated._x0007__x0007_Best wishes,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>_x0007__x0007_lg_error <- function() {_x0007__x0007_set.seed(<zipRedac>0<zipRedac><zipRedac><zipRedac>)_x0007__x0007_x0 = mvrnorm(50,rep(0,<zipRedac>0),diag(<zipRedac>0))_x0007__x0007_x<zipRedac> = mvrnorm(50,rep(c(<zipRedac>,0),c(5,5)),diag(<zipRedac>0))_x0007__x0007_train = rbind(x0,x<zipRedac>)_x0007__x0007_classes = rep(c(0,<zipRedac>),c(50,50))_x0007__x0007_dat=data.frame(train,classes=as.factor(classes))_x0007__x0007__x0007_glmfit=glm(classes~.,data=dat,family=binomial)_x0007__x0007_test_x0 = mvrnorm(500,rep(0,<zipRedac>0),diag(<zipRedac>0))_x0007__x0007_test_x<zipRedac> = mvrnorm(500,rep(c(<zipRedac>,0),c(5,5)),diag(<zipRedac>0))_x0007__x0007_test = rbind(test_x0,test_x<zipRedac>)_x0007__x0007_test_classes = rep(c(0,<zipRedac>),c(500,500))_x0007__x0007_test_dat = data.frame(test,test_classes=as.factor(test_classes))_x0007__x0007_fit = predict(glmfit,test_dat)_x0007__x0007_error = sum(fit !=test_dat$test_classes)_x0007__x0007_return(error)_x0007__x0007_}",1,1,1
"Hello,_x0007__x0007_I find the videos far more viewable at 1.5x speed, but I am no longer able to get the videos to display at that speed.  Was that a deliberate change?_x0007__x0007_Thank you,_x0007_Greg",1,1,1
"A recent paper was published in which the authors wanted to create a scoring system for predicting recurrent cdiff disease. http://www.ncbi.nlm.nih.gov/pubmed/<phoneRedac>0_x0007__x0007_Here is there methods:_x0007_77 baseline CDI factors were classified: demographics, co-morbidity, medications, vital signs, laboratory tests, severity and symptoms. Predictors with the highest discrimination in each class (using Receiver Operating Characteristics Curve) were selected. For the final model, stepwise selection was performed._x0007__x0007_Question: Is it valid to run stepwise selection (of pre-selected features) on the full data set after you pre-select features using the same full dataset? Or, is this confounded in a way that is somehow analogous to the problem discussed in lecture in which cross-validation is confounded when performing it on pre-selected features that were selected on the full dataset? _x0007__x0007_I guess the question is, even if the performance of the model is inflated according to cross-validation on pre-selected features, can you still use cross-validation (or step-wise approaches) to compare different models to select the best one? Apologies in advance if this question needs clarification...",1,1,1
"It would seem that the idea of using kernels to move to a higher dimensional space might get around one potential problem with trees, i.e. the boundaries are parallel to the coordinate axes._x0007__x0007_Has this been tried anywhere?",1,1,1
"I've enjoyed the exchange as well.  You've forced me to attempt to state clearly some things that I had previously only half thought about (I've never formally studied statistics; I'm self-taught; so this has been very educational for me).  You're right that we agree on most things now.  I think we still disagree on the fitted values, though.  As stated in the stackexchange quote in your first post in this thread, if you're willing to assume that the coefficients are normally distributed (say, by appealing to the CLT) then the fitted values will be normally distributed, as well, and pointwise confidence intervals formed by standard errors are a reasonable thing (of course, that's different than saying that the prediction interval will given by the predict function will be valid; that relies on the normality of the irreducible error).  So, rather than arguing that point with more math, I went ahead and continued the R code example you gave in your first post.  I found it educational, so I'll try posting it here in case you do too:_x0007__x0007_    require(MASS)_x0007_    fit1<-lm(medv~lstat,data=Boston)_x0007_    summary(fit1)_x0007_    hist(resid(fit1))_x0007_    qqnorm(resid(fit1))_x0007_    qqline(resid(fit1))_x0007_    # residuals do not look at all normal_x0007_    #_x0007_    # now, let's extract some stuff from the summary object_x0007_    (beta<-coef(fit1))_x0007_    # coefficients_x0007_    (sigma<-summary(fit1)$sigma)_x0007_    # residual standard deviation_x0007_    (dof<-summary(fit1)$df[2])_x0007_    # residual degrees of freedom_x0007_    #_x0007_    # Now, let's get an estimate on a new point_x0007_    #_x0007_    sum(c(1,4.98)*beta)_x0007_    # estimate on new datum lstat=4.98_x0007_    sqrt(t(c(1,4.98)%*%vcov(fit1)%*%c(1,4.98)))_x0007_    # SE of estimate, based on normality of betas_x0007_    #_x0007_    # first, let's look at pointwise condfidence interval:_x0007_    #_x0007_    predConf<-predict(fit1,newdata=data.frame(lstat=4.98),se.fit=TRUE,interval=\confidence\"")_x0007_    predConf$fit_x0007_    # agrees with above estimate; see below for CI_x0007_    predConf$se.fit_x0007_    # agrees with above SE_x0007_    predConf$fit[1]+qt(c(0.025,0.975),dof)*predConf$se.fit_x0007_    # agrees with CI; should be good if betas approximately normal _x0007_    #_x0007_    # Now, let's look at prediction intervals:_x0007_    #_x0007_    predPred<-predict(fit1,newdata=data.frame(lstat=4.98),se.fit=TRUE,interval=\""prediction\"")_x0007_    predPred$fit_x0007_    # estimate is the same; see below for interval_x0007_    predPred$se.fit_x0007_    # SE of estimate is the same_x0007_    predPred$residual.scale_x0007_    # this is the same as sigma_x0007_    predPred$df_x0007_    # this is the same as dof_x0007_    predPred$fit[1]+qt""",0,0,1
"I too found the description of the test data set overly convoluted and technical. Once I'd figured out what they meant, it wasn't actually that far away from the example used in the video._x0007__x0007_It's a terminology thing, I'm just not at all familiar with the statistical terminology and they haven't spent a lot of time on basic statistics - but then it was listed as a pre-requisite for the course, so I suppose that's fair enough._x0007__x0007_For reference: I did this in a very similar way to the video, but with some added looping to add the repeated tests. I did not use the MASS package as the standard rnorm works fine.",1,0,1
"Note that one thing this shows is that I was wrong in thinking that the bootstrap standard errors should be about the same size as the theoretical ones if the sampling distributions of the statistics are normal.  For both the coefficients and the fitted values in this example, the bootstrap standard errors were quite a bit bigger than the theoretical ones, despite asymptotically normality of the sampling distributions looking like a good approximation.",1,0,0
"The FAQ for installing R on windows strongly implies that it is possible to install without admin privileges. It states: \To install use ‘R-3.0.3-win.exe’. Just double-click on the icon and follow the instructions. If you have an account with Administrator privileges you will be able to install R in the Program Files area and to set all the optional registry entries; otherwise you will only be able to install R in your own file area. You may need to confirm that you want to proceed with installing a program from an 'unknown' or 'unidentified' publisher.\""_x0007__x0007_So at the risk of suggesting something you've already tried, download and run R-3.0.3-win.exe.   Installer should ask where you want program installed, you can specify path to your USB drive._x0007__x0007_If not asked for in installer, for some reason, can also specify path in command line.   See http://cran.r-project.org/bin/windows/base/rw-FAQ.html""",1,0,1
How would you handle that issue if you wanted to use the methods from chpt 9?  How does something like `lm()` handle it?,1,1,1
"C is what is known as a constraint (we don't let the sum of epsilons exceed C)._x0007__x0007_cost is done as a penalty (you add cost*$sumepsilon_i$ to the objective)._x0007__x0007_As an analogy, C is to cost as s is to $lambda$ (from ridge or lasso).",1,0,0
"Try fit2 = ifelse(fit>.5,1,0) then_x0007_error=sum(fit2!=data$test_classes)/1000",1,0,1
"Just found this topic!_x0007__x0007_The ones who want more R practice with real-life data sets, I would recommend \The Analytics Edge\"" by MIT at edx: _x0007_http://www.edx.org/course/mitx/mitx-<zipRedac>5-07<zipRedac>x-analytics-edge-<zipRedac>4<zipRedac>6_x0007_It is currently running. If you join now, you've missed only one week, but it is small % of the grade -> you can still get you certificate. _x0007__x0007_For more rigor theory on machine learning (VC dimension, etc.) and language-agnostic practice exercises, The Learning from data by Caltech was great! _x0007_http://www.edx.org/course/caltechx/caltechx-cs<zipRedac><zipRedac>56x-learning-data-<zipRedac><zipRedac>20_x0007_You can wait for the next session, or take it at you own pace at iTunes-U._x0007__x0007_For the Bayesian methods, I heard that Probabilistic Graphical Models by Coursera is quite advanced: http://www.coursera.org/course/pgm_x0007__x0007_For those that want to learn more R, consider the Data Science specialization at Coursera: http://www.coursera.org/specialization/jhudatascience/<zipRedac>/courses _x0007__x0007_For Python/iPython community, you can go through videos from a Harvard Data Science course _x0007_http://cm.dce.harvard.edu/20<zipRedac>4/0<zipRedac>/<zipRedac>4328/publicationListing.shtml_x0007_http://www.cs<zipRedac>09.org/""",1,0,1
"Yes, it happened with the several lectures I had previously watched, on two different browsers, connecting from two different networks._x0007__x0007_However, on a third browser, it now works.  Sigh.  Thank you for your response.",0,0,1
"Good day everyone,_x0007__x0007_I understand that sequential supervised learning is different from times series [\Machine Learning for Sequential Data: A Review\"" by Dietterich]. How can I apply bootstrap for these 2 different cases?_x0007__x0007_Thanks!""",1,1,1
"(not part of assignment or quiz)_x0007_I am trying to run the predict() function on the boost variable coming from:_x0007_boost.carseats=gbm(Sales~.,data=Carseats[train,],distribution=\gaussian\"",n.trees=<zipRedac>0000,shrinkage=0.0<zipRedac>,interaction.depth=4)_x0007__x0007_what I've tried is: boost.pred=predict(boost.carseats,Carseats[-train,],type=\""class\"")_x0007_and get errors:_x0007_Error in paste(\""Using\"", n.trees, \""trees..._x0007_\"") : _x0007_  argument \""n.trees\"" is missing, with no default_x0007__x0007_I'd like to compare the results of the Boost predictive in a Confusion Matrix to compare how well it does against the Tree and Random Forest methods. Any tips on this?""",1,1,1
"Edit: I realized my response did not address your primary problem.  You just need to specify n.trees (it seems odd to me predict does not default to what was computed by the model).  For example, from the ch8.Rmd sample code:_x0007__x0007_    n.trees=seq(from=<zipRedac><zipRedac><zipRedac>,to=<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>,by=<zipRedac><zipRedac><zipRedac>)_x0007_    predmat=predict(boost.boston,newdata=Boston[-train,],n.trees=n.trees)_x0007_    dim(predmat)_x0007_    berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))_x0007_    plot(n.trees,berr,pch=<zipRedac>9,ylab=\Mean Squared Error\"", xlab=\""# Trees\"",main=\""Boosting Test Error\"")_x0007_    abline(h=min(test.err),col=\""red\"")_x0007_    min(berr)_x0007__x0007_I'm using caret to do confusion matrix comparisons for a variety of algorithms.  Attached is some example output.  I think you can just use the caret confusionMatrix function by itself.  For example:_x0007__x0007_    confusionMatrix(caretPreds$pred, caretPreds$obs)_x0007__x0007_    ## [<zipRedac>] \""=====================================\""_x0007_    ## [<zipRedac>] \""Model earthFit<zipRedac> of type earth\""_x0007_    ## Confusion Matrix and Statistics_x0007_    ## _x0007_    ##           Reference_x0007_    ## Prediction Male Female_x0007_    ##     Male    523     7<zipRedac>_x0007_    ##     Female   59    56<zipRedac>_x0007_    ##                                        _x0007_    ##                Accuracy : <zipRedac>.894        _x0007_    ##                  95% CI : (<zipRedac>.875, <zipRedac>.9<zipRedac>)_x0007_    ##     No Information Rate : <zipRedac>.52         _x0007_    ##     P-Value [Acc > NIR] : <2e-<zipRedac>6       _x0007_    ##                                        _x0007_    ##                   Kappa : <zipRedac>.787        _x0007_    ##  Mcnemar's Test P-Value : <zipRedac>.379        _x0007_    ##                                        _x0007_    ##             Sensitivity : <zipRedac>.899        _x0007_    ##             Specificity : <zipRedac>.889        _x0007_    ##          Pos Pred Value : <zipRedac>.882        _x0007_    ##          Neg Pred Value : <zipRedac>.9<zipRedac>5        _x0007_    ##              Prevalence : <zipRedac>.48<zipRedac>        _x0007_    ##          Detection Rate : <zipRedac>.43<zipRedac>        _x0007_    ##    Detection Prevalence : <zipRedac>.489        _x0007_    ##       Balanced Accuracy : <zipRedac>.894        _x0007_    ##                                        _x0007_    ##        'Positive' Class : Male         _x0007_    ##                                        _x0007_    ## [<zipRedac>] \""=====================================\""_x0007_    ## [<zipRedac>] \""Model gbmFit<zipRedac> of type gbm\""_x0007_    ## Confusion Matrix and Sta""",1,0,1
"Thank you for the heads-up! I've checked with R code, and indeed the test curve should be above the oob one. This kind of fits in the usual pattern, when training/cv error is smaller than the test one.",1,0,1
"One way to implement this is base R is to create new variable breaking each salary into distinct categories, then plot each successively using par(new=T)_x0007__x0007_    x <- rnorm(100, mean=50, sd=25)_x0007_    x1 <- cut(x, breaks=3, labels=c(\low\"", \""med\"", \""high\""),ordered_result=TRUE)_x0007_    plot(x[x1=='low'], ylab=\""\"", col=\""green\"", pch=19, ylim=c(min(x), max(x)), xlim=c(1,60))_x0007_    par(new=T)_x0007_    plot(x[x1=='med'], ylab=\""\"", col=\""yellow\"", pch=19, ylim=c(min(x), max(x)), xlim=c(1,60))_x0007_    par(new=T)_x0007_    plot(x[x1=='high'], ylab=\""\"", col=\""red\"", pch=19, ylim=c(min(x), max(x)), xlim=c(1,60))""",1,0,0
"I think the standard error is too large. Isn't the standard error $	ext{sd(RMSE)}/sqrt{n}$? I'm not sure, but in any event it doesn't make sense to have it depend on the number of folds -- which I'm guessing is what is meant by $sqrt{10}$ in that line. _x0007__x0007_Fore one reason in LOOCV the folds is $n$, and the variance of the error is not necessarily smaller for LOOCV than for 5- or 10-fold... in fact I believe the profs said it is higher and thus they prefer 5- or 10-fold cross-validation. _x0007__x0007_Anyway, if you replace `sqrt(10)` by `sqrt(nrow(Hitters))` in calculation of of standard error you'll get a picture like this. Note it identifies $6$ as the simplest model within range of the minimum. _x0007__x0007_![enter image description here][1]_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>5.png",1,1,1
">  I've Googled several ways of doing it, but none I liked._x0007__x0007_You didn't like this? I thought the base R answer was fine._x0007__x0007_http://stackoverflow.com/questions/<phoneRedac>/colour-points-in-a-plot-differently-depending-on-a-vector-of-values_x0007__x0007_Of course, you should just use `ggplot2` for continuous color scales... it makes it really easy.",1,0,1
"Yes, that was the problem.  Thanks.",1,0,1
"Thanks! That result looks a lot more sensible to me.  As I tried to intuit my way through things, I decided to take the SD of the RMSE based on the 10 folds (of the k-fold cross validation) as my numerator. Since I used the 10 k-fold values in the numerator, I use an n of 10 in my denominator.  While this seemed like a sensible thing to me (at the time), I believe that you have hit the proper method of estimating the SE.",1,0,0
"Yellowknife, Canada",1,0,1
Thanks - did this.  _x0007__x0007_Actually not so disimilar to the code I had previously.  _x0007__x0007_Problem is I still get an answer that comes out close to 0.5 and nowhere near the required answer.,1,1,1
"I've never seen a combination of kernels and trees, although you could use an [approximate expansion of the kernel's feature map][1] and fit a tree or tree ensemble to that. If this problem of trees really bothers you, I'd suggest doing a PCA to realign the axes (and maybe throw away some noise) instead._x0007__x0007__x0007_  [1]: http://peekaboo-vision.blogspot.nl/2012/12/kernel-approximations-for-efficient.html",1,0,0
"Hi ,_x0007_I am from India and have been working in the field of Computer Science for more than 15 years . Have been looking for the right online Stats course for a while and was thrilled to find this one.",1,0,1
"IIRC there are a few notes a bit like \this would be computationally too expensive\"", and these questions are to force you to calculate the cost.  Wikipedia or other sources for combinations/permutations can help.  When I get stuck I like to work out the problem for examples of 1, 2, 3, 4.""",1,0,1
"The result of 5.5.R.1 does not require any claim about orderings._x0007__x0007_First, attach to each of the points in the data set a unique label and think of them as uniquely colored balls in a box. Then, to obtain one bootstrap sample of size n, we independently sample n times from the box with replacement. Since the probability of not selecting one particular ball is (n-1)/n, the result follows as a consequence of n independent trials. _x0007__x0007_On the other hand, you may be concerned as to why we allow two sets, 1,2,3 and 2,1,3 to be 2 different bootstrap samples. In fact, the bootstrap procedure treats each of the B samples differently and equally. The occurrence of (1,2,3) and (2,1,3) among the set of samples simply indicates that the histogram/distribution of the points 1, 2 and 3 are rather uniform resulting in many identically looking samples. Intuitively, if you are monitoring pairs of consecutive coin flips, it wouldn't be surprising to see many Head-Head sequences._x0007__x0007_Hope this helps(:",1,0,0
"Agree that the answer was correct (I got it right on the first try :-).  My suggestion was that the explanation was unclear.  Not a big deal, just trying to improve the materials.",1,0,1
"It is so helpful, but the only problem for me now is that I do not know how to calculate error rate. Can anyone help me with that?",1,1,1
"Okay, just realized I need to install BeautifulSoup but I'm running under Windows 7 and new to Python.  Can someone tell me the steps needed to install it?  I have Python installed already and this seems to be what I need to get things working.",1,1,1
"Sorry I'm a bit behind on the course, hope someone still sees this.  _x0007_I was wondering what is the use of Ridge and Lasso if they produce bias estimators. Why is the fact that the estimators have reduced variance more important than the increase in bias. _x0007__x0007_I'm an ecologist and I can understand that an estimator that indicates the \direction\"" of an effect (factor effect) is important even if this estimator is biased. However, if the bias is too large, then the direction or importance of an effect resulting from ridge regress may lead to false or incorrect conclusions._x0007_Am I understanding this right?_x0007_Thanks for any comment_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>,""",1,1,1
Figured it out.  Here's the link:_x0007__x0007_http://stackoverflow.com/questions/1<phoneRedac>/how-can-i-install-the-beautiful-soup-module-on-the-window-7,1,0,1
"Ok. Let's say that after apply predict() with the model fitted you got a vector, that i will call response, full with 0's and 1's. _x0007__x0007_To compare the value of two vectors, use for example:_x0007__x0007_> y == response_x0007__x0007_the output will be something like TRUE FALSE TRUE and so on, and you will have as much logical outputs as the length of your vectors.  _x0007__x0007_If you sum:_x0007__x0007_> sum(y == response) _x0007__x0007_you will have the number of matches, that could be 997. In this case to get the error rate, just do:_x0007__x0007_> 1 - sum(y == response)/1000",1,0,0
"I did it with summary(tree()),is it right",1,0,1
"Here you want to produce a prediction model only. You are not interested in some true, causal effect of a variable (you need the right model for it anyway!), i.e. you dont want to draw conclusions. The idea of ridge and lasso is purely technical in this sense: improve the prediction model. Why the bias-variance trade-off works is explained well in the lecture.",0,0,0
"Mahorino's answer is correct. Test set error is the sum of squared bias, variance, and irreducible error. You could reduce test set error by reducing variance (by fitting a model with fewer independent variables--like stepwise or lasso--or by shrinking the coefficients--like ridge). However, reducing variance comes with a cost, higher bias._x0007__x0007_To your point about interpreting coefficient estimates or 'directions'. I refer you to slide 17 of Ch 3._x0007__x0007_> \The only way to fi_x000C_nd out what will happen when a complex system is_x0007_> disturbed is to disturb the system, not merely to observe it_x0007_> passively\"" --Fred Mosteller and John Tukey, paraphrasing George Box_x0007__x0007_Unless your data comes from a well-designed experiment, any causal inferences from a regression model (linear, logistic, lasso, ridge) should be taken with a grain of salt. Another problem with interpreting coefficients is multicollinearity._x0007__x0007_In other words, if you care about interpreting your coefficients:_x0007__x0007_ 1. Design an experiment_x0007_ 2. Analyze the experiment results with Design-of-Experiment techniques""",1,0,0
Alternatively I like to use the Flash Video Downloader add-in for FF.,1,0,1
"Will the course materials remain accessible to course participants flowing March 21st?  Or, should we download all pdfs and videos we might like to keep for future reference?  Thank you!",1,1,1
"Hi, _x0007__x0007_I am just starting the course and wanted to know till when the instructors plan on keeping the course materials available. Also will I be able to attempt the quizzes even after the Mar-21 deadline. I understand I will not get credit for the quizzes, but it would be nice to know how I did if the quiz checker will still be on. _x0007__x0007_Thanks,_x0007_<nameRedac_<anon_screen_name_redacted>>.",1,1,1
"David_L,_x0007_btw, what is your background, if you don't mind?  I study statistics in grad school, I am still struggling with a lot of these stuff._x0007__x0007_Maybe it's due to weak math background.",1,0,1
"Hi,_x0007__x0007_And when will the course be available again to get a certificate? Thanks._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"I think your original code is correct. When computing the standard error of the mean, you divide by the square root of the number of observations of the mean, not the number of samples the mean was taken over. This makes sense intuitively - if you have more observations of the mean, your confidence in the true value improves._x0007__x0007_If you look at the values of `rmse.mat[,12]` there is considerable variation so a standard error of 25 doesn't seem unreasonable._x0007__x0007_See page 18 of these notes for an example: http://www.stat.cmu.edu/~ryantibs/datamining/lectures/18-val1-marked.pdf",1,0,0
"I don't know, this looks fine to me: _x0007__x0007_    x <- rnorm(1000)_x0007_    y <- x + I(x^2) + rnorm(1000, mean=0, sd=0.5)_x0007_    plot(x, y)_x0007_    fit<- smooth.spline(x,y,cv=TRUE)_x0007_    pred <- predict(fit)_x0007_    str(pred)  # inspect this object_x0007_    lines(pred$x, pred$y, col=\blue\"", lwd=3)""",1,0,1
"you must include 'type=\response\""' to get probabilities (see ?predict.glm)_x0007__x0007_Like this:_x0007__x0007_fit <- ifelse(predict(glmfit,test_dat,type=\""response\"")>0.5,1,0)""",1,0,1
"Ideally, given a sample size n, we would like to evaluate estimates based on all n observations. However, in cross validation, we are leaving some part of the sample out in every iteration. As a result, the effective sample size shrinks. The correction made is to account for any deviation away from the total size n._x0007__x0007_Example:_x0007__x0007_5      - fold cross validation means we are always using    4 parts (80% of n)_x0007__x0007_10     - fold cross validation means we are always using    9 parts (90% of n)_x0007__x0007_n-1    - fold cross validation means we are always using (n-1) parts (99% of n)_x0007__x0007__x0007_Noting that n-1  - fold is just the LOOCV case, we see that there is still a shortfall from the ideal size n. Hence the bias correction follows. In addition, it is not too difficult to see that the difference in deltas from cv.glm increase K decreases_x0007__x0007_Personally, i don't find the two estimates too different except in the case where you need to present your findings. Then again, your audience's preference for correction is largely subjective._x0007__x0007_Hope this helps ~",1,0,0
"Hi there, perhaps there are 3 popular cases relating to \Forward Orthogonal Stepwise Selection\"" that you mentioned_x0007__x0007_Case 1) The independent/explanatory variables (columns of design matrix x) are orthogonal for some strange reason to begin with. Then performing the usual stepwise selection will do._x0007__x0007_Case 2) The independent/explanatory variables (columns of design matrix x) are not orthogonal and you wish to find an orthogonal basis (another p columns that are orthogonal to each other) such that it spans the same column space as the original matrix x before performing stepwise selection. To do so, you may consider the method of principal component analysis (PCA). Practically speaking, if you have a design matrix x with n rows of observation and p columns of variables. simply do, \""x.orth = predict(princomp(x))\"" to transform your x matrix into one with columns that are computationally orthogonal. Then after, just apply the stepwise selection using models build on the original response y and the new found x.orth instead of x_x0007__x0007_Case 3) The independent/explanatory variables (columns of design matrix x) are not orthogonal and not only do you wish to find an orthogonal basis but you want them to fix non-linearity issues. In this case, you may refer to chapter 7 using splines (e.g. bs(), ns(), etc) and polynomials (e.g. poly()) all of which generates orthogonal basis. So create a new design matrix using one of these function and then perform stepwise selection using models built on the original response variable and the new found x's._x0007__x0007_Note that i am unaware of why you are performing forward orthogonal stepwise selection, but it should most probably be for making predictions. As mentioned in the lectures, interpreting coefficients become very difficult after you make transformations of the original x variables.""",1,0,0
"In an example with single gene mutation, Professor Rob Tibshirani mentioned that features with low variance are removed. May I know what criteria to determine if a feature can be removed?_x0007__x0007_Thanks,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
Helpful!! thanks!,1,0,1
This is really helpful :) And it reminds me to always keep for loop in mind...,1,0,1
"I too could not understand how \complexity\"" can be quantified or derived from the knots.""",1,1,1
"I have used R square as a measure of model fit in linear regression, but wanted to know if there is some metric which captures importance/ impact of each variable (for example: \% variance explained by variable xyz\""). In cases where variables are standardized, we can compare absolute values of coefficients, but does that alone capture variable importance? I have a similar question regarding logistic regression - I have used odds ratio estimates to represent variable importance, but is there a better way to do so?""",1,1,1
"Look again at section 4.7 which talks about ROC curves and AUC. If you had a process that generated random outcomes, what would the ROC curve look like? That will tell you the AUC.",1,0,1
"I tried to use this simple approach like this_x0007__x0007_`mean1 <- c(1,1,1,1,1,0,0,0,0,0)`_x0007__x0007_`x1<-matrix(rnorm(1000*10,mean=mean1),ncol=10)`_x0007__x0007_But I got stuck after I performed a simple check:_x0007__x0007_`mean(x1[,1])`_x0007__x0007_gives_x0007__x0007_` [1] 0.<phoneRedac>`_x0007__x0007_and_x0007__x0007_`mean(x1[,10])`_x0007__x0007_gives_x0007__x0007_` [1] 0.<phoneRedac>`_x0007__x0007_Although I expected to get mean equals one for first and zero for last vector in `x1` matrix. What do I simulate wrong?",1,1,1
"In Section 9.2 (SVM), Prof. Hastie talks about the regularization parameter C. _x0007__x0007_He draws arrows to show the distance to the margin. I think in one arrow he made a mistake: He draws the arrow above the hyperplane, such that the data point lies actually closer to the margin on the other side. Isn't that the distance between the data point an the margin?",1,1,1
"Baden-Württemberg, Germany",1,0,1
"I think you can register now as well, put a little effort in and finish by the end of next week",1,0,1
"Dear all,_x0007__x0007_I would like to use a set of binary variables to predict a continuous outcome. The tricky thing is that interactions between the predictor variables are presumably of high importance. One approach would be to use Boosted Regression Trees (BRT). Indeed, trees include interactions, but they are not differentiable from main effects. Thus, for prediction BRTs are useful. _x0007__x0007_Is there a method that selects relevant interactions  and allows to show and interpret them?_x0007__x0007_Cheers_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"IMHO, I think it depends on what you mean by \importance\"".  At least in my own experience, interpreting fully standardized coefficients is not as intuitive (...\""given a 1 SD change...\"" etc.) as just measuring the variables on a uniform metric, such as 0 to 1, where appropriate.  _x0007__x0007_For a logistic, changes in predicted probabilities could capture variable importance, too.""",1,0,1
Thanks zhq.,1,0,1
"As stated in the answer to [this similar question](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/531cc95df192a92bcf<zipRedac><zipRedac><zipRedac><zipRedac>18), the website will be taken down, so download now whatever you want to have for future reference!",1,0,1
"I disagree with the official answer.  I use linear regression very frequently in practice and at least for the data I study it is usually quite true and accurate._x0007_Oh well, I got that one \wrong\"".""",0,1,1
"R fills matrices by columns, so I believe you need to add `byrow = TRUE` after `ncol = 10` (i.e. in the `matrix()` call to get the desired behaviour. R will recycle through `mean1` 100 times (to give the 1000 random variables we asked for) and hence the first 5 have mean 1, then next 5 mean 0, the next 5 mean 1 and so on. Hence the need to fill by rows.",1,0,1
"You don't need `mvrnorm()` at all; you can do this using `rnorm()` which has been used on the course. The error rate is the proportion of incorrect \answers\"" on independent test sets. The results tell us how we'd expect each method to perform on this problem for some new data; for this problem linear SVMs and logistic regression outperform, slightly, the more complex SVM with radial kernel. Also, learning how to do this kind of simulation is very useful as you approach a new problem. It was advanced a little beyond the level of some of the other questions, but that's what courses are for, to challenge the students. Perhaps it could have been explained better, and perhaps my judgement is clouded by the fact that I'm quite familiar with R, but once I'd read the question a couple of times and sat down with R and sketched out some code, the solutions ended up being relatively simple. It would certainly help in future if the \""Answer\"" was a little more helpful than the one provided for this question; if you struggle implementing the R code, the Answer provided isn't going to help you with that.""",1,0,1
"Not within **glmnet** when I looked quite recently. There are other packages that can do grouped lasso in R though. For some, see the section *Regularized & Shrinkage Methods* in the [*Machine Learning Task View*](http://cran.r-project.org/web/views/MachineLearning.html) on CRAN.",1,0,1
"Simulation is probably going to be your best bet if you know something about the problem - the implied \truth\"" say. You can simulate from that \""truth\"" equation data sets of given size with particular amount of noise, then go through the data analysis step and record if you can detect the \""truth\"" or not. Repeat a large number of times each for different sized data sets and produce a plot of size vs probability that you recovered the \""truth\"". How you decide you've gotten the \""truth\"" will depend on the problem, but you could look at the MSE or the error rates for example.""",1,0,0
"Kent, thanks for the pointer.  I can see there that it does appear that I had it right on my first stab at it... BTW, those notes are awesome! _x0007__x0007_Now, though, I'm still back to thinking that 6 \looks\"" like a better choice than 4.  Even though the 1SE rule suggests that sampling error might be what's contributing to my choice of 6 over 4.""",1,0,1
"Sorry I'm coming back to this late; I don't find this worrying at all. It is known, and IIRC the lecturers mentioned this in one of the videos, that $k$-fold has some uncertainty to it arising from the fact that you are using smaller subsets of data to do the fitting and how those subsets are taken. The repeated $k$-fold CV method is a CV approach that addresses this issue, but if the underlying model (trees) is so variable, I would question the choice of the model for the problem/data set to hand, not the validity of CV.",1,0,0
"An outside vendor was used (a human, I believe, rather than a machine)",1,0,1
"Calculate the coefficient of variation for each $x$ variable. Sort the list of coefficients of variation from high to low. Use the top <zipRedac>00 variables with the highest coefficient of variation in your model._x0007__x0007_http://en.wikipedia.org/wiki/Coefficient_of_variation_x0007__x0007_    # some data_x0007_    _x0007_    x<zipRedac> <- c(<zipRedac>234500, <zipRedac>345000, <zipRedac>222200, <zipRedac>500000, <zipRedac>600000)_x0007_    x2 <- c(-<zipRedac>0, <zipRedac>, <zipRedac>00, <zipRedac>000, -<zipRedac>000)_x0007_    _x0007_    # variance_x0007_    var(x<zipRedac>)_x0007_    var(x2)_x0007_    _x0007_    # relative variance_x0007_    var(x<zipRedac>) / (mean(x<zipRedac>)^2)_x0007_    var(x2) / (mean(x2)^2)_x0007_    _x0007_    # some data unscaled_x0007_    _x0007_    x<zipRedac>.un <- x<zipRedac> / mean(x<zipRedac>)_x0007_    x2.un <- x2 / mean(x2)_x0007_    _x0007_    var(x<zipRedac>.un)_x0007_    var(x2.un)",1,0,0
"I was playing around with viewing the mixture SVM with a heatmap instead of just a single decision boundary contour. Turned out kind of interesting:_x0007__x0007_    temp.par <- par(mfrow=c(1,2))_x0007_    image(matrix(abs(func), 69, 99), main=\Predicted\"")_x0007_    image(matrix(abs(prob-0.5), 69, 99), main=\""Actual\"")_x0007_    par(temp.par)_x0007__x0007_![Heat maps of predicted vs. actual probabilities][1]_x0007__x0007_You can also try scaling the matrices before displaying (they're already centered, correct?):_x0007__x0007_    image(scale(matrix(abs(func), 69, 99), center=FALSE), main=\""Predicted\"")_x0007_    image(scale(matrix(abs(prob-0.5), 69, 99), center=FALSE), main=\""Actual\"")_x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>97.jpg""",1,0,1
"Based on answers when this question was asked earlier, it appears that website will be removed when course is over.  As you are probably aware, you can download videos and all of the slide presentations.  You can also download the R sessions.  You will miss out on quiz questions, but they are (in my opinion) the least useful aspect of the course.",1,0,1
I am using R 2.14.1 and I can not get the Matrix package.,1,1,1
"For the record, I've basically figured this out, by carefully reading sections 7.4 (esp. the first paragraph) and (horrors!) section 7.12 (esp. the 2nd paragraph)._x0007__x0007_I'm not highly motivated to elaborate here, as: (1) I and the single upvoter are the only ones who are likely interested; and (2) this site's going to be taken down in the near future. That said, I would be explain a bit more to an interested person *who has actually engaged with the book sections mentioned here and in my original question* and remains interested but perplexed._x0007__x0007_FWIW, my confusion rested on an apparent unadvertised shift in the meaning of \training set\"" in section 7.12. Read that second paragraph of the section carefully!""",1,0,1
"Since the variables may be correlated to some degree, an easy solution to clearly see the importance of each variable on its own is to run each predictor separately in a single variable linear model, and look at $R^2$. But since this is equivalent to squared correlation, might as well run the function `cor` on the whole data.frame (you might also like the function `pairs`).",1,0,1
"Thanks all for your suggestion. I tried caret for nearZeroVar, which did not keep many variables to work with. Maybe I did something wrong. I will need to go back to rerun the code. It could be that I need to adjust the cutoff in the function. I also tried range/mean, similar to <redacted> suggestion. I arbitrarily assigned +/- 5% cutoff. This makes sense intuitively. That is why I want to hear what the standard practice is for this sort of problem._x0007__x0007_Best regards,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Curitiba, Brazil",1,0,1
"I couldn't answer this question after 5 submissions. I assume that it is due to a typo in my code, but I can't figure out why. Any help appreciated since the answer provided after 5 submissions does not contain the correct code. Here is what I had typed:_x0007__x0007_fit2=lm(y~poly(x,2))_x0007__x0007_I realize that it is missing the data set selection at the end, but it had worked fine for 7.R.R1 without it, so I didn't think that I needed it for 7.R.R2",1,1,1
http://www.revolutionanalytics.com/r-language-resources_x0007__x0007_http://onepager.togaware.com/,1,0,1
"I got lost on this step_x0007_# 3) repeat (1-2) many times and averaging the error rate for each trial_x0007_errors = replicate(100, svm_error())_x0007_I did not see where svm_error was defined. What am I missing?",1,1,1
Has anybody installed Tsboot in R verion 3.x or lower ?_x0007__x0007_Warning in install.packages :_x0007_  package ‘tsboot’ is not available (for R version 3.0.2),1,1,1
"tsboot is a function in the \boot\"" package (which is available for R 3.0.2)""",1,0,1
"yes, because I'm trying to improve a prediction model that already exists! So then I have the \predicted wind speed\"" as a predictor.""",1,0,1
"Hi guys,_x0007__x0007_As you may know data preprocessing (e.g. data cleaning, data integration,...) is one of the most important parts of data analysis, because without clean data there would be no clean mining. A real world data set needs to be cleaned first, but I'm afraid in this course I didn't learn much about data preprocessing. Anyone knows about good resources to learn how to preprocess data wisely or any opinion about the whole preprocessing part?_x0007__x0007_Thanks in advance",0,1,1
www.r-project.org,1,0,1
Follow these two discussion_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/531dd393d1bf779c7a<zipRedac><zipRedac><zipRedac><zipRedac>1d_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-quiz-review/threads/531ccc3b5<zipRedac>d<phoneRedac><zipRedac>2b,1,0,1
Try this nice package: http://cran.r-project.org/web/packages/relaimpo/index.html,1,0,1
It's a shame that the hardcover book is out of stock right now at the discount. Just when I thought I would like to get the ESL to further advance my knowledge ...,0,0,1
"While analyzing the data for the 'state' dataset in R, when I ran the predict function on my model, it showed the predicted values state-wise, by default. How do I change the behavior if I would like to see the output grouped by another variable?_x0007__x0007_Thanks",1,1,1
[www.r-bloggers.com][1]_x0007__x0007__x0007_  [1]: http://www.r-bloggers.com,1,0,1
"I didn't have a scooby, but after following those two discussions, I managed to produce correct answers._x0007__x0007_It's the stuff that has nothing to do with SVM that really stumped me._x0007__x0007_Whilst I wouldn't claim to have a deep understanding of how the answers were produced, I learned quite a lot from those two discussions and doing the question.",1,0,0
:-),1,0,1
"I know this has been somewhat asked before but still not clear._x0007__x0007_From the source at Berkeley: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr_x0007__x0007_> The out-of-bag (oob) error estimate In random forests, there is no_x0007_> need for cross-validation or a separate test set to get an unbiased_x0007_> estimate of the test set error. It is estimated internally , during_x0007_> the run._x0007__x0007__x0007_Question 1: Is the above actually true?_x0007__x0007_Question 2: I created a random forest classification model on 10,000 observations of 4 hour forex data from 2004-2010 and the OOB Error rate was 46%. I then used that model to run a prediction on 6,000 observations of 4 hour forex data from 2010-2013 and the error rate was 1%. **This sounds too good to be true. What are some things I can do for a sanity check of that error rate?!**_x0007__x0007__x0007_Thanks!",1,1,1
[Alteryx Designer][1] is a fantastic piece of software for data cleansing and integration. Plus you can use R and pre-built R-based tools within Alteryx Designer. Makes for a tidy way to perform data preprocessing and statistical learning all within a single work flow._x0007_ _x0007__x0007__x0007_  [1]: http://www.alteryx.com/alteryx-designer,1,0,1
"The gbm function (Generalized Boosted Regression Modeling) has a argument, cv.folds, which does cross validation. I don't know what this does related to the three parameters required to do the modeling. I assume that for a given set of parameters (number of trees, shrinkage and number of splits) the function will produce a given model. If cross validation is used, is a different model produced and what is this model in relation to the one produced without cross validation?_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"This quote sent me careening off the tracks, and put a spotlight on my current lack of understanding of vectors (calculus?).  Since beta 1 = 0.8 and beta 2 = 0.6, I can see that (.8^2) + (.6^2) = 1, but I'm just not getting the link between this fact and how the function of the normal (based on beta1 and beta2) helps determine the distance of points from the hyperplane.  I would appreciate a cliff notes-style reference that might set me in the right direction?",1,1,1
You may have more luck trying to find a several page summary of linear algebra.,1,0,1
"why is the max number of tries always 5, even when the possible answers are fewer?",1,1,1
b/c that is the number the staff set...,1,0,1
"Try command:_x0007_install.packages(\e1071\"")_x0007_It should work if you connect to internet without proxy.""",1,0,1
"This is a REALLY helpful hint, thanks Stefano. One thing to add, Assume 2 classes.",1,0,1
"haydn, thanks!!!  You also help clear up a related issue I had trying to understand the relationship between the above and the \length\"" of the normal vector.  It appears to me that the length of the normal vector helps define the relationship between projection points (on the normal) and distances from the hyper plane surface.  My head is still exploding, but in the best \""wow-mathematics-is-like-magic\"" way possible.""",1,0,0
"Undergrad Psych 20+ years ago, a decade before students having their own computer really caught on. There was lots of stats in the research papers, and \Statistics for Psychology\"" which I didn't understand great. Since then, after learning how to code, I study math and statistics (like this course) as a hobby. When I see the math formula, I write the code to do it. It is so funny, after writing the code, I regularly find myself thinking, \""So that's what they meant, why didn't they just say ..., that would have been so much clearer\"" when running into the same topic from 20+ years ago. Math notation is very vague and abstract, but coding helps to understand, forces you to note all the details and visualize the steps and what the formula is doing.""",1,0,0
"The class notes define entropy as a LOG function, but the quiz expects us to use LN to find the cross-entropy value.",1,1,1
"Unfortunately log is ambiguous.  Some user communities assume this means base 10.  Some assume base 2. Many mathematically oriented communities assume base e.    Statisticians pride themselves on having theorems, so they align themselves with the base-e group.  In practice you have to either stay in one community  or get used to paying close attention when \log\"" first appears.""",1,0,1
"Awesome that someone else thought about this, and in a way responded to a lonely post I threw up a few days ago :/ _x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/undefined/threads/531cb614f192a<phoneRedac>f",1,0,1
"`?gbm` says \in addition to the usual fit, will perform a cross-validation, calculate an estimate of generalization error returned in cv.error.\"" So in other words, it will return a model fit on all of the data, but entirely separately will perform CV, in which case different models will be fit on each fold purely to spit out a `cv.error` term and not influencing the overall model.  At least that's my interpretation (did not verify this).""",1,0,0
great answer!,1,0,1
"I used log base 2 and thus got the answer wrong.  However there is a good explanation for using log2, in that if the target attribute can take on K values, then the log2(K) serves as an upper bound for the expected encoding length measured in \bits\"" : which is a definition of Entropy.""",1,0,1
"As indicated by the slides will we get a Chapter 10 courseware package of videos, exercises and quiz ??, If so when, we have 7 days more till March 21st deadline._x0007__x0007_regards",1,1,1
"Not sure how you did it but do copy and paste the following code to see if it works_x0007__x0007_rm(list=ls())  _x0007_if(!(\ISLR\"" %in% rownames(installed.packages()))){install.packages(\""ISLR\"")}  _x0007_if(!(\""tree\"" %in% rownames(installed.packages()))){install.packages(\""tree\"")}  _x0007_attach(Carseats)  _x0007_High = ifelse(Sales <= 8, \""No\"", \""Yes\"")  _x0007_Carseats = data.frame(Carseats, High)  _x0007_tree.carseats = tree(High ~ . - Sales, data = Carseats)  _x0007_summary(tree.carseats)  _x0007__x0007_Note: The first line is used to clear the memory in R, the second and the third simply checks if you have the packages, if not it will install them_x0007__x0007_Hope this helps~""",1,0,1
"Hi there, changing the order of columns will not affect the predictive power of the model. Theoretically speaking, orthogonalization is done via the gram schmidt procedure. That is, we use the first column as a reference and project the second column onto it to create an orthogonal vector. Then we use the third to project on the space span by the first and the second, and so on. Therefore, if you change the column of the design matrix it will certainly change the output of your orthogonalization. However, the seemingly different output is merely a rotation of the output you would have gotten if you have used the original orientation. Don't take my word for it. It is easy to see using say PCA for example. Note that if you change the columns of your design matrix, the loadings associated will change. However, the proportion of variation explained will remain the same._x0007__x0007_Hope this helps~",1,0,0
"> As indicated by the slides will we get a Chapter 10_x0007_> courseware package of videos, exercises and quiz ??, If so_x0007_> when_x0007__x0007_[Courseware -> Course Logistics -> Getting Started](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/courseware/d<phoneRedac>fd3642f19ee45e273e3dfafa/5bc11f959e6641a7abdffbd29ac1c66e/)_x0007_says:_x0007__x0007_> Week 9: Unsupervised Learning (Chapter 10, starts Mar 15)_x0007__x0007_----_x0007__x0007_> we have 7 days more till March 21st deadline._x0007__x0007_Yes, all assignment deadlines seem to be._x0007__x0007_> Mar 21, 2014 at 11:00 PDT",1,0,1
"I´ve made a logistic regression model of the odds one sports team beats another._x0007_The model consists of an intercept, team A´s rating and team B´s rating. Both variables are highly significant, and the model seems to produce credible odds. It can correctly predict 75% of the 1500 matches in my dataset._x0007__x0007_So I decided to try something different. I put team B´s rating into the model where team A´s rating should be. Logically, there is no reason either team should come first. When applying the model to new data, either team could be written down first._x0007_Interestingly, the odds of team A winning in one version and the odds of team B (now in team A´s position) winning in the other version did not add up to 1, or a figure near it. The question is why?_x0007__x0007_**Pwin(a)** given B0 + Ba*Xa + Bb*Xb_x0007__x0007_**Pwin(b)** given B0 + Ba*Xb + Bb*Xa_x0007__x0007_Pwin(a) + Pwin(b) does not add up to 1._x0007__x0007_My idea:_x0007_Team A´s coefficient and team B´s coefficient are calculated on different sets of teams. It is one sample of combinations, but the sides of the combinations don´t hold the exact same composition of teams. Could it be that these 2 groups are not random, therefore leading to biased betas? _x0007__x0007_All teams in group A are going to a large tournament, while teams in group B are the ones they happend to play against._x0007__x0007_Potential solutions:_x0007__x0007_-I could draw a random sample from my sample, or flip a random subset of cases in an attempt to equalize the two groups._x0007__x0007_-Or should I go by one version of the model, and just use 1-p for the options the other way around?_x0007__x0007_-Alternatively, I could replace the two rating-coefficients by one coefficient of the difference in ratings of the two teams, essentialy making it impossible for R to fit different coefficients.",1,1,1
"Thanks a lot guys! this thread was very helpful. I learned a lot and got the right solutions at the first attempt. Yes, there should be more programming exercises. But the time is short, and the amount of topics covered large. I hope I can do many of them using the book, after the course is finished.",1,0,0
What are the default values for Gamma and cost?,1,1,1
"I understand your point, but I think that it would have been much easier if the slide had used Ln; that would have avoided any confusion. I grew up abroad and LOG was always used for a base 10, except when otherwise clearly stated. So I had not expected it to mean something else. Thankfully this is just one quiz, I will keep that in mind for the rest of the class",0,0,1
"And, to answer the other implied question, in case you were wondering..._x0007__x0007_*Why did you use `data=whatever` before and don't need it now?*_x0007__x0007_For this question, your `x` and `y` variables exist \out in the open\"", all by themselves. When you type \""x\"", for example, R knows exactly which vector you're talking about._x0007__x0007_This is rare case!_x0007__x0007_In most examples (and in real life, usually) the parameters used in your formula are actually columns (vectors) in a **data frame**. Just specifying the column name alone isn't enough for R to know what you're talking about--you have to *also* tell it the data frame to look in._x0007__x0007_So let's say `x` and `y` *were* in a data frame called, for example, `foo`. You could **not** do this:_x0007__x0007_        fit <- lm(y ~ x)_x0007__x0007_...because R wouldn't have a variable called \""y\"" or \""x\"". Or, even worse, it would have an x and y variable left over from some previous session that was still sitting around, and it would use that instead of the data you really intended!_x0007__x0007_But you **could** do this:_x0007__x0007_        fit <- lm( foo$y ~ foo$x )_x0007__x0007_...because R *does* have a variable called \""foo\"", which contains the \""x\"" and \""y\"" vectors._x0007__x0007_And, as a sort of shortcut, you can also do this:_x0007__x0007_        fit <- lm( y ~ x, data=foo)_x0007__x0007_That tells R, \""When I say `x` and `y`, I really mean the x and y that live inside of `foo`.\""""",1,0,0
"FWIW, the name (e1071) seems rather odd but this R package is a wrapper for the popular LIBSVM package. Source is available and it has bindings to many languages. So any work you do with e1071 is an investment if you choose to use LIBSVM later._x0007_http://www.csie.ntu.edu.tw/~cjlin/libsvm/",1,0,1
"Hi everyone,_x0007__x0007_I'm <nameRedac_<anon_screen_name_redacted>> from Bangalore, India. I am an MBA and a supply chain professional and have recently moved into the realm of data science at work. I worked on a short stint at Market research back in 2009 and the interest in looking at data and analysis has never quite reduced. After moving across multiple business functions for the last few years, I finally got to begin my first project recently in the Sensor Data Analytics space._x0007__x0007_It's great to learn Statistical Learning from the pioneers in this field. And the teaching style is amazing! Looking forward to summon the courage to pick up ESL after this course._x0007__x0007_LinkedIn: [in.linkedin.com/pub/<nameRedac_<anon_screen_name_redacted>>-<nameRedac_<anon_screen_name_redacted>>-cscp/15/82/bbb/][1]_x0007__x0007__x0007_  [1]: http://in.linkedin.com/pub/<nameRedac_<anon_screen_name_redacted>>-<nameRedac_<anon_screen_name_redacted>>-cscp/15/82/bbb/",1,0,1
"SVM splits on many variables, and as long as there are not checkerboard patterns, should do a good job with one vs. one or one vs. all. Thus the set of SVM models forms a set of rules. There are some research projects that have used SVMs to create decision boundaries for trees (BUTIA, SVMODT) but I don't know of an R package that does this._x0007__x0007_Edit: Adding cubist<br>_x0007_R package \cubist\"" (Quinlan's C5.0 for regression trees):<br>_x0007_http://cran.r-project.org/web/packages/Cubist/index.html<br>_x0007_http://cran.r-project.org/web/packages/Cubist/vignettes/cubist.pdf_x0007__x0007_R package \""oblique.tree\"":<br>_x0007_http://cran.r-project.org/web/packages/oblique.tree/index.html <br>_x0007_Here is a page with a graph of data and the tree created:<br>_x0007_http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/_x0007__x0007__x0007_Steven Salzberg's OC1 decision tree:<br>_x0007_http://www.cbcb.umd.edu/~salzberg/announce-oc1.html  (old)<br>_x0007_http://ccb.jhu.edu/people/salzberg/Salzberg/Software.html  (current)_x0007__x0007_Murthy, S. K., Kasif, S., & Salzberg, S. (1994). <br>_x0007_A system for induction of oblique decision trees. <br>_x0007_Journal of Artificial Intelligence Research, 2(1), 1-32. <br>_x0007_http://www.jair.org/media/63/live-63-1401-jair.pdf <br>_x0007_http://arxiv.org/pdf/cs/<phoneRedac>.pdf_x0007__x0007_Sreerama Murthy1, Simon Kasif1, Steven Salzberg1, Richard Beigel2, <br>_x0007_OC1: Randomized induction of oblique decision trees <br>_x0007_http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.1037&rep=rep1&type=pdf <br>_x0007_http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.6304""",1,0,0
"But there is no quiz after the video, where is , if there is any link? What is the minimum percentage to pass the course.",1,1,1
"I believe what to learn from this question is that in order to keep 0 <= phatmk <= 1, the log must be the natural log similar to logistic regression, since otherwise phatmk would go outside that range.",1,0,0
You can run the summary() function after the svm fit and R will print those for you.,1,0,1
"I am developing species distribution models, using GLM and GAM to estimate the probability of herbaceous species occurrence and abundance. So this statistical learning course is great for me to get a good grasp of how to understand the advanced statistical methods and how to implement them in R. _x0007__x0007_In chapter 6, I have learned how to use the regsubsets function for finding the best subset of linear models. I think regsubsets() is available only for linear regression model selection. I am looking for a function to select the best subsets of variables for non-linear models such as GAM with smoothing splines and logistic regression. Is there any such function in R packages? _x0007__x0007_Thank you for this interesting course.",1,1,1
In statistics log will always mean base $e$.,1,0,0
"I already value their work based on reading their books and papers, so being able to see and hear them adds another dimension.  They already have given so much to us by their research and teaching; this adds a human touch.  Great work professors!",1,0,1
"Those warnings can occur any time R is VERY confident about some of the observations.  It is a good thing to note, but does not mean that anything went wrong.",1,0,0
"As I understood, the feature selection methods in Chapter 6 select the best model among_x0007_the models with k predictors based on the fit on the training set._x0007_Cross validation is only used to make the final decision on the number of predictors._x0007_Wouldn't it be beneficial to apply cross validation in the first step as well,_x0007_i. e. to also compare the models with the same number of predictors using cross-validated prediction error?_x0007__x0007_In some work I select the features with a wrapped non-linear SVM and I use cross validation for each of the candidate models._x0007_Is it necessary? I would expect that even with the same number of predictors some of the candidate models might be over-fitted while others might be fine. That is why I apply cross validation to each candidate model for comparing predictive performance._x0007_Or is it a question of model complexity whether it is necessary?",1,1,1
Thanks for the answer. This seem reasonable that only the cv.error would be affected by the cross validation and not the overall model. Thanks again for you quick answer.,1,0,1
"Dear Professor Hastie and Professor Tibshirani as well as Sam and Will, I truly enjoyed this course. It was as I expected it to be. The course title, \Statistical Learning\"", and the book title, \""An Introduction to Statistical Learning with Applications in R\"", led me to believe this was more of a statistical learning course than a course for learning R. Others seems concerned not enough R was presented, but I felt the course lived up to it's title and I learned some R programming to boot. Though, as a matter of course, I am a statistician who programs and not a programmer who uses statistics. Although, I have been taught many of these statistical methods previous, many I have not. It was great to have a thorough exposition of the many supervised statistical learning methods providing practical compare and contrast between them. I also enjoyed learning about cross-validation and the bootstrap methods of choosing best fit models and parameter estimation when it is otherwise unavailable. These methods suggest a different answer other than the traditional, \""we need more samples\"". Throughout the course and text is discussed high dimensionality which I find very useful and that of prediction accuracy versus interpretable models. I have spent my career in R&D and have never had the luxury of any other than having to interpret the statistical results. In my field people viewed predictive models as always wrong and sometimes useful. It was nice to see the genomic examples of predictive usefulness. I will keep a look out for follow on courses to this one for data science. Many thanks to the teaching team.""",1,0,0
"That did it.  I must have had a modified \Carseats\"" file in there?""",1,0,0
Thank you,1,0,1
Just received email saying that the deadline has been extended to April5.,1,0,1
"Switzerland, Neuchâtel",1,0,1
"I think I get the part about the unit circle: since beta_1 and beta_2 are on the unit circle, they can spin the hyperplane 360 degrees, or in other words give the hyperplane any rotation. Then beta_0 moves it up and down the y-axis, so the hyperplane can be moved and rotated to any separating setting.",1,0,0
"Anyone else having trouble downloading the HD versions of the videos (1280x720) from YouTube, as of Chapter 9?  aTube Catcher worked great through Chap. 8, but started reporting \Error 403\"" trying to download Chap 9 videos. After checking the thread Olmo notes above, I tried using clipconverter.cc, but without success.""",1,1,1
"@biplot, have you been able to download HD versions (1280x720) of the Chap. 9 videos from YT?  aTube Catcher worked great for me through Chap. 8, but started reporting \Error 403\"" trying to download Chap 9 videos. I also tried using clipconverter.cc, but",1,1,1
** preparing package for lazy loading_x0007_Error : package ‘class’ was built before R 3.0.0: please re-install it_x0007_ERROR: lazy loading failed for package ‘e1071’_x0007__x0007_It's not that easy to install R packages! I had to install a newer version of R and now it seems that it's too new? _x0007_What should I do?,0,1,1
"thanks! oblique trees seem to be a solid candidate, and that brief tour of trees and forests was a great post! i had seen \oblique RF\"" in the [caret model list (link)][1], now I know something about it. _x0007__x0007__x0007_  [1]: http://caret.r-forge.r-project.org/modelList.html""",1,0,0
this also seems to be quite promising: _x0007_http://people.csail.mit.edu/menze/papers/menze_11_oblique.pdf,1,0,1
@aradianto:_x0007__x0007_You might also find the first response in_x0007_[[this thread]](https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedbac,1,0,1
Update: Just got clipconverter.cc to work ... using Chrome rather than IE 9.,1,0,1
"Hi there,_x0007__x0007_Your answer should tally after removing (type=\response\"") from the predict function_x0007__x0007_With a logistic model, using (type=\""response\"") will essentially extract the predicted probabilities. What is shown in the ch7.Rmd is log(p/(1-p)) which is the logistic transformation._x0007__x0007_Hope this helps ~""",1,0,0
"Hi there,_x0007__x0007_I presumed that you used the setting as in CH9.R. I did it too in that way and got a similar graph as you. However, that setting is slightly different from what is being use in Dr.Hastie's code. In short, the exercise in CH9.R was used to find on average the error rate attach a particular sample size. Therefore, the \test\"" data there serves the role of the population. However, in the usual case of validation/cross-validation/out-of-bag/out-of-sample the test data's role is not the population. Rather, it serves as a benchmark to prevent us from overfitting. As in Dr.Hastie's code, 300 out of 506 of the data is used to train. This is indeed the usual setting where we set aside a large amount of our original data to do training and leave a small but significant portion for testing. Out-of-bag is used to help us assess the over-fitness and should not be mixed with what is done in CH9.R. _x0007__x0007_I did up a small code that does what i think should be the correct setting:_x0007__x0007_library(MASS)  _x0007_n  = 500  _x0007_mu = c(1,1,1,1,1,0,0,0,0,0)  _x0007_x1 = mvrnorm(n, rep(0,10), diag(10))  _x0007_x2 = mvrnorm(n, mu,        diag(10))  _x0007_y  = rep(c(0,1),c(n,n))  _x0007_testResponse = y  _x0007_testData     = data.frame(x=rbind(x1,x2),y=y)  _x0007__x0007__x0007_n  = 2000  _x0007_x1 = mvrnorm(n, rep(0,10), diag(10))  _x0007_x2 = mvrnorm(n, mu,        diag(10))  _x0007_y  = rep(c(0,1),c(n,n))  _x0007_trainResponse = y  _x0007_trainData     = data.frame(x=rbind(x1,x2),y=y)  _x0007__x0007_oob.err = test.err = double(10)  _x0007__x0007_for (mtry in 1:10) {  _x0007_    rf.fit = randomForest(as.factor(y) ~ ., data = trainData, mtry = mtry, ntree = 400)  _x0007_    oob.err[mtry] = rf.fit$err.rate[400,1]  _x0007_    pred = predict(rf.fit, testData)  _x0007_    test.err[mtry] = sum(with(testData,y != pred))/nrow(testData)  _x0007_}  _x0007__x0007_matplot(1:mtry, cbind(oob.err,test.err), pch = 19, col = c(\""red\"", \""blue\""), _x0007_    type = \""b\"", ylab = \""Misclassification Rates\"", xlab=\""mtry\"", ylim=c(0.08,0.2))  _x0007_legend(\""bottomright\"", legend = c(\""OOB\"",\""Test\""), pch = 19, col = c(\""red\"", \""blue\""))  _x0007_abline(v=match(min(oob.err),oob.err),lty=2)  _x0007__x0007_**Here, training data is about 80% and test data is about 20%**_x0007__x0007_![enter image description here][1]_x0007__x0007_Hope this helps ~_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>1.png""",1,0,0
"Hi all, _x0007__x0007_i believe the follow code could be potentially wrong_x0007__x0007_matplot(1:mtry, cbind(**test.err**, oob.err), pch = 19, col = c(**\red\""**, \""blue\""), _x0007_    type = \""b\"", ylab = \""Mean Squared Error\"")  _x0007_legend(\""topright\"", legend = c(\""OOB\"", **\""Test\""**), pch = 19, col = c(\""red\"", **\""blue\""**))""",1,1,1
"To answer question 1,_x0007__x0007_Let \A\"" be the original set of data and \""B\"" to be a bootstrapped sample from A. Naturally B is a subset of A and from 36/51 in the lecture notes on trees, B has on average 2/3 of the values in A. Thus, we can use the other 1/3 to do validating._x0007__x0007_Now one might be quick to jump in and say that this is 3-fold cross-validation but this is not exactly. In k-fold cross validation, we slice the original sample up into k different parts and then do the validating by building the model on all except one part. In OOB however, notice that although B is approximately 2/3 of A, the composition is not deterministic. Also, in k-fold cross validation, we need only to perform k computation and average them. On the other hand, OOB does it as many times as the number of Bootstrap samples required._x0007__x0007_All in all, OOB is some kind of cross validation though we cannot directly pin point the number of folds it is doing. Since it is naturally to add an additional prediction step for the other 1/3 of the sample that is not included in the bootstrapped sample, it is only computationally wise to just compute it internally. Rest assure, it is definitely an unbiased estimate of the errors as expectation is linear and thus averaging it over B estimates don't matter versus K estimates. The only thing that will differ could be the estimated variance of the error rate since variance generally decreases as the number of samples increase (e.g. for variance of mean is (sigma^2)/n). Then again, its not straightforward to compute the variance of the error rates in OOB since the samples are correlated. (e.g. var(x+y) = var(x) + var(y) + *2cov(x,y)*)_x0007__x0007_Hope this helps ~""",1,0,0
"Hi there, could you include your code ?",1,0,1
"An email was sent out (should be in your mailbox):_x0007__x0007_> Subject: [Statistical Learning] Course Assignments deadline extended  _x0007_> From: \Statistical Learning\"" Course Staff <StatLearning-course-updates@class.stanford.edu>  _x0007_> Date: Fri, 14 Mar 2014 18:14:20 +0000_x0007_>_x0007_> Hello Statistical Learning participants,_x0007_> _x0007_> The course is in its end run, and the deadline of March 21_x0007_> is looming._x0007_> _x0007_> We have seen on the discussion forum that it would be_x0007_> helpful to have an extension. We have decided to do this,_x0007_> and have extended the end date for the material and all the_x0007_> quizzes to April 4._x0007_> _x0007_> Rob and Trevor _x0007_>_x0007_>     ----    _x0007_> This email was automatically sent from Stanford OpenEdX.""",0,1,1
"Thanks (: Sorry didn't see the post, i realized the mistake while i was using the code for some explanations elsewhere in the forum",1,0,0
"Does anyone else get the error message \cannot open connection\"" when trying to download the data set? load(url(\""http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/ESL.mixture.rda\""))_x0007__x0007_Any workaround?_x0007__x0007_THanks""",1,1,1
Thanks. This makes both the data structure and logic understandable.,1,0,0
(9.10) in book: Why is sum of beta squared assumed to be equal to one? _x0007__x0007_Thanks!,1,1,1
Great explanation. Thank you.,1,0,0
"@ElinaJ_x0007__x0007_> I think I got it! It's a unit vector and since there are_x0007_> more than one feature (in which case the length for the only_x0007_> feature would have been 1), the length is divided among the_x0007_> different features. Did I get it?_x0007__x0007_Yes._x0007__x0007_If, by \divided\"", ",1,0,1
"load(url(\http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/ESL.mixture.rda\""))""",1,0,1
Great!  I haven't had time to try that.  Please post if you find anything interesting.,1,0,1
This course has been amazing! I've learned so much and I really think it will help me prepare for the goal I have to complete.  Thank You for taking the time and sharing your knowledge!  <nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>,1,0,0
"I know the end data has been extended, but where is chapter 10? I hope to finish it as soon as possible.",1,1,1
"Seconded!_x0007__x0007_I've previously taken the Johns Hopkins MOOCs on data analysis, which were also very good and covered some of the same ground in a slightly different way. I'm sure there are some concepts in this class that would've gone right over my head if I hadn't taken the previous courses to give me a bit of a foundation._x0007__x0007_But at the same time, there were some things that I never quite understood in the other MOOCs that this class really helped me \get\"". For example, I had always struggled mightily with wrapping my head around Principle Components, but it was explained here in a couple of brilliant ways I'd never seen before, helping me finally see what was going on!""",1,0,0
"The RandomForest function does not like missing data. If there's missing data it generates an error message and halts.  The documentation refers to an na.action argument which I've set =na.omit.  That seems pretty obvious.  This [web page][1] claims that it can handle missing data well and I infer there might be other functions to handle missing data.  My first question is what might these functions be and how do I specify them (such as using a median value).  _x0007__x0007_My more general question relates to the documentation.  When I type \?randomFores()\"" and see something like \""na.action A function to specify the action...\""  How do I find more information on how these functions work/where to find them/or (if worse comes to worse) how to write one?  Frankly, when I read documentation like this I find myself coming to dead ends (or in this case, lost in a forest) not know where to go.  _x0007__x0007__x0007_  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp""",1,1,1
Maybe they're on Spring Break?,0,0,1
"Remember that Stanford is in the Pacific time zone. Give them some time to wake up. :-)_x0007__x0007_They have consistently opened the new week at some point every Saturday, but not first thing in the morning.",1,0,1
"Hi,_x0007__x0007_I am having some trouble to understand factors in R._x0007_It seems that we need to use factors to represent classes in SVM, but not to represent categories in BLR. What is the difference?_x0007__x0007_Many thanks,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
I did the same but I still have the wrong answer. I changed the seed to avoid generating similar random points.,0,1,1
Thank you! I've got it resolved.,1,0,0
"Would the choices in this problem that have loadings of the form (.71, .71, 0, 0) violate the sum of squares equal 1 condition?  This would come out to 1.0082  So the choices involving these would be incorrect right?_x0007__x0007_Thanks",1,1,1
"Hi there, the description you have provided is actually bagging. Random forest does an addition step in each iteration to select randomly from the p predictors, m < p predictors from which we use to decide the split. _x0007__x0007_As for trees that does a really lousy job at classifying the test set, i believe a model does only as well as the data it is trained on. Therefore, it falls back to the question of whether the bootstrap procedure generates samples that are representation of the population. This in return is related to whether the main sample you are bootstrapping from has been collected well. Thus, if all is well, there shouldn't a reason not be democratic about it. After all, the deviations should even out in expectation/average according to large sample theories. (Note that we do monitor the error rate and estimate as the number of trees used increases)_x0007__x0007_Hope this helps ~",0,0,0
"Many thanks to Profs Hastie and Tibshirani for conducting the course and sharing their valuable insights. More than the actual techniques, what I found most useful were the insights surrounding them (such as when to scale and why, ideas behind feature selection, the need for lasso and ridge loss functions, etc).",1,0,0
"Problem solved. I had made a blunder in calculating the test error using \predict\"" function.""",1,0,0
A tip from Stack Exchange..._x0007__x0007_http://stats.stackexchange.com/questions/<zipRedac>2839/how-to-use-r-prcomp-results-for-prediction,1,0,1
"I tried n! and fact{n}.  Both times got \Sorry couldn't parse formula.\""""",0,0,1
"Hello,_x0007_I think I am doing 9.R.1 okay but if I want to do_x0007_plot(svmfit.dat) I get the error below. Any idea?_x0007__x0007_set.seed(105)_x0007__x0007_x=matrix(rnorm(1000),100,10)_x0007__x0007_dat=data.frame(x,y=as.factor(y))_x0007__x0007_svmfit=svm(y~.,data=dat,kernel=\radial\"",scale=FALSE)_x0007__x0007_plot(x,col=y+3,pch=19)_x0007__x0007_print(svmfit)_x0007__x0007_plot(svmfit,dat)_x0007__x0007_Error in plot.svm(svmfit, dat) : missing formula.""",1,1,1
"No, AFAIU, wiggly generally performs better on the training data, often due to overfitting to those particular training data, but then it will perform less well on the test data.",1,0,1
"The scores of your principal components as in the x part of your pca object, i.e.:_x0007__x0007_    pca<-prcomp(...)_x0007_    pca$x_x0007__x0007_You can then use normal R indexing to get the scores of the first five components.",1,0,0
"I got within 10% of the correct response on question 10.R.2, but I did not get the exact same numbers. Would it be possible to briefly sketch (in the answer explanation) the path used to get to the official answer ?",1,1,1
"prcomp returns a list with class containing the component x._x0007__x0007_Here is an example:_x0007__x0007__x0007_    pca.out<-prcomp(datapc, scale=TRUE, retx=TRUE)_x0007__x0007_Learning data_x0007__x0007_    xols<-pca.out$x[1:300,1:5]_x0007__x0007_Hope it will help",1,0,1
"I think it definitely is supervised learning, you have to train the data to classify labels into groups.  In here, we are assigning it into 0 or 1.  Or, the prof calls it positive and negative examples.",1,0,0
Thanks for your answer.,1,0,1
"This has been a great course. The lectures and book are both outstanding. Now to assess what we learned and to further advance our knowledge, I would like to suggest that the professors give us a final exam in which we are asked to analyze data but are NOT given step by step guidance. We do not need to have our work graded but I would hope there would be answers.",1,1,0
"Thank you from here, too. This must be the coolest course I've ever taken: two guys who know and like their field throwing clever ideas at me, making me laugh out loud and challenging me with interesting questions for a couple of hours every week, telling me exactly how to do my work, giving me two free books and zero pressure. I would count myself so lucky if I could work with people like this in real life._x0007__x0007_Thank you so much also to fellow students who responded to my questions on forums and who started and answered other threads from which I learned.",1,0,1
What I would enjoy would be to do my research with the lecturers of this course.,1,0,1
"One more happy student._x0007_Mr Hastie and Mr. Tibshirani, with this wonderful course I'm one more step close to becoming a Data Scientist, thank you, a hundred times._x0007_<nameRedac_<anon_screen_name_redacted>> F. <nameRedac_<anon_screen_name_redacted>>",1,0,0
"Minor note: you did not specify `y` in your code. _x0007__x0007_Re: the error, when there are more than two variables you have to specify a formula indicating which two input variables you want to be displayed. For example, for your data, give this a try. _x0007__x0007_    plot(svmfit, data=dat, formula=X1 ~ X2)",1,0,1
"Thank you for all the effort you put into providing such a great course.  I have been a fan of The Elements of Statistical Learning for a while.  When I saw there was a MOOC based on Introduction to Statistical Learning, I knew I had to take it._x0007__x0007_Thanks again.",1,0,1
no... that did not work...,0,0,1
"Excellent code. I was wondering, for the theory part (not bootstrap), why is the prediction interval so much bigger than the confidence interval?",1,1,1
This is the first time I signed-up for a course like statistical learning. Was a bit difficult initially since I had to balance both work and study. But enjoyed it. Looking forward to other statistical courses._x0007__x0007_Thanks a ton.,1,0,0
"For this question, I have a hard time to calculate mean squared prediction error, I tried LOOCV,but it does not work since it is over the range of k. What method can I use?",1,1,1
Thanks Colin & Jeff for the help; works! Appreciate it.,1,0,1
"prcom(data, ..., tol=#)_x0007_This tol # is a cut of (ignore portion below of) SD of 1st PC for the model.",1,0,1
"To make it simple without any numbers, think how many straight lines (p=1) you can make if you have only two points (n=2)? Is the solution unique?_x0007__x0007_Now think again how many straight lines (p=1) you can make if you have only one point (p=1)? Is there a unique solution?_x0007__x0007_Now try the same logic with a parabola (2nd order polynomial; p=2), but with the number of points n=3, n=2, n=1._x0007__x0007_What will happen if if you have p=1, or p=2 and n=100?",1,0,1
"Yea, if there is a particular fold that looks most like OOB its the LOOCV since it gets tested against models build on many different training data sets (:",1,0,1
"You welcome (: not really sure what was the issue when you first ran it but from experience, the attach command can get quite tricky. You can check what you have attached via the command search(). Also, it is good practice to do a detach() right after your exercise before saving the work pace",1,0,1
![enter image description here][1]_x0007__x0007_Hope this helps ~_x0007__x0007__x0007_  [1]: https://edx-west-uploads.s3.amazonaws.com/1<phoneRedac><phoneRedac>08.png,1,0,1
I was wondering if the class videos could still be surfed in this website after 2014 4/4?,1,1,1
Great point. I feel the same too. Using the x's from the test data will not allow us to to validate the performance of our factor loadings on say a new set of x's,1,0,1
+1 had the same issue,0,0,1
"Hi All. In chapter 8 R session at line 87 (ch8.Rmd), it seems to me that test and oob curves are interchanged. test should be red and oob blue as defined in that line. This differs from the following line (line 88) which may induce a misinterpretation of the results. Am I wrong?",1,1,1
"That's right, it was discussed elsewhere in forum as well._x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter<zipRedac>014/discussion/forum/i4x-HumanitiesScience-Stats<zipRedac>16-course-Winter<zipRedac>014-course-material-feedback/threads/53<zipRedac>3f9dea47ddbc<zipRedac>b<zipRedac>00000c",1,0,1
"A binary categorical results in 1 dummy/indicator variable that takes on zero or one -- so that will be kept as a group. If there are > 2 levels, then true they are in a sense split up. But that also has an advantage, say if you have three levels French, Italian, German, then if the French one is shrunk, then it doesn't matter if the person is French, but it still may matter that the person is Italian or German; in that case you might not want to ignore nationality entirely.",1,0,0
"I put the code up there and explained it as well... you specify the input variables not the response -- it's not the same formula as what you used to fit. It's what you want to see with notation Y-axis-predictor-variable ~ X-axis-predictor-variable. Again, look at the one line of code I posted, or the examples in `?plot.svm`.",1,0,0
"I think PCA is a data transform like converting from Cartesian to polar coordinates. The transform does not change the data, but it makes it easier to work with. The difference with PCA is that variation in the data defines the new reference frame.",1,0,1
That's to the Prof. Hastie and Tibshirani for there time and effort for this course. I hope you offer another course in the future.,1,0,1
"Where can I find more examples of using R in real-world analysis?_x0007__x0007_This has been a good introduction to R, and I'd like to continue learning. Frankly I found a lot of the data manipulation challenging because I was not familiar with how R does things. For example, I still have no idea how to extract the PCA vectors, or to transform a new data set to the PCA coordinates. I'm sure it is a 2 line solution that has something to do with pca.x$rotation[,1].",1,1,1
Great response. Thanks.,1,0,1
"Just wondering if anybody can see what I did wrong?_x0007__x0007_    #Function to generate data_x0007_    data <- function(n1, n2) {_x0007_      x0 <- rmvnorm(n1,mean=rep(0,10))_x0007_      x1 <- rmvnorm(n2,mean=c(rep(0,5),rep(1,5)))_x0007_      x <- rbind(x0,x1)_x0007_      y <- c(rep(0,n1), rep(1,n2))_x0007_      dat <- data.frame(x,y=as.factor(y))_x0007_    }_x0007_    _x0007_    test <- data(500,500)_x0007_    train <- data(50,50)_x0007_    svm.fit <- svm(y~., data=train)_x0007_    pred <- predict(svm.fit,test)_x0007_    1 - mean(pred==test$y)_x0007_    _x0007_    # this is giving an error rate of around 0.3%",1,1,1
check this out yo_x0007_https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416,1,0,1
"It was a great learning experience from great teachers!  I have been exposed to a lot and intend to follow up using their two books, ESL and ISL,when the course is over. I cannot wait for whatever they offer next.",1,0,1
Great course.  Great book.  Thank you very much to Professors Hastie and Tibshirani and everyone involved in creating this course and making it available to us.  Great job!  Thanks!,1,0,1
How does the certificate of completion work? Is there any way of knowing if some course was skipped?,1,1,1
"Additionally, what if the variable is just text like \customer name\"", \""address\"" etc. how do we handle these variables in principal component analysis. _x0007__x0007_Sometimes you get data that has numeric, some categorical and text variables. PCA seems to assume that variables are numeric?""",1,1,1
They are in stock at Amazon.com.,1,0,1
"Yes, I wondered the same thing. How can the test set error be valid when the test set is used to build the model? It's pretty similar to doing feature selection before cross-validation, which we were warned against.",1,1,1
Thanks a lot Colin!,1,0,1
"I have done exactly this and I don't get the right answer._x0007__x0007_I mean:_x0007__x0007_train <- cbind(y,x)_x0007__x0007_fit<- lm(y~.,data=train)_x0007__x0007_predict(fit,x.test)_x0007__x0007_and compare with y.test._x0007__x0007_...",0,0,1
"Is it maybe related to use \normalized\"" x?""",1,1,1
Do the two professors have any suggestions for R code for neural networks?_x0007_Thanks_x0007_<nameRedac_<anon_screen_name_redacted>>,1,1,1
"Is the answer a number? Because if I use N and K, it says N and K are not permitted as an answer.",1,1,1
There is an error on the page when I ticked the correct answer it marked it wrong but when checking the answer it was right could anyone get this corrected as I did tick the correct box and without giving the answer here,0,1,1
Have you seen these Playlists ?_x0007__x0007_https://www.youtube.com/channel/UC4OWDcPB1peiBXDfCSZ3h-w/playlists_x0007__x0007_should all be there in a (semi-)structured form...,1,0,1
"Why did you use `cbind()`? It is not correct since in this case you have 201 independent variables instead 200, which is correct number. You should use only x._x0007__x0007_ Try      `fit<-lm(y~.,x)`",1,1,1
"No, it is not a number. Use small letters.",1,0,1
"I am also struggling with this question._x0007_At present, I have the following script:_x0007__x0007_train <- cbind(y,x)_x0007__x0007_train= as.data.frame(train)_x0007__x0007_fit<- lm(y~,data=train)_x0007__x0007_yhat=predict(fit,x.test)_x0007__x0007_mean(yhat-y.test)^2_x0007__x0007_and I am unable to obtain the correct answer._x0007_Your feedback is appreciated._x0007__x0007_Best wishes,",0,1,1
"Notice that the question is asking for the *error* rate. So, if you're getting around 83% accuracy, then your error rate is around 17% and should be entered as `0.17`_x0007__x0007_As an aside, the question seems to me to be asking us to randomly generate a fresh data set of 100 observations *each time*. So, in 500 trials, we'd wind up with 50,000 unique, randomly-generated observations._x0007__x0007_You are generating a large data set only once, then randomly sampling from that. No matter how many simulations you run, it can only contain observations that exist in your original 1100 observation set._x0007__x0007_(The question even points out that \in real life don't know the generating distribution, so we have to use resampling methods *instead of the procedure described above*\"")_x0007__x0007_In the end, I don't think it will make much different in the results. Still, I thought I would point it out.""",1,0,0
"Thanks, Duh! I got the answer right first time when I used n instead of N and k instead ofK. Thanks",1,0,0
ups_x0007__x0007_squared... sorry_x0007__x0007_Now yes!,1,0,1
But it's not exactly the answer... I'm puzzled. Something apparently so simple.,0,1,1
"I think because the `predict` functions work slightly different between `svm` and `glm`. Here's the relevant parts. _x0007__x0007_    if (modeltype==\svm\"") { _x0007_      fit=svm(y~., data=dat, kernal=kernal) _x0007_      ypred=predict(fit, testdat) _x0007_    } else if (modeltype==\""logistic\"") { _x0007_      fit=glm(y~., data=dat, family=\""binomial\"")_x0007_      ypred=predict(fit, testdat, type=\""response\"")_x0007_      ypred=ifelse(ypred < 0.5, 0, 1) _x0007_    }""",1,0,1
"I have been trying to solve this problem by first creating some simple examples and counting the number of possibilities and see if I can find patterns, but they all seem to be \blowing up\"" even with something as small as n=4. I figure that the pattern has to be some type of combinatoric problem, but can't quite figure it out._x0007_Is k*[(nCk + nCk-1+...+nC1]  (where C is the symbol for combination) anywhere close to where I need to be?_x0007_ Could someone give me some advice in how to be more efficient in how to solve this problem?_x0007__x0007_Thanks in advance for any help!""",1,1,1
"nice find, here's another reference that addresses PCA with categorical variables. _x0007_http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont",1,0,1
It's much simpler than that. What if n=3 and k=2? How many ways can you assign three things into two groups?,1,1,1
"It works fine on Safari, so it's a Chrome issue (I really hope someone is reading this)._x0007__x0007_It freezes when I enter ^ symbol.",1,1,1
"I have a similar issue: Win 7, Firefox: I would stretch my browser across my two monitors then hit the double-box icon to enlarge the video. By stretching like this, the enlarged video would center align far enough to the left that the closed caption had plenty of black margin space to the right of the video to display without blocking the video. However, similar to above, in the last couple of weeks or so, I can do this trick, but the video does not enlarge, it stays the same size, and aligns to the left side, middle of the browser. Any idea what may have changed?",1,1,1
"Sure.  No problem.  As to the CLT and the irreducible errors, I guess I was making a subtle distinction in a not very clear way.  Expecting the irreducible errors to be typically be nearly normal by an appeal to the CLT as stated in the wikipedia article is sort of a philosophical choice...but it should be checked to see if they really are in any given situation.  And if they aren't, gathering more data probably won't make them that way.  The data generating mechanism either has noise that is a sum of independent random jolts or it doesn't.  On the other hand, gathering more data will tend to make the sampling distribution of the parameters more normal by the CLT because they are estimated as sums of the observed responses weighted by combinations of the observed explanatory variables.  So as long as the irreducible errors aren't from some pathological distribution that the CLT doesn't apply to, we can expect the betas to be distributed normally if we gather enough data.",1,0,0
Actually PCA is only used for transformation. Then use 1:300 for fitting & 301:1300 for prediction._x0007_Cheers,1,0,1
It would be great if the lecture videos are available anytime on YouTube.,1,0,1
"I was able to break up the original result into pieces like Krisha suggests. This code should give you an idea of how to make it happen. I'm sure there is a more elegant solution, but this worked for me._x0007__x0007_p2_data = data.frame(pca.out$x[1:300,1:5],y=y)_x0007__x0007_p2_lm = lm(y~.,p2_data)_x0007__x0007_Good luck",1,0,0
thank you both. It did the trick.,1,0,0
"Hi guys. You are great help. I got 1 and 3 right, but in this one I get messed up with _x0007_Error in data.frame(pr.out$x[1:300, 1:5], y = y.test) : _x0007_  arguments imply differing number of rows: 300, 1000_x0007_Can you give me a hint on how to proceed with the predict() function?_x0007_Thanks !!",1,1,1
"This is a pretty good way to do it, but as I said. Often times I don't have enough information about the data distribution or implied \truth\"". I have tried Monte Carlo simulations and I use it as a rough estimate, but I was hoping there was an analytical way to go about it. Thanks a lot for your reply.""",1,0,1
"Could someone please help me on this._x0007__x0007_#create the train set for regression_x0007_xols<-pca.out$x[1:300,1:5] _x0007__x0007_#run the regression regfit=y~xols_x0007__x0007_#create the test set with the remaining 1000 observations_x0007_xtest=pca.out$x[301:1300,1:5]_x0007__x0007_#use the predict function to predict y based on xtest (i.e. 1000 obs)_x0007_predict(regfit,data.frame(xtest))_x0007__x0007_But I get the following warning message:_x0007_****Warning message:_x0007_'newdata' had 1000 rows but variables found have 300 rows**** _x0007__x0007_I can't figure out my mistake. As i have 1000 rows in xtest, I would expect to get the same number of predicted values, which I would use to find the MSE (as y.test also has 1000 obervations)._x0007__x0007_Thanks for the help!",1,1,1
"Regarding, questions 9.R.* a lot of people complained that there was not enough programming problems in this course._x0007__x0007_I found these questions hard too. However there are a lot of stats / data analysis / data science courses around at the moment. I really, really like this one and I am really grateful for it as it has different content and is more detailed than many of the others. Thank you Trevor, Rob, and everyone else involved in the course._x0007__x0007_If you want more practice programming in R here are two courses I am following at the moment that I would recommend:_x0007__x0007_Coursera has Data Analysis and Statistical Inference_x0007_https://www.coursera.org/course/statistics_x0007_This is a basic course that is a good introduction for beginners but it's comprehensive in it's treatment of basic statistical concepts._x0007_Even though the course has started you can still check out the on-line training sessions for R here:_x0007_https://www.datacamp.com/_x0007__x0007_EdX has The Analytics Edge_x0007_https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416_x0007_Unfortunately this course has also started. It has a lot of programming homework each week that really helps get familiar with R. _x0007__x0007_Also I suggest you checkout the the upcoming Coursera Data Science specialization as I have done some courses with Coursera / John Hopkins before and they had in-depth coverage of R:_x0007_https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage",1,0,1
"Me too._x0007__x0007_Did you get d.d<zipRedac>2277<zipRedac> ? _x0007__x0007_(I replaced two digits by \d\"" as not to give the answer away)._x0007__x0007_In 10.R.3 I got d.d<zipRedac>71<zipRedac>7 , again close to but not quite the official answer. _x0007__x0007_Perhaps they used another random seed for their computation than the one used for the 10.R.Rdata file...""",1,0,1
Coursera has a 4 week course on R beginning April 4th.,1,0,1
"I got full credit for the questions, but my answers do not match the solution key exactly.",1,1,1
"Hi,_x0007__x0007_I seem to lack some basic understanding of the Kernel associated jargon. _x0007__x0007_Apparently, the computational effort to solve a kernel support vector machine does not increase as the dimension of the basis increases. The answer suggests that \The beauty of the \""kernel trick\"" is that, even if there is an infinite-dimensional basis, we need only look at the n^2 inner products between training data points.\""_x0007__x0007_So I am confused:_x0007__x0007_ 1. What is this \""basis\"" they are talking about? Is it related to the dimension 'p'?_x0007_ 2. If the \""basis\"" is related to the x_i vector, why is computational effort not affected?_x0007_ 3. What are the \""basis functions\"" (slide 15) ? _x0007__x0007_Thanks for your help and suggestions!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
"Thanks Holger, it should indeed.  I stupidly didn't think to check for typo-graphical errors as I've encountered the same error message in the past when packages haven't been updated for newer R versions._x0007__x0007_Now successfully installed and I'm only slightly embarrassed at my own stupidity!  _x0007__x0007_Thanks again,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,0
"So I ran it without intercept, and the results changed._x0007_Where before they added up to more than 1, they now add up to less then one._x0007__x0007_I`ll try reformatting the data, and see if the predictions are more accurate. _x0007__x0007_But there seems to be one problem with this approach: it prohibits me from entering any more variable than team dummies. This is problematic if we assume the team may change over time. I could include interaction terms with time to solve this problem, but I´ll probably end up overfitting or introducing a lot of autocorrelation._x0007__x0007_Currently, my data is set up as follows:_x0007_Each row is a match._x0007_There are colums for \team A rating\"", \""team B rating\"", \""team A home dummy\"", \""team B home dummy\"".""",1,1,1
"You use the first 300 pca.out$x and y to fit linear regression model, then use the model you established to predict y using the last 1000 pca.out$x.",1,0,1
Thanks a lot sir!! Its a great opportunity to learn statistics from you.,1,0,0
"Thank you Dr Hastie and Dr Tibshirani for such a great course and a great book on statistical learning, I've really learned a lot during the last ten weeks.  I'm looking forward to more courses from you two!",1,0,0
"Yes, that does the job and it struck me why. Logistic regression models the **probability** of an outcome and not the actual outcome itself. So the result of: _x0007__x0007_    ypred=predict(fit, testdat, type=\response\"")_x0007__x0007_is a vector of probabilities.""",1,0,1
How can we predict 1000 observations (x.test and y.test have 1000 rows) if our training set has only 300 observations?,1,1,1
"When you create a data frame (with the data.frame(xtest)) command, you need to give the correct \names\"" to each column/feature of your data. The names of your \""test\"" dataframe, have to be the same (and have the correct correspondance) with the names of the \""train\"" dataframe that you used to calculate your linear regression model._x0007__x0007_Note, you can change the names of a data frame with the following code:_x0007__x0007_    my_data_pca5 = data.frame(...)_x0007_    names(my_data_pca5) # check automatically created names_x0007_    names(my_data_pca5) <- paste(\""PC\"", 1:5, sep = \""\"")_x0007_    names(my_data_pca5) # check new names_x0007__x0007_Good luck!""",1,0,0
"When you create a data frame, with the data.frame(x.test) command, you need to give names to each column/feature of your data. The names of your \test\"" dataframe, have to be the same (and have the correct correspondance) with the names of the \""train\"" dataframe that you used to calculate your linear regression model._x0007__x0007_Note, you can change the names of a data frame with the following code:_x0007__x0007_    my_data_pca5 = data.frame(...)_x0007_    names(my_data_pca5) # check automatically created names_x0007_    names(my_data_pca5) <- paste(\""PC\"", 1:5, sep = \""\"")_x0007_    names(my_data_pca5) # check new names_x0007__x0007_Good luck!""",1,0,0
"> If you're using forward selection, you'll have one model for a given number of predictors_x0007__x0007_If p is the total number of candidate predictors and k+1 is the given number of predictors in this step (as in the slides), there are p-k models to consider. We can compare these by RSS on the training set as described in the book and slides, or we can apply cross-validation for each of the p-k models to select the best.",1,0,0
I would also like to express my appreciation for this course.  Thank you to everyone involved._x0007_<nameRedac_<anon_screen_name_redacted>>,1,0,1
"I don't think there can be an absolute minimum error rate since we are always drawing a sample. In theory you could draw a sample, however unlikely, which is perfectly classified -- in this case your minimum error rate would be 0% but I'm guessing that's not what you're after..._x0007__x0007_There may indeed be an expected error rate with some standard deviation around that expectation. Is that what you're asking about? In that case, bootstrapping seems to be a good way to model that. While it's not a proof, at least you can see through many gaussian samples on fixed parameters and SVMs to fit them that, to take numbers out of thin air, a 10% error rate only occurs about 0.001% of the time.",1,0,0
"Oh, now I see! So forward selection using (average) CV RSS as opposed to training RSS. That seems very reasonable.",1,0,0
see code below.,1,0,1
"@imag: _x0007__x0007_I'm hoping this is a minimal example to show how the `plot.svm` works. If there are more than two variables in the data frame you give it, you have to specify the column names in the data frame so it knows which two of them to select (as I guess ",1,0,1
"No worries mate! _x0007__x0007_\An expert is a person who has made all the mistakes which can be made in a very narrow field.\"" - Niels Bohr""",1,0,1
I get the same error. How is one supposed to enter n choose k?,1,1,1
"Hi, with LDA you can have multiple values in the class, trees can be used also, but SVM not. Does anybody know other technique that can be used to deal with multiple class values?_x0007__x0007_Thaks in advance.",1,1,1
"I had the same problem, and it stumped me for a little while. Presuming you have are having the same issue I had, then it comes down to the code used for the regression; and, in my naive appreciation of the difference, the difference between fitting a model to data, and fitting a method to data._x0007__x0007_So, don't use `lm(y~x)`; use `lm(y~.,data=x)`.",1,0,0
Albany  you are talking about project i am still unable to load 7.R.R1. Please help if you can ._x0007_<nameRedac_<anon_screen_name_redacted>>,1,0,1
"I had a bad time with this item :-( only got OK at 3rd trial._x0007__x0007_I was fooled by the phrase *\using the first five principal components ***(computed on rbind(x,x.test))*** instead as low-dimensional derived features\""* and due to the bold comment I regressed an lm() in the PCA vectors obtained with all the data!_x0007__x0007_If the lm() model is trained in the 5 first PCA vectors coming for all the data (x and x.test merged with rbind()) and then it is used to predict y.test, we get a wrong answer. So beware, *you must train using only x to get the first 5 PCA,* then apply lm() to these reduced features in the test data and then predict in the (x.test,y.test) data set and calculate the MSE. _x0007__x0007_To get the first 5 PCA vectors you do:_x0007__x0007_    pr.out = prcomp (x , scale =TRUE)_x0007_    rot = pr.out$rotation[,1:5]_x0007__x0007_To get the 5 dimension reduced features in train data_x0007__x0007_    xred=pr.out$x[,1:5]     # the reduced features_x0007__x0007_Or also works this_x0007__x0007_    xmat=data.matrix(x, rownames.force = NA)_x0007_    xred = xmat %*% rot_x0007__x0007_To get the 5 dimension reduced features in test data_x0007__x0007_    xtestmat=data.matrix(x.test, rownames.force = NA)_x0007_    xtestred = xtestmat %*% rot_x0007__x0007_And from here you fit the lm(), predict, etc... switching between maatrices and data frames..._x0007__x0007_Note that in the reduced data you can use simply _x0007__x0007_    lmfit=lm(y~.,data=...)_x0007__x0007_without worrying with boundaries._x0007__x0007_HTH_x0007__x0007_Tony""",1,0,0
"My guess is that in this case 5 out of 200 (2.5%) variables explain a relatively important amount of the variance._x0007__x0007_Assuming that your saved you results in the variable **pca**, try and plot the cummulative proportion of variance explained, using something like:_x0007__x0007_    plot(summary(pca)$importance[3,], type=\l\"")_x0007__x0007_This might give you a better picture of why the proportion of variance explained by the first five principal components may be considered by some as *substantial*""",1,0,1
"I had the same problem. My solution is to subset a training set before fitting lm like we did in labs. I can't explain how exactly it works but it worked for me._x0007__x0007_    pca.out <- prcomp(...)_x0007_    dim(pca.out$x) ##should be 1300 200_x0007_    df <- data.frame(x = pca.out$x[,1:5], y = c(y, y.test)) ##create a data frame for lm_x0007_    train <- seq(from = 1, to = 300, by = 1) ##subset our training set_x0007_    fit <- lm(y~., data = df[train,]) ##fit lm on training set_x0007_    pred <- predict(fit, df[-train,]) ##predict on test set_x0007__x0007_And then compute mse.",1,0,0
"It was said \The deadline for completing all the requirements to get your Statement of Accomplishment is March 21.\"" But what was the criteria for completing, I have failed in answer couple of the exercises, will I get the accomplishment statement or not. From the progress page I see my progress is 94% for the time being._x0007__x0007_Thanks!""",1,1,1
"Hi!_x0007_Prediccion must do it with the same data os the regression model:_x0007_Prediccion<-predict.lm(p2_lm,p2_data) (thanks JEFFH)._x0007_Then you must compute MSE._x0007_This solution works.",1,0,1
"Yes, that's what I got as well.",1,0,1
"Hi Fazal,_x0007_you can get the file from the following link:_x0007_https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/7.R.RData_x0007_. You can load the file by directly clicking it or from R",1,0,1
"Thanks, boethian!_x0007__x0007_I managed plotting of SVMs now._x0007__x0007_Also one important note to overcome the obstacle for everyone who creates the datasets to fit the SVMs - **response variable must be converted by function `as.factor()`!**_x0007__x0007_The confirmation of this in the `summary` call of SVM: the type is C-classification._x0007__x0007_If the conversion of response to factor somehow failed/forgotten, then response treated as numeric variable (SVM type is eps-regression) and, the most strange, `plot()` function for SVM produces nothing without any errors!",1,0,0
"when I was trying to verify the **prcomp()** results by **svd()** in doing quiz 10.R today, _x0007_I found that **pca.out`$`sdev * 36.04 == svd.out`$`d**._x0007__x0007_And further I found **pca.out`$`sdev** is consistent with **sqrt(eigen(cov(X))`$`values)**,_x0007_where **X = scale(rbind(x,x.test))**._x0007__x0007_I get confused. _x0007_Why should **svd()** return scaled values? _x0007_What's the meaning of the scaling factor (36.04)?",1,1,1
**Ch1 Introduction**_x0007__x0007_1.1 Opening Remarks_x0007_https://www.youtube.com/watch?v=2wLfFB_6SKI_x0007_ _x0007_1.2 Examples and Framework_x0007_https://www.youtube.com/watch?v=LvaTokhYnDw_x0007__x0007__x0007__x0007_**Ch2 Overview of Statistical Learning**_x0007_ _x0007_2.1 Introduction to Regression Model_x0007_https://www.youtube.com/watch?v=WjyuiK5taS8_x0007__x0007_2.2 Dimensionality and Structured Models_x0007_https://www.youtube.com/watch?v=UvxHOkYQl8g_x0007__x0007_2.3 Model Selection and Bias-Variance Tradeoff_x0007_https://www.youtube.com/watch?v=VusKAosxxyk_x0007__x0007_2.4 Classification_x0007_https://www.youtube.com/watch?v=vVj2itVNku4_x0007__x0007_2.R Introduction to R_x0007_https://www.youtube.com/watch?v=jwBgGS_4RQA_x0007__x0007_Interview with John Chambers_x0007_https://www.youtube.com/watch?v=jk9S3RTAl38_x0007__x0007__x0007__x0007_**Ch3 Linear Regression**_x0007__x0007_3.1 Simple Linear Regression_x0007_https://www.youtube.com/watch?v=PsE9UqoWtS4_x0007__x0007_3.2 Hypothesis Testing and Interval Confidence_x0007_https://www.youtube.com/watch?v=J6AdoiNUyWI_x0007__x0007_3.3 Multiple Linear Regression_x0007_https://www.youtube.com/watch?v=1hbCJyM9ccs_x0007__x0007_3.4 Some Important Questions_x0007_https://www.youtube.com/watch?v=3T6RXmIHbJ4_x0007__x0007_3.5 Extensions of the linear models_x0007_https://www.youtube.com/watch?v=IFzVxLv0TKQ_x0007__x0007_3.R Linear Regression in R_x0007_https://www.youtube.com/watch?v=5ONFqIk3RFg_x0007__x0007__x0007__x0007_**Ch4 Classification**_x0007__x0007_4.1 Introduction to Classification Problems_x0007_https://www.youtube.com/watch?v=sqq21-VIa1c_x0007__x0007_4.2 Logistic Regression_x0007_https://www.youtube.com/watch?v=31Q5FGRnxt4_x0007__x0007_4.3 Multivariate Logistic Regression_x0007_https://www.youtube.com/watch?v=MpX8rVv_u4E_x0007__x0007_4.4 Logistic Regression - Case Control Sampling and Multiclass_x0007_https://www.youtube.com/watch?v=GavRXXEHGqU_x0007__x0007_4.5 Discriminant Analysis_x0007_https://www.youtube.com/watch?v=RfrGiG1Hm3M_x0007__x0007_4.6 Gaussian Discriminant Analysis - One Variable_x0007_https://www.youtube.com/watch?v=QG0pVJXT6EU_x0007__x0007_4.7 Gaussian Discriminant Analysis - Many Variable_x0007_https://www.youtube.com/watch?v=X4VDZDp2vqw_x0007__x0007_4.8 Quadratic Discriminant Analysis and Naive Bayes_x0007_https://www.youtube.com/watch?v=6FiNGTYAOAA_x0007__x0007_4.R Classification in R.A_x0007_https://www.youtube.com/watch?v=TxvEVc8YNlU_x0007__x0007_4.R Classification in R.B_x0007_https://www.youtube.com/watch?v=2cl7JiPzkBY_x0007__x0007_4.R Classification in R.C_x0007_https://www.youtube.com/watch?v=9TVVF7CS3F4_x0007__x0007__x0007__x0007_**Ch5 Resampling Methods**_x0007__x0007_Interview with Bradley Efron_x0007_https://www.youtube.com/watch?v=6l9V1sINzhE_x0007__x0007_5.1 Cross-Validation_x0007_https://www.youtube.com/watch?v=_2ij6eaaSl0_x0007__x0007_5.2 K-fold Cross-Validation_x0007_https://www.youtube.com/watch?v=nZAM5OXrktY_x0007__x0007_5.3 Cross-Validation: the wong and right way_x0007_https://www.youtube.com/watch?v=S06JpVoNaA0_x0007__x0007_5.4 The Bootstrap_x0007_https://www.youtube.com/watch?v=p4B,1,0,1
For me the most difficult was 10.R.2/10.R.3 and 9.R.*,1,0,1
"Can someone help me read the question in 9.R?  I am not sure how to read the following notations:_x0007_1.  In the second sentence, I would read x_sub_i to be a real number element of R_super_10 but I don't know how to read what the superscript 10 denotes._x0007_2.  In the \given\"" paragraph, I would expect the tilde to be read as \""approximately\"" but I don't know what the italic N_sub_10 function refers to or what the italic I_sub_10 means.""",1,1,1
"Seems like when exploring a large feature space with Principal Components ... that a biplot can result in a very busy picture, with arrows all over the place, making it difficult to interpret what's going on.  Am I thinking about this correctly?_x0007__x0007_The crime-rate example in the lecture uses p=4 and the biplot was clear.  The problem in quiz 10.R on the other hand has p=200.  I made a biplot there ... and it looked absolutely ridiculous (200 unreadably dense loading vectors)._x0007__x0007_Is there a way to determine a subset of features with which to limit the number of displayed loading vectors?",1,1,1
"I would like to note that the answer speaks about \autocorrelation\"", not \""correlation\"". I think they are different. Autocorrelation depends on the sequence of the samples. As we see a smooth curve in the matplot, it means each sample (row) is highly related with the previous sample i.e., the samples are highly autocorrelated._x0007__x0007_However, I was wondering that if the samples (rows) were ordered randomly (keeping each and every sample), then the autocorrelation would not be clearly visible. Please see the following code._x0007__x0007_    matplot(Xy[sample(1:1000, 1000),],type=\""l\"")_x0007__x0007_So, it remains unnoticed that the effective sample size is lower than the number of rows. Can anyone please explain how we can determine effective sample size in such situation?""",1,1,1
"My interpretation: X is a vector of 10 variables, each element in the vector xi is a random variable that follows (tilde) a normal distribution with parameters defined by the values inside the parentheses (mean, variance). _x0007__x0007_The I_sub_10 specified as a variance represents an identity matrix of size 10 used to communicate that each xi has a variance equal to 1.",1,0,0
"On the bracket challenge, you have like a 1 in 9 quintillion chance of winning. (For those who don't know about it:(https://tournament.fantasysports.yahoo.com/quickenloansbracket/challenge/) _x0007__x0007_ This guy has given it some serious thought: [https://www.youtube.com/watch?v=pdnxTr6hG14][1]_x0007__x0007_I don't have suggestions for data, but second the call for references on any stat/machine learning approaches that have proven successful --- for having the smallest error rate in the office pool! _x0007__x0007__x0007_  [1]: https://www.youtube.com/watch?v=pdnxTr6hG14",1,0,1
It was a great course for introduction to a lot of modern techniques to statistical learning.  Will there be a more in-depth class that follow the Elements of Statistical Learning book? _x0007__x0007_Thanks again for this class!,1,1,1
"Mine didn't match exactly, either.",1,0,1
"There's an easy way to see variance explained of principal components resulting of \prcomp\"" function: with the \""screeplot\"" function..._x0007__x0007_pca.object <- prcomp(x, scale = T)_x0007__x0007_screeplot(pca.object)_x0007__x0007_HTH""",1,0,1
page 388!,1,0,1
"Thank you, Dr. Hastie, Dr. Tibshirani and Dr. Witten for a top course in Statistical Learning. I've learned a lot._x0007__x0007_Great explanations and a fantastic book. I got it! Now it's time to deep in \The Elements of Statistical Learning\""... Maybe it will be another course. If so, count on me!_x0007__x0007_Thank you to students that helped me with some tricky questions. At last there's light!_x0007__x0007_:)""",1,0,0
I think so: seems to me that's data snooping!,1,0,1
But extracting the principal components is part of building the model. I think it's data snooping!_x0007__x0007_I'm expecting profs. answering this important question...,1,1,1
"Thank you !!!! I figured I might watch some of them after the course and was hoping I could find them by searching. This list makes that task much easier, I won't have to search, I will have this list to click on the link. This is very helpful, thank you for doing the search and posting this.",1,0,0
Ditto.,1,0,1
"I have successfully used carats on chrome on mac, so it is probably even more specific.",0,0,0
Just a heads up that I am not sure the videos will be left up after the course ends.,1,0,1
"Hi Mauricio,_x0007__x0007_One of Rob's students, <redacted><redacted> (now at U of W) has done some cool work of sparse PCA.  It uses similar ideas to the Lasso to create loadings that are sparse.  You might be interested in checking it out.  Google \penalized matrix decomposition.\""""",1,0,1
"Thank you professors and TAs!  I'm so happy I found an R course before I finished my Masters (R is still not covered in our more traditional stats classes, so I felt left behind compared to what some people in my field are using right now.)  _x0007_And a special thank you for extending the course a bit longer; because of that I was able to concentrate on a 2nd draft revision of the thesis and get it in on-time!  And now while it's being reviewed, I can focus on lessons I glossed over (sorry, its true!) and bum that progress % up.",1,0,0
"<redacted> <redacted>, I didn't know about the mvrnorm _x0007_function. I also just saw your post on R-bloggers which had some other interesting courses in it._x0007_On another note, the 'Metrics' package has some functions to calculate classification error, mean squared error, and other measures for predictive models.",1,0,1
"Nairobi, Kenya",1,0,1
Thank you!,1,0,1
I found the R session in the text book was very help.,1,0,0
"Hello,_x0007__x0007_Thank you for the response. The test set error is about 32%, which I think is pretty high. Without the model, i.e. if I were to guess the same outcome repeatedly every single time, I would be incorrect 37% of the time. So, the model only improves prediction by 5 ppt - which sounds really low to me._x0007__x0007_I don't know what's realistic in real life problem. So, it's very hard for me to gauge if my model is \good\"" or \""bad\"".""",1,0,1
"I had the exact same problem in my first attempt to put factorial in the denominator. The I read the question few more time, it emphasize \naive\"". So just omit the denominator and be really naive:)""",1,0,0
"Hi,_x0007__x0007_this must have been surely an important lesson, but I didn't understand most of it, and there's nothing in the book about this topic._x0007_First of all, why the CV error for Step 2, ignoring Step 1, is zero? Even if the 100 best predictors are the same for each fold (I'm ignoring step 1), I don't see why classification should be perfect. Considering a case with 5 millions predictors, as suggested by Trevor, I came up with the following reasoning. _x0007__x0007_Maybe, if I have 5 millions predictors and 50 samples, it's highly likely that, out of 5 millions Xs, there are at least 100 of them which are perfectly correlated with the 50 results. However, if I include Step 1 in my cross-validation, then each time I subtract 10 samples (for 5-fold CV) and now it's less likely that I find always the same predictors, perfectly correlated with the answer, since predictors and answer are independent. Is this right? I'm the first to be not completely convinced, so I'd really like your input on this.",1,1,1
"Hi,_x0007__x0007_I'm not sure why the variance of the LOOCV estimate of test error is large. Since just one sample is left out each time, there are exactly K=N folds, each made of N-1 samples. Thus, the LOOCV error curve is a deterministic one. So, where does the variability in this estimate come from? I guess we are considering variability of the training set. I.e., if we would consider a different training set, then the LOOCV curve would different. Now, the LOOCV estimate of test error is the average of the outputs of N models, each one trained on a data set which differs by only one sample from the other folds. So, these outputs are likely to be highly positively correlated, and thus their average will have a large variance. Is this correct?",1,1,1
"If you haven't yet, read the comment from <redacted> <redacted> on the 10.R.2 tread Building a regression model... I reckon that should explain why you have this problem",1,0,1
"No,_x0007__x0007_I am clearly not the one who's gonna give the right answer, but I'd appreciate it if someone puts a clear version of how to get the right answers. I have gone through all the suggestions and great help available which allowed me to solve 1 and 2 (with a significant error margin, i.e. 1.05 << 1.088 for Q2), but I honestly do not think I really get it. I failed in all attempts to solve the 3 so that's probably a bad sign. _x0007__x0007_If I understand the last question correctly we are using the betas from the y~x regression to calculate predicted values for y.test, using the x.test data and then we calculate the MSE of those predicted values using mean((predicted y.test - actual y.test)^2). I don't see how this is different from what we did in question 2 actually._x0007__x0007_Any further guidance would be well appreciated!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",0,1,1
"The difference between Q2 and Q3 lies in which predictors you are using to fit the OLS model (the response remains the same, the y)._x0007__x0007_In Q2, you are using 5 predictors that correspond to 5 orthogonal linear combinations of the original 200 predictors (you obtain them by projecting the original data onto the first 5 principal components that PCA gave you using the predict function) while in Q3, you are using 200 predictors (the original ones)._x0007__x0007_In terms of R code, the model fitting should look the same (`lm(y ~ ., data=train)`) but the data frame `train` should be different between Q2 and Q3. One uses the original x and y while the other uses the projected x over the first 5 principal components and y._x0007__x0007_Note that indeed I did not get the expected answers but mine still fell within the error margin.",1,0,0
"I agree with your analysis in point 3. The situation in the exercise is exactly that assumed by LDA - two gaussian distributions. The Bayes decision boundary is the hyperplane at the midpoint between the two centers. A point will be classified correctly if it lies closer to the center of its cluster than to the other, and incorrectly if it lies on the other side of the center._x0007__x0007_You don't say how you calculated your results in point 5. This is what I did: If you imagine rotating the axes so the centers are at the origin and at `sqrt(n)/2` from the origin on the X axis, you can see that it is only the distance in the X dimension that matters. That can be computed with `pnorm()`. So the expected error, for dimensions <zipRedac> to 5, is given by _x0007__x0007_`pnorm(sqrt(<zipRedac>:5)/2, lower=FALSE)` _x0007__x0007_which is_x0007__x0007_`0.<phoneRedac> 0.<phoneRedac> 0.<zipRedac>93238<zipRedac> 0.<zipRedac>586553 0.<zipRedac>3<zipRedac>7762`_x0007__x0007_This is pretty close to your numbers.",1,0,1
"I could not find the answer to this on the site.  I understand that the deadline for the course certificate is April 4, but what happens to the course material (slides, videos, R code) after April 4?  Will we retain access at that point?_x0007__x0007_Thank you.",1,1,1
"Hello Tony, thanks for this tip!_x0007_> I was fooled by the phrase \using the first five principal components (computed on rbind(x,x.test)) instead as low-dimensional derived features\"" and due to the bold comment I regressed an lm() in the PCA vectors obtained with all the data!_x0007__x0007_Also, I believe that you made some errors in the code that you posted:_x0007__x0007_> `xmat=data.matrix(x, rownames.force = NA)`_x0007_> `xred = xmat %*% rot`_x0007__x0007_If you want to calculate to apply the projection of the test data on your just obtained principal components you need to run the following code (note that pca.out is the output from the prcomp(...) function):_x0007__x0007_    rot = pca.out$rotation[,1:5]_x0007_    center = pca.out$center_x0007_    center = matrix(rep(center,times=nrow(x)), nrow=nrow(x), ncol=ncol(x), byrow=TRUE)_x0007_    scale   = pca.out$scale_x0007_    scale   = matrix(rep(scale,times=nrow(x)), nrow=nrow(x), ncol=ncol(x), byrow=TRUE)_x0007_    _x0007_    center[1:10,1:5]_x0007_    x[1:10,1:5]_x0007_    (x-center)[1:10,1:5]_x0007_    scale[1:10,1:5]_x0007_    ((x-center)/scale)[1:10,1:5]_x0007_    _x0007_    xmat=data.matrix( (x-center)/scale , rownames.force = NA)_x0007_    xred = xmat %*% rot_x0007_    xred[1:10,1:5]_x0007_    x.train.pca5[1:10,1:5]_x0007__x0007_Anyway, thanks for your help!!""",1,0,0
"I just forgot to add that the `(x-center)/scale` calculation is made in order to regularize the data! Regularization (or at least substracting the mean of the data, `center` in the code snippet) is very important in the calculation of the principal components, because it centers the data around the origin of coordinates.",1,0,0
The principal components are found using the full data._x0007_You can still fit using only the training data and check the performance on the test data (using the first 5 principal components); It's not cheating as the y's of the test set have not been used in the creation of the fit.,1,0,1
"Christian and others,_x0007__x0007_Thanks for your suggestions and sharing your problems. It is good to see I'm not the only one struggling here :). Regarding Christian's solution, I get within the boundaries of the right answer, but I don't get the exact answer (differences are respectively (-0.0957 and -0.25). Somehow, the proposed solution thus underestimates the value of the MSE we are actually looking for. Anyone a clue on how to get closer to the exact solution?",1,1,1
Is it ok to share the downloaded videos with others that didn't originally sign up for the class?,1,1,1
"I guess there is no problem since all of them are converted and uploaded to Youtube, there is a thread with a list of them.",1,0,1
Are we provided with the certificate after completion of course,1,1,1
"In 10.R.Q2, the instructions say to compute the principal components on `rbind(x, x.test)`, fit a regression of y on x, and then predict y.test and compute test MSE. But the test MSE isn't really a valid measure of model uncertainty here, is it? The principal components process has already 'seen' all of the \\(x\\)'s so the model is partially fit to the test data.",1,1,1
"Hi,_x0007_A simple question,_x0007_How to calculate MSE with R._x0007_Thanks in advance",1,1,1
"not sure about the problem at hand, but the scalar 36.04 should be $sqrt{n - 1}$.  Consider the following_x0007__x0007_calculate the eigenvalues of the covariance matrix for $x$.  i.e., _x0007__x0007_    eig.val.cov <- eigen(cov(x))$values_x0007__x0007_Next, calc the singular values of $x$_x0007__x0007_    svd.d <- svd(x)$d; _x0007__x0007_If you square the singular values of $x$ and divide by $n - 1$ where $n$ is the number of records in $x$, you end up with the eigenvalue for the covariance matrix.  You will see that they agree.  This is no accident and is due to the way the singular values decompsition is calculated.  WLOG suppose that each column of $X$ has 0 mean and unit variance, then we compute the covariance matrix as $frac{1}{n-1} X^TX$.  Basically (not 100% correct) the SVD computes the eigenvectors and eigenvalues for $XX^T$ and $X^TX$.  Now if $X$ is $m$ by $n$, then the first $n$ eigenvalues of $XX^T$ will coincide with the eigenvalues of $X^TX$ and the remaining $m-n$ eigenvalues of $XX^T$  will be 0.  The square root of the eigenvalues are the singular values of $X$.",1,0,0
"A proportion is a percentage times 100.  For example:_x0007__x0007_If the proportion of dogs with spots is .15, then 15% of dogs have spots.",1,0,0
"The answer is that it depends what you mean by test error.  If you believe the regression model (fixed $X$, random errors), then the above procedure is totally fine (in the sense of producing an unbiased (as unbiased as CV can be) estimate of test error).  In fact, under the regression model you can do any sort of screening before CV as long as you don't look at $y$._x0007__x0007_If you DONT believe the regression model (random $X,y$) then you should never look at the test $X$ if you want valid estimates of test error.  It really depends on what procedure you want to estimate the error for.  Note that this is a topic that the course staff has gotten into long discussions regarding.  It is definitely not straightforward or obvious.",1,0,0
"I didn't finish my response.  $U$ are actually eigenvectors from $XX^T$ and $V$ are eigenvectors of $X^TX$. If you try to do the computations, it may not work out because of the way they are computed.  YOu will see they some are the same, and others may be pointing in the opposite direction.",1,0,0
"me too - arantzazu, did you figure out how to get past that?",1,0,1
Cool!  Hope you enjoyed the class.,1,0,1
https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-quiz-review/threads/52fa4<zipRedac>2c<phoneRedac>71f<zipRedac><zipRedac><zipRedac><zipRedac>27,1,0,1
"I took that course and it was great, there're programming assignment every week, but as long as you know how to do matrix manipulation in octave or matlab, you should be ok.",1,0,1
"See ISLR, pp. 29-30, Formulas 2.5, 2.6.  On p. 191, 192, 249, 250, 253-258, 328, there are one-line R calculations to calculate MSE._x0007__x0007_MSE = Mean squared error = mean(squared(error))_x0007__x0007_    # Compute MSE._x0007_    mean((y.pred-y.test)^2)",1,0,1
"I like to think about it as a 'digit' analogy. In our problem, each of the points (observations) can be assigned to one of k clusters. We can represent each possible assignment of points to clusters as an n-digit number in base k.  We all know base 10 where each digit can have a value from 0...9 inclusive.  If there were 3 points in 10 clusters (remember they say naive implementation) the set of all assignments is represented by the set of numbers 000 ... 999 and the count of combinations is 10^3 = k^n.",1,0,0
"I want to thank and congratulate Profs Hastie and Tibshirani on a truly great course that far exceeded my expectations.  Not only did I get a very thorough introduction to modern statistical learning techniques but I also greatly benefited from an accessible introduction to the 'R' scripting language.  But wait there was more!  By generously making their two text books free to download to course participants, I believe I have two excellent references to guide my future exploration of statistical learning methods._x0007__x0007_Many thanks._x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,0,0
"Good point, thanks! Indeed, I missed that **the model was not trained on the actual response values**._x0007__x0007_Although, in neighbouring thread the similar question raised. And the response by staff is that \it is not obvious answer\"". _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-quiz-review/threads/5329ca0def<phoneRedac>0014""",1,0,1
"Dear Sir,_x0007__x0007_Will the course material be available in our online profile at the end of the course. Kindly tell me know so I keep a backup so I am not surprised at the end of the course when I am denied access.",1,1,1
Thanks that was very useful!,1,0,1
"Hi,_x0007_someone can help me with this sintaxis, I can't find the mistake but results seems to be wrong._x0007__x0007_Thanks a lot_x0007__x0007_k=1000; #test sample size_x0007__x0007_M=1;    #mean_x0007__x0007_S=1;    #variance_x0007_samples=100_x0007__x0007_ER=rep(0,samples)_x0007__x0007_for (i in 1:samples){_x0007__x0007_x=matrix(rnorm(1000,mean=M,sd=S),100,10)_x0007__x0007_y=rep(c(0,1),c(50,50))_x0007__x0007_x[y==1,]=x[y==1,]-1_x0007__x0007_xtest=matrix(rnorm(10*k,mean=M,sd=S),k,10)_x0007__x0007_ytest=rep(c(0,1),c(k/2,k/2))_x0007__x0007_xtest[ytest==1,]=xtest[ytest==1,]-1_x0007__x0007_dat=data.frame(x,y=as.factor(y))_x0007__x0007_test=data.frame(xtest,ytest=as.factor(ytest))_x0007__x0007_fit=svm(y~.,data=dat)_x0007__x0007_ypred=predict(fit,xtest,data=test)_x0007__x0007_ER[i]=1-mean((ypred==ytest))_x0007__x0007_}; mean(ER)",1,1,1
When I compute the PC's aren't they 200 elements long? then how do we fit them onto y which is 300 long?,1,1,1
I can't coheres x into a data form that will run in a linear model?,1,1,1
"> How does one test if 2 vectors are orthogonal?_x0007__x0007_Two vectors, $a$ and $b$, are orthogonal if the inner product is zero:_x0007__x0007_$_x0007_sum a_i b_i = 0_x0007_$_x0007__x0007_For example:_x0007__x0007_    a <- c(1, 2, 3)_x0007_    b <- c(0.<phoneRedac>,  0.<phoneRedac>, -0.<phoneRedac>)_x0007_    sum(a * b)_x0007__x0007_Gives the result:_x0007_    _x0007_    -<zipRedac>.<zipRedac><zipRedac>111<zipRedac>e-17_x0007__x0007_which is pretty much zero, given the number of decimal places_x0007_included in the vectors._x0007__x0007_----_x0007__x0007_Or, to give a more straightforward example:_x0007__x0007_    a <- c(1, 0)_x0007_    b <- c(0, 1)_x0007_    sum(a * b)_x0007__x0007_Gives the result:_x0007_    _x0007_    0",1,1,1
"The PCs should be 1300 observations (x=300 + test.x=1000) long and have 200 variables. Y has 300 observations, so you need to take 300 observations from the PC matrix. Don't forget str() is your friend!",1,0,0
Thank you for asking this question. It helped me figure out what to do,1,0,0
"Q10.2.R1_x0007__x0007_I used the inner product definition as a1%*%a2, and try to see if their inner product is zero, I found last four have zero bu the submitted answer was wrong, Can anybody have any suggestions._x0007__x0007_Thank you._x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Thank you, thank you, thank you. Now I understand the question.",1,0,0
"I would like to also extend my gratitude to all those involved in preparing, delivering and supporting this course.  I've found it very interesting and well structured and hope to start putting some of the techniques to use in my own work._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,0
"I am wondering why Principal components aren't used in supervised learning such as regression. _x0007__x0007_It appears to me that by using a transformation of axes, the data can be made to line up \better\"" along certain directions. Use of regression afterwards of the transformation, may let us fit the data better to a line or a curve which we can then transform back to our original axes. _x0007__x0007_What are the difficulties with this method ? It doesn't seem that it is popular method. Why ? Is it just computationally more taxing or there are some other issues involved as well ? _x0007__x0007_Any thoughts ?_x0007__x0007_Thanks""",1,1,1
"Please, search for \All Videos\"" response, and after that you can download each video by the following site: http://www.clipconverter.cc/.""",1,0,1
"Great course, great book, great professors and researchers!",1,0,1
"Should data normalization be done to the entire data set before performing k-cross-validation or for each fold, the test and train sets should be normalized independently?_x0007__x0007_I think the drawback of normalizing the entire data set is that when building a model with the training set, that training set was already normalized using information from the test set so the model will have some dependence on the test set._x0007__x0007_On the other hand, if test and train sets are independently normalized the values may have slightly different 'scales'.",1,1,1
"Any advice or suggestions will be valuable for me! _x0007__x0007_My research for PhD relate mainly to  model selection approaches and feature selection methods (in regression tasks). I used something like Forward Stepwise Selection, Floating Forward Selection and some others my own modifications of some similar methods. Due to this online course, I discovered also ridge regression and Lasso algorithm that can also be helpful to me (at least for the comparison of models obtained by different methods)._x0007__x0007_But in my country (Ukraine) as well as in Russia it is very popular to use the Group Method of Data Handling (GMDH , http://en.wikipedia.org/wiki/Group_method_of_data_handling). There are a lot of information about this method in Russian language (at least because a creator of this method was the Ukrainian scientist Ivakhnenko and there are a scientific groups and schools that actively develop this method now). _x0007__x0007__x0007_Nevertheless, reputable professors of this online course did not say a word about this method. I just need to know, this is because you maybe not even have heard about it? Or maybe you used it before but it has not proved helpful in the analysis of real data and it turned out to be the worst method?_x0007__x0007_It is important for me to understand whether or not to spend my limited time (and nerves) to study GMDH? On the one hand, Ukrainian group of scientists claim that it is \the best of the best techniques\"" (so they say), but on the other hand, world known scientists in the field of statistical learning say nothing about this method. Where is the truth? :-)_x0007__x0007_It seems to me quite doubtful and excessive (and, among other things, it is quite timeconsuming for software implementation)._x0007__x0007_I need authoritative advice or opinion, whether or not to include this method for comparison with other methods. If someone used it, please share your experience and impression of its use. _x0007__x0007_Hope, the wording of my question is clear enough in my English. If not, then sorry for my English :-)""",1,1,1
"I_x0007_I didn't scale the data, but I recognize it is important. _x0007__x0007_I have an error in my value of about -0.004 compared to the answer given. I tried a quick and dirty value (this was my last answer from all the quiz set and I wanted to finish quickly). _x0007__x0007_Good work._x0007__x0007_Tony",1,0,1
"Though the course covered a great deal on the many applications of statistical learning, I don't think much was said about its general limitations or caveats.  I have two specific questions/concerns._x0007__x0007_First, can the methods presented be used to model feedback loops?  For example, can I use regression when the response affects future values of the features?_x0007__x0007_Second, what if the phenomena being modeled is changing or being disturbed during the period of time during which data is being collected.  In that case, even my best cross-validated model may be invalid.  How can I detect this condition?_x0007__x0007_The above problems are common in a business context.  Can I use the methods presented in the course to address them?",0,1,1
Thats great! Thanks much for this link!!!,1,0,1
Look at this thread:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac><zipRedac>4/discussion/forum/i4x-HumanitiesScience-Stats2<zipRedac>6-course-Winter2<zipRedac><zipRedac>4-course-material-feedback/threads/532aa<zipRedac>7<zipRedac>8<zipRedac>be<zipRedac>6635b<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>3,1,0,1
Look at this thread:_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac><zipRedac>4/discussion/forum/i4x-HumanitiesScience-Stats2<zipRedac>6-course-Winter2<zipRedac><zipRedac>4-course-material-feedback/threads/532aa<zipRedac>7<zipRedac>8<zipRedac>be<zipRedac>6635b<zipRedac><zipRedac><zipRedac><zipRedac><zipRedac>3,1,0,1
"Many methods do **not** require standardizing your data:_x0007__x0007_ 1. Linear regression_x0007_ 2. Polynomial regression_x0007_ 3. Spline regression_x0007_ 4. Logistic regression_x0007_ 5. Trees_x0007__x0007_Some methods do require standardizing your data, but most R packages do this automatically:_x0007__x0007_ 1. Ridge regression_x0007_ 2. Lasso regression_x0007_ 3. Principal Component Analysis/Regression_x0007_ 4. K-means Clustering",1,0,1
Shouldn't M be equal to 0 for half of the samples?,1,1,1
">First, can the methods presented be used to model feedback loops? For example, can I use regression when the response affects future values of the features?_x0007__x0007_Google for ARIMA, GARCH models. In this case, the simple linear model is not appropriate._x0007__x0007_>Second, what if the phenomena being modeled is changing or being disturbed during the period of time during which data is being collected. In that case, even my best cross-validated model may be invalid._x0007__x0007_Did you ask how frequently do you have to update the coefficients in the model? If yes, then the general answer to your question: it depends._x0007__x0007_It seems like you want to predict the stock market prices :)",1,1,1
"In my effort to answer 3.1.R1, I had submitted as my 2nd effort what was eventually indicated as the correct answer, however the validation system had marked my submission incorrect._x0007__x0007_Is there any way to validate the answers that I had submitted so as to determine why my 2nd submission was not credited?",1,1,1
"@Jack ... a very cogent explanation, sir!  Kudos!",1,0,1
"Warning Message: when I created the lm model, I had added the y vector as a column, then did:       _x0007__x0007_    lmfit = lm (y ~., data = extractedTrainSetFromPcOut)   _x0007__x0007_I got the warning error too. But then, for the test set, I added a y column of zeroes:     _x0007__x0007_    extractedTestSetFromPcOut$y = 0_x0007_    yPred = predict (lmfit, extractedTestSetFromPcOut)_x0007__x0007_Kind of silly to have to add the column of zeroes, but then the test set columns will match the train set columns, and predict will work.",1,0,1
Excellent example!,1,0,1
"I am doing the lab of Chapter 5 and run the bootstrapping code. It returns:_x0007__x0007_> set.seed(1)_x0007_> alpha.fn(Portfolio,sample(100,100,replace=T))_x0007_[1] 0.<phoneRedac>_x0007_> boot( data=Portfolio , statistic=alpha.fn , R=1000)_x0007_Error in boot(data = Portfolio, statistic = alpha.fn, R = 1000) : _x0007_  could not find function \isMatrix\""_x0007__x0007_I don't know what's going on. Does it mean that I have to transfer the data to a matrix?""",1,1,1
"I measured the length of the total PC's and it is 40,000 which is 200 by 200. Maybe I am measuring the wrong output?",1,1,1
"yes, it was one of those \get off the computer and the answer will appear to you\"" GOTCATAWATU moments, ta.""",1,0,1
Although I would love to spend loads of time on the course this is difficult with two jobs. Is there a chance of another extension or a heads up for when this course might be offered again? _x0007__x0007_I deeply appreciate that this course exists - it just kills me that I can't devote as much time as I would like to this course at the moment,1,1,1
"i don't believe this includes the Week 10 lectures, but those are found here. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/<zipRedac>3284<zipRedac>f7ac<zipRedac>f6e1<phoneRedac>0e",1,0,1
"ight)_x0007_$_x0007__x0007_and do a Taylor series expansion of the exponential function, we'll_x0007_end up with terms involving higher powers of the original features all_x0007_the way to infinity.  So this would be like an infinite dimensional_x0007_feature space._x0007__x0007_(Compared  with the polynomial kernel in equation (9.22), or the_x0007_plain support vector classifier in equation (9.21) which only have_x0007_finite powers of the features)""",1,0,1
"@CompSciPerson:_x0007__x0007_> Hello, Can anyone please let me know how to get Pmk for this problem?_x0007_> I'm not get what would be Pmk for this question?_x0007__x0007_On the lecture slides, slide 25/51, it explains what_x0007_$hat p_{mk}$ _x0007_is:_x0007__x0007_> Here_x0007_$hat p_{mk}$ _x0007_represents the propor",1,0,1
"Not managed this on a windows machine._x0007__x0007_Installed python then youtube-dl.  Created Videos folder, saved links.txt and download_vids.py into Videos folder._x0007__x0007_Set working directory from python.exe using command: _x0007__x0007_os.chdir('E:DropboxData AnalysisStatLearningVideos')_x0007__x0007_Tried to run:_x0007_python download_vids.py links.txt_x0007__x0007_Returns SyntaxError: invalid syntax on the 's' of download_vids.py",1,1,1
"I am not authoritative, but from looking at a few of the web sites it looks to me like GMDH is linear regression with interaction terms and subset selection. The Kolmogorov-Gabor polynomials shown on the Wikipedia site are linear regression equations with increasing numbers of interaction terms. The \COMBI\"" method seems to be doing exhaustive subset search at each interaction level using mean-squared-error and either a separate test set or LOOCV to compute the error. The \""MIA\"" method does subset search across multiple interaction levels. It seems to me you could do much of this with the `leaps` package as shown in lab 6.5.1 of the book.""",1,0,1
"I suppose you evaluate the \latent variables\"" via the first 5 principal components of the \""merged matrix\"" x, x.test. That way you get 5 variables  and 1300 observations. On the other hand, to \""regress y on the first 5 p.c.\"" you do the same thing to y, and get one variable with 300!!! observations. Isn't there something weird here?""",1,1,1
"For the first question you might want to look at the compositions and robComposition packages in R.  These are designed to handle \closed\"" data.""",1,0,1
"When I run (for example):_x0007__x0007_    x1 <- matrix(rnorm(500, mean=0, sd=1), 500, 6)_x0007__x0007_I get:_x0007__x0007_                [,1]        [,2]        [,3]        [,4]        [,5]        [,6]_x0007_    [1,]  0.<phoneRedac>4  0.<phoneRedac>4  0.<phoneRedac>4  0.<phoneRedac>4  0.<phoneRedac>4  0.<phoneRedac>4_x0007_    [2,] -0.<phoneRedac>1 -0.<phoneRedac>1 -0.<phoneRedac>1 -0.<phoneRedac>1 -0.<phoneRedac>1 -0.<phoneRedac>1_x0007_    [3,]  1.<phoneRedac>1  1.<phoneRedac>1  1.<phoneRedac>1  1.<phoneRedac>1  1.<phoneRedac>1  1.<phoneRedac>1_x0007_    [4,] -0.0<phoneRedac> -0.0<phoneRedac> -0.0<phoneRedac> -0.0<phoneRedac> -0.0<phoneRedac> -0.0<phoneRedac>_x0007_    [5,] -0.<phoneRedac>8 -0.<phoneRedac>8 -0.<phoneRedac>8 -0.<phoneRedac>8 -0.<phoneRedac>8 -0.<phoneRedac>8_x0007_    [6,]  1.1<phoneRedac>  1.1<phoneRedac>  1.1<phoneRedac>  1.1<phoneRedac>  1.1<phoneRedac>  1.1<phoneRedac>_x0007__x0007_Is this normal behavior for rnorm inside a matrix call?  There is repetition of x_p in each of p columns?  Not exactly random-looking to me?  Is there something wrong with my code?  Is my random number generator broken??",1,1,1
"I'm not sure what you mean by \you do the same thing to y, and get one variable with 300!!!\""?_x0007__x0007_What is wrong with building a fit for a one-dimensional (scalar) response variable, which is what y is in this context, over 5 predictors (the top 5 principal components of the PCA analysis), and for that fit to be derived from a set of 300 observations?_x0007__x0007_So this is a case of p=5, N=300, and a scalar response variable.""",1,1,1
"Agreed, tremendous value from the lectures and the R labs! Warm thanks.",1,0,1
"Hi Sam,_x0007_I will do that as I do have the book as both hard and soft copies. I will also look for this course to be offered for credit again so I may begin the certificate program.",1,0,1
By defult the matrix is filled row by row; so every column in your matrix is the same vector created by rnorm().,1,0,1
"Page 86 of the text states:_x0007_\Rather than rely on the individual coefficients, we can use an F-test to test H-null of Beta1 = Beta2 = <zipRedac>;  this does not depend on the coding.  This F-test has a p-value of <zipRedac>.96, indicating that we cannot reject the null hypothesis that there is no relationship between balance and ethnicity.\""_x0007__x0007_Would I be correct in presuming that this test was and could be conducted in R using the following code:_x0007__x0007_> credit <- read.csv(\""http://www-bcf.usc.edu/~gareth/ISL/Credit.csv\"")_x0007_> str(credit)_x0007_> fit = lm(Balance ~ Ethnicity, credit)_x0007_> summary(fit)_x0007__x0007_... and by using the resulting F-Statistic at the bottom of the summary:_x0007_Call:_x0007_lm(formula = Balance ~ Ethnicity, data = credit)_x0007__x0007_Residuals:_x0007_    Min      1Q  Median      3Q     Max _x0007_-531.<zipRedac><zipRedac> -457.<zipRedac>8  -63.25  339.25 148<zipRedac>.5<zipRedac> _x0007__x0007_Coefficients:_x0007_                   Estimate Std. Error t value Pr(>|t|)    _x0007_(Intercept)          531.<zipRedac><zipRedac>      46.32  11.464   <2e-16 ***_x0007_EthnicityAsian       -18.69      65.<zipRedac>2  -<zipRedac>.287    <zipRedac>.774    _x0007_EthnicityCaucasian   -12.5<zipRedac>      56.68  -<zipRedac>.221    <zipRedac>.826    _x0007_---_x0007_Signif. codes:  <zipRedac> ‘***’ <zipRedac>.<zipRedac><zipRedac>1 ‘**’ <zipRedac>.<zipRedac>1 ‘*’ <zipRedac>.<zipRedac>5 ‘.’ <zipRedac>.1 ‘ ’ 1_x0007__x0007_Residual standard error: 46<zipRedac>.9 on 397 degrees of freedom_x0007_Multiple R-squared:  <zipRedac>.<zipRedac><zipRedac><zipRedac>2188, Adjusted R-squared:  -<zipRedac>.<zipRedac><zipRedac>4818 _x0007_F-statistic: <zipRedac>.<zipRedac>4344 on 2 and 397 DF,  p-value: <zipRedac>.9575_x0007__x0007__x0007_I believe that I have remembered this technique correctly, but I would certainly appreciate validation, if possible.""",1,1,1
"I'm still curious about this, but I just ended up building my own custom function._x0007__x0007_    build.ran.matrix <- function(mean, sd, row, col){_x0007_      res.matrix <- matrix(NA, row, col)_x0007_      for(i in 1:col)_x0007_        res.matrix[, i] <- rnorm(row, mean=mean, sd=sd)_x0007_      return(res.matrix)    _x0007_    }",1,0,1
"Thanks for this - unfortunately robComposition does not seem to be available for \  R version 3.0.2 (2013-09-25) -- \""Frisbee Sailing\""  \"". But I will chase it up anyway on the web.""",0,0,1
"<redacted>:_x0007__x0007_> When I run (for example):_x0007_> _x0007_> x<zipRedac> <- matrix(rnorm(500, mean=0, sd=<zipRedac>), 500, 6)_x0007_> _x0007_> I get:_x0007__x0007_> ..._x0007__x0007_> Is this normal behavior for rnorm inside a matrix call?_x0007__x0007_Yes._x0007__x0007_> There is repetition of x_p in each of p columns? Not ex",1,0,1
"<redacted>, if your post below is any indication, to work on Windows, the python script would probably need to say 'youtube-dl.exe' instead of 'youtube-dl'.",1,0,1
"Thanks Jeff, another clever R tool :)",1,0,1
Thanks for your comments but still I would like to know if one should standardize the entire data set or test/train sets separately.,1,1,1
Excellent course!,1,0,1
"wrt part 2, missing data, there are several options. Which one you use depends on your objectives, the data and any patterns in the missing data. The simplest choice is to deleted any cases with missing values, see complete.cases in R. This is fine so long as you have sufficient data and there is no pattern in the missing cases, there is of course a downside, you've lost the information in the deleted cases and also you've built a model that cannot handle missing data in the test set. _x0007__x0007_Alternatively you can fill the missing data in (called imputation) with an appropriate value, say mean or median. If you do so, you should create an additional predictor that marks which values were imputed, do this for each predictor seperately. Then you can build your model using the new improved set of predictors (no missing values). If the added predictors are significant, it tells you there may be a pattern in the missing values that can be useful in prediction. There are more complicated methods including multiple imputation (see google). _x0007__x0007_Alternatively (again) some methods can handle missing values natively (for instance random forests). Two references are Little and Rubin - Data Analysis with missing data, though I generally prefer Enders - Applied Missing Data Analysis._x0007__x0007_Unfortunately which is best depends on your objective and the data. If you don't care about missing values in the test (or real world usage) and you have sufficient data you can delete the cases, else you need to develop an imputation scheme and which is best is data dependent. Which ever you use I recommend adding the extra predictors, it is not uncommon to find patterns in missing bits.",1,0,0
"<redacted>:_x0007__x0007_By the way, on an unrelated note:_x0007__x0007_Thanks for being such a great contributor to the forum.  _x0007__x0007_When I've been reading the forum, I've noticed that you have_x0007_very often taken time to answer people's questions, share_x0007_your own thoughts, and give",1,0,1
"Thanks for your kind response._x0007_Now I can understand what makes the difference._x0007__x0007_So basically, PCA compute the eigens of covariance matrix X'X/(n-1), whereas SVD compute the eigens of X'X. So if we scale the singular values by sqrt(n-1), we get standard deviations of the principle components._x0007__x0007_Thank you again! This really helps me understand  PCA and SVD, also the link between them!",1,0,0
"CV (cross validation) error is a random variable due to randomness of the sample S as well as the randomness of the partition (S1, S2, ..., Sk) for k-fold CV. _x0007_So when we try to define the expectation (or other statistics such as variance),_x0007_we should take the expectation over both S and (S1...Sk),_x0007_i.e. E_{S, pi}[cv error], where S is of n examples, and pi is a random permutation of 1,...,n._x0007__x0007_But what I was wondering is: do we really need to take the expectation over pi?_x0007__x0007_I intuitively think that the randomness of pi can be obtained by the randomness of S,_x0007_so I believe it's equivalent to define it as E_{S}[cv error]. _x0007_But I can't prove or disprove it._x0007__x0007_Any one working on this?",1,1,1
Thank you. You've been wonderful too.,1,0,1
Add me on linkedin www.linkedin.com/in/<nameRedac_<anon_screen_name_redacted>>,1,0,1
"the loadings matrix describes weights to be placed on each of the original variables to create the actual PCs, not the PCs themselves.",1,0,1
"Very nice explanation...  The concept of an infinite dimensional feature space can be difficult to wrap your brain around, but the way the math works out really is a beautiful thing.",1,0,1
"alhf...bravo, my friend.... many thanks for the efficiency tips.",1,0,1
Really like Introduction to Statistical Learning...plan on getting Elements of Statistical Learning next. _x0007__x0007_Does anyone else have any recommended books on the topic?,1,1,1
"Since it comes to the end of the course, is there any possibility that we could have all the discussion section as a document and downloading in case we want to come back and revisit in future? Thanks.",1,1,1
"> ss <-summary(prcomp(rbind(x,x.test),scale=TRUE))$importance_x0007__x0007_> ss[1:3,1:5]",1,0,1
"According to the notes and the slide the equation is D = - sigma(pmk * log(pmk)._x0007__x0007_When I answered the question, grader did not accept. After getting them wrong for 5 submission, I learned that the grader uses ln instead of log. Not sure which is right, the grader or the course notes. Can the instructor please clarify? Thanks, <nameRedac_<anon_screen_name_redacted>>",1,1,1
"That would indeed just be an estimate of the test error for the model/method by which you select the optimal gene set. If you plan to use the same method to select the optimal gene set on the whole data set, that should be a reasonable way to estimate test error for that method. _x0007__x0007_However, since you're interested in finding the right gene set as well, what you might do is have a training and holdout set. On the training set, perform $K$-fold CV, but not to estimate errors, rather to see the $K$ different optimal gene sets. Then given these, perhaps you can combine them in some way that would be better than just selecting the optimal for the training set overall._x0007__x0007_For example, if you had a way to compute variable importance like the random forest does, you could take the average (CV) importance for each of the variables/genes through CV, then take the top ones (however many you want to use) to form your optimal set. In other words, the top ones would be those genes that hopefully keep showing up as important when predicting the validation set (the $1/K$th fraction from CV). That may give you a slightly different \optimal\"" gene set than fitting once on the whole training set. _x0007__x0007_Or use the lasso on training, then just predict on a test set, or use stepwise selection on training set to get your gene set... _x0007__x0007_These are just ideas / things to try.""",1,0,0
"I have a problem with 9.R. I produced following code to train my SVM:_x0007__x0007_    for(j in <zipRedac>:<zipRedac>0) {_x0007_    trainMatrix = matrix(rnorm(<zipRedac>000), <zipRedac>00, <zipRedac>0)_x0007_    trainMatrix[<zipRedac>:50,<zipRedac>:5] = trainMatrix[<zipRedac>:50,<zipRedac>:5] + <zipRedac>_x0007_    traindf <- data.frame(trainMatrix)_x0007_    traindf$y = 0_x0007_    for (i in <zipRedac>:<zipRedac>00) {_x0007_    	if(i <= 50) {_x0007_    		traindf[i,]$y = 0_x0007_    	} else {_x0007_    		traindf[i,]$y = <zipRedac>_x0007_    	}_x0007_    }_x0007_    svmfit = svm(formula = y ~ ., data = traindf, kernel = 'radial')_x0007__x0007_And the following code to test it:_x0007__x0007_    testMatrix = matrix(rnorm(<zipRedac>0000), <zipRedac>000, <zipRedac>0)_x0007_    testMatrix[<zipRedac>:500,<zipRedac>:5] = testMatrix[<zipRedac>:500,<zipRedac>:5] + <zipRedac>_x0007_    testdf <- data.frame(testMatrix)_x0007_    testdf$y = 0_x0007_    for (i in <zipRedac>:<zipRedac>000) {_x0007_    	if(i <= 500) {_x0007_    		testdf[i,]$y = 0_x0007_    	} else {_x0007_    		testdf[i,]$y = <zipRedac>_x0007_    	}_x0007_    }_x0007_    testdf$predict = predict(svmfit, testdf)_x0007_    for (i in <zipRedac>:<zipRedac>000) {_x0007_    	if(testdf[i,]$predict <= 0.5) {_x0007_    		testdf[i,]$predict = 0_x0007_    	} else {_x0007_    		testdf[i,]$predict = <zipRedac>_x0007_    	}_x0007_    }_x0007_    result = 0_x0007_    for (i in <zipRedac>:<zipRedac>000) {_x0007_    	if(testdf[i,]$y <= testdf[i,]$predict) {_x0007_    		result = result + <zipRedac>_x0007_    	}_x0007_    }_x0007_    print(result)_x0007__x0007_I got about 93% of correct answers in first task. But neither 0.93 nor 0.07 is passing the test. I am wondering what is wrong with my solution. Maybe assumption that result of predict function above 0.5 means <zipRedac> and below means 0? Maybe I am entering result in wrong format? If someone help me I will be grateful.",1,1,1
"The Taylor series expansion for $exp(x)$ is readily available, you would just need to expand the summation in place of $x$ but I think this suffices to see the infinite dimensionality. See $e^x$ here: http://en.wikipedia.org/wiki/Exponential_function",1,0,1
"in statistics, \log\"" is always base e""",1,0,0
"I'd say it really depends on which direction you're looking to go. Stats? Machine Learning? R proficiency, like more data manipulation, visualization? For overall theory ISL/ESL do a pretty darn good job in my opinion. _x0007__x0007_You may consider perusing cross-validated and stackoverflow questions on the topic, like these: _x0007__x0007_http://stats.stackexchange.com/questions/<zipRedac>2386/machine-learning-cookbook-reference-card-cheatsheet_x0007__x0007_http://stackoverflow.com/questions/<phoneRedac>/learning-r-where-does-one-start_x0007__x0007_But, to add a personal flair, here's a list that I bookmarked some time ago when I was interested in getting more of an understanding of probability, but nothing too math-heavy: _x0007__x0007_http://www.stat.berkeley.edu/~aldous/<zipRedac>57/books.html",1,0,1
"As long as you are consistent, the base of the log technically does not make a difference.  These days, log in the context of higher mathematics (calculus, et al) usually denotes a natural logarithm, unless the base is specified._x0007__x0007_Some reasons for preference toward natural logarithms can be found here: http://stats.stackexchange.com/questions/<zipRedac>768<zipRedac>/what-is-the-reason-why-we-use-natural-logarithm-ln-rather-than-log-to-base-10",1,0,0
Good point although the grader notation should be consistent with the notes. Thanks for the question Ramesh.,1,0,1
Thank you <redacted> and <redacted>.,1,0,1
"10.7, Applied, p. 416, they ask to compare the squared Euclidian distance and quantity (1 - r). They describe r_i,j as \the correlation between the ith and jth observations\"". But, in the text example and the first line of the question, they refer to \""correlation-based distance\"". For the comparison part, do they mean the correlation value, or the correlation-based distance value?      _x0007__x0007_For correlation-based distance, I think means: for 2 observations, calculate the vector of differences of Obs 1 and Obs 2 for each variable. Then, correlate the vector.""",1,1,1
"I am not quite done, but have been thinking the same thing - this is an EXCELLENT course!  From the near perfect balance of delving into functional form, yet retaining an overall accessibility of the material, to the interviews with the people who've come-up with some of the methods._x0007__x0007_I fully intend to go through the labs in ISLR, and will be holding on to both books for future reference._x0007__x0007_A big heartfelt thank you for putting this together!_x0007__x0007_BTW: If professors Hastie and Tibshirani are representative of the grade of faculty at Stanford - I can see what all the fuss is about!",1,0,0
"While I don't understand your problem, I don't think you would ever want to run PCA on your response variables, especially if they have different physical meaning. Because in that case, when you predict your transformed responses, you will be predicting something that doesn't have any physical meaning, which most likely isn't terribly useful. _x0007__x0007_As far as categorical PCA, there was some info on that in this thread mentioning how to deal with that. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/5324f<zipRedac>34a47ddb9b<zipRedac>e<zipRedac><zipRedac><zipRedac><zipRedac>23",1,0,1
"Many thanks Scott. I will follow-up your leads - my preference is to pursue the imputation approach, even though this may be more work. A lot of the geochemical data I work with (from published papers etc.) has_x0007__x0007_ -  poor analyses (low resolution) for some elements or oxides and these should be deleted from the data set anyway, and_x0007_ - an incomplete list of trace elements",1,0,1
"ESL is great for getting deeper into the theory. Another book I have been reading is Applied Predictive Modeling by Max Kuhn and Kjell <nameRedac_<anon_screen_name_redacted>>. It has a strong focus on \how to do it\"", similar to this course, but covers a broader selection of methods.""",1,0,1
"Thanks for quick response. Indeed my code provides only one iteration (I wanted to provide concise code). I know all this loops and conditionals look ugly, but I have CS background and I don't know about all R catches._x0007__x0007_But now I came to some concrete questions:_x0007__x0007_1. Did you use multivariate normal generator?_x0007_2. What values of parameters should I pass to svm? (for example cost and scale)_x0007_3. Predict function returns real number, how should I convert it to certain class?",1,1,1
This is really clear! Thanks!,1,0,0
50% I believe. Hence the line on the progress page.,1,0,1
"Thanks for your comments._x0007__x0007_Springer is offering a 30% discount, while Amazon is not._x0007__x0007_Actually, I checked Springer website today and it looks like they have removed the discount. Maybe it was till March 21st, and was not postponed as the course itself._x0007__x0007_What a pity, I really wanted to buy those books.",1,0,1
"I would like to express my gratitude for this interesting course both teachers and supporting team.  It have been a little hard, because my native language is not English and because my algebra, combinatory and error theory knowledge was forgotten and I have had to study again. I have not too much free time, and working a little bit every night I know enough, to know there are a lot of things i do not know about statistics._x0007__x0007_Thank you",1,0,0
"Thanks a lot for these references, Evgeniy.",1,0,1
"Aaaah of course! I didn't think that logistic regression is very similar to linear regression. So here actually we have more unknowns (predictors) than equations (samples). Since X has column rank n, the linear system Xa=y has infinite solutions. The least square system X'Xa=X'y thus has a single solution, which corresponds to the solution with the smallest Euclidean norm, among the infinite solutions of the original problem. Thanks!",1,0,1
"I'm wondering why we combines x,x.test to run PCA? Won't that create look ahead bias? I thought it's more fair to run PCA on x only, then use the loading to calculate PCA scores on x.test to test prediction accuracy?",1,1,1
I agree with you!_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/<phoneRedac>c7651ce<phoneRedac>_x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/5329ca0def<phoneRedac>0014,1,0,1
This looks promising and the price is right:_x0007_*Forecasting: Principles and Practice* https://www.otexts.org/fpp/,1,0,1
Thanks alhf.  I appreciate you taking the time to make this involved clarification.,1,0,1
Go back to chapter 6 in the book and check out the pcr function (principal components regression)._x0007_You can specify how many principal components to use in the regression model.,1,0,0
Thank you to all involved.  _x0007__x0007_I found the applied R courses extremely valuable coming as I do from an industry background. _x0007__x0007_Thank you again.,1,0,0
"The course was great, and the book is a gem._x0007_ _x0007_Thank you and congratulations to all involved, including also the book publisher that allowed the free pdf. I did end up buying the hard copy anyway, definitely worth the money._x0007__x0007_If I could express a wish, I would like a follow up book (and a course?) purely on applications. A short list of real problems your team worked on with detailed explanations on the steps/reasoning towards the solution. The idea would be for the reader to get a glimpse on how experts tackle real life problems. Applications of Statistical Learning?_x0007__x0007_The course has been real fun. Thank you again._x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"Hi everyone,_x0007__x0007_Does the order of the columns in the regression matrix affects the solution obtained by LASSO?_x0007__x0007_Thanks!",1,1,1
"Tom, not sure if this is a typo, but you wrote \predict y using the fit\"" when you should predict `y.test`.""",1,0,1
"It might be thought of a \bias\"", but it can also be thought of as a smarter way to do prediction if it helps improve the performance. _x0007__x0007_In detail:_x0007__x0007_The test error we would get would be for a procedure that includes the test x values as part of the model-fitting process (sometimes called transductive learning)._x0007__x0007_But of we would like to know how our present model performs on all future data, without refitting, then yes, we should compute the PCs on the training data, and apply the transformation to the test data.""",1,0,0
We loved your post Kate. Thank you!,1,0,1
"After some scrambling I managed to complete the quiz for chapter 9._x0007__x0007_Without posting the answers, my svm(radial & linear), as well as the logistic regression error rates were very much in the same neighbourhood (><zipRedac>% diff)._x0007__x0007_I had a test set of <zipRedac>0000, and a training set of <zipRedac>00.  I ran the train/test cycle <zipRedac>00 times for each.  I noticed that if I ran it <zipRedac>000 times, the error rate for the radial svm was close to what the grader expected._x0007__x0007_All told, is this simply a case of my not running enough train/test cycles to get to the precision shown in the grader?",1,1,1
"So I choose to do it step by step because timeseries function wasn't working and it was my last chance. So, as Didier said the standard error is much bigger. This is what i suggest. _x0007__x0007_> boot.block <- sapply(1:1000, function(i){_x0007_> _x0007_>   selec.blocks <- sample(unique(Xy$block), length(unique(Xy$block)),_x0007_> replace = TRUE)_x0007_> _x0007_>   sub.blocks <- ldply(selec.blocks, function(bloc){_x0007_>     subset(Xy, block == bloc)   })_x0007_> _x0007_>   mod <- glm(y ~X1+X2, data = sub.blocks)_x0007_> _x0007_>   coef(mod)[2] })",1,0,1
"Here is my code, it returns MSE = 0.<phoneRedac>. Any idea what's wrong?_x0007__x0007_    xx = rbind(x,x.test)_x0007_    pca.xx = prcomp(xx)_x0007_    _x0007_    # 1300x5 matrix of PC's_x0007_    pc = as.matrix(xx) %*% pca.x$rotation[,1:5]  _x0007_    _x0007_    # train and test data_x0007_    train = data.frame(cbind(y,pc[1:300,]))_x0007_    test = data.frame(pc[301:1300,])_x0007_    _x0007_    fit = lm(y~., data=train)_x0007_    p = predict(fit, test)_x0007_    MSE = mean((p-y.test)^2)",1,1,1
"It doesn't and it shouldn't. You can convince yourself quite quickly by repeating the exercise presented in Prof. Hastie's lab and randomly shuffle the columns around before fitting a lasso regularised regression. The model comes back with exactly the same features selected and with coefficients that have the same values within a small tolerance margin. If you think about it, this is what should happens since LASSO tries to identify features by penalising outliers but doesn't really care about the order within which the different features are examined. For completeness here is the code I used:_x0007__x0007_s<-sample(seq(20))_x0007__x0007_z<-x(,s)        _x0007_##### x is the matrix name as per the convention used in the lab and I called  the_x0007_#####          new matrix z to keep track of data_x0007_colnames(z)<-colnames(x)[s]_x0007__x0007_cv.lasso.shuffled=cv.glmnet(z,y)_x0007__x0007_From there you can print coefficients, plot and inspect results at your leasure",1,0,0
"If your pca model is in pcr.out, then pcr.out$x has the principal components._x0007__x0007_It has 200 components and you just need the first 5 columns, and the first 300 rows only (the PCs corresponding to the training observations that correspond to y, which has 300 observations):_x0007__x0007_    pcs <- pcr.out$x[,c(1,2,3,4,5)][1:300]_x0007_    ds <- data.frame(pcs, y)_x0007__x0007_Then create a linear model on ds.... and so on.",1,0,0
"Hi Shoreline _x0007__x0007_That is close but it did not quite work for me ... I suggest_x0007__x0007_    train <- data.frame(pca.out$x[1:300,c(1,2,3,4,5)], y)_x0007_    test <- data.frame(pca.out$x[301:1300,c(1,2,3,4,5)], y.test)",1,0,1
"Thanks for the idea, teranceee.",1,0,1
"Yes, and keep in mind that the X variables were generated using a random process. So no two students will have the exact training and test data sets (unless the students used the same seed values).",1,0,1
The R assignments/labs are amazing. I hope that 3rd ed of ESL will have R labs!,1,0,1
"On what planet does the relationship between height and income equate to grades?_x0007__x0007_I respectfully suggest that a better way to phrase this question would be: \Suppose there is a planet where parent income and student height are related to grades. Which of the following tools would be well suited for predicting if a student will get an A in a class based on the student's height, and parents’ income?\""""",0,1,1
Would someone be kind enough to tell me how many data files we used throughout the class.  I think we had a total of 3 that we had to download and import into R?,1,1,1
Thank you so much for this course and for the book! _x0007_I really enjoyed taking this course!,1,0,1
"The question was about tools, not about the actual relationship.",1,0,1
"It was a great pleasure to listen and practice with both of you and the discussion community. _x0007_Enjoyed too youre great sense of humour ;-)_x0007_And actually, I hope see you soon perhaps on future courses :-)",1,0,1
I agree with veroneti. And for me it was a privilege having the possibility to take a course taught by people who I cite in my Ph.D. thesis! :),1,0,1
"Good point.  Yeah, I figured if I were to up the iterations by a couple of orders of magnitude, the minor distinctions between efficacy of SVM and LR methods would become more pronounced.  That said, it really did seem like they were very much in the same ballpark._x0007__x0007_Thanks for taking the time to reply!  :)",1,0,0
"That is incorrect.  See page 396.  If $r_{ij}$ is the correlation between $x_i$ and $x_j$, then $1-r_{ij}$ can be used as a distance metric.  It is called \correlation based distance.\""_x0007__x0007_I don't know what \""correlate the vector\"" means in your second paragraph, but you should just be doing 1-cor($x_i,x_j$).""",1,0,0
Agree with the above comments. Great course indeed. Enjoyed every bit of the course material as well as the sense of humor of Prof.Tibshirani and Prof.Hastie. I am looking forward to future courses.,1,0,1
Scigeek is correct.,1,0,1
"Thanks for offering this course.  I found it very useful._x0007__x0007_If I were to recommend this class to friends or colleagues, when could I tell them that it will next be offered?  Please keep us in mind as word of mouth advertisers for the course and pass along details when they are available.",1,1,1
"I posted an answer to your comment. Long story short, the problem has nothing to do with the modeling. It has to do with the fact that R does not interpret column names in a matrix correctly.",1,0,0
"### create some fake data_x0007_    set.seed(1)_x0007_    _x0007_    x1 <- rnorm(1000, 50, 10)_x0007_    x2 <- rnorm(1000, 500, 20)_x0007_    _x0007_    y <- 500 + 50*x1 + 5*x2 + rnorm(1000,0,100)_x0007_    _x0007_    mydata <- data.frame(x1,x2,y)_x0007_    _x0007_    ### split mydata between train and test_x0007_    _x0007_    set.seed(10)_x0007_    _x0007_    train <- sample(nrow(mydata), 700, replace=F)_x0007_    _x0007_    training <- mydata[train,]_x0007_    y <- training[,\y\""]_x0007_    _x0007_    test <- mydata[-train,]_x0007_    y.test <- test[,\""y\""]_x0007_    _x0007_    ### create a test data set where the x columns are switched, _x0007_    ### but the column names_x0007_    ### are still correct_x0007_    _x0007_    test2 <- as.data.frame(cbind(test[,\""x2\""], test[,\""x1\""], test[,\""y\""]))_x0007_    _x0007_    names(test2) <- c(\""x2\"",\""x1\"",\""y\"")_x0007_    _x0007_    head(test2)_x0007_    head(test)_x0007_    _x0007_    ### create model matrices_x0007_    _x0007_    x <- model.matrix(y~., training)[,-1]_x0007_    _x0007_    x.test <- model.matrix(y~., test)[,-1]_x0007_    _x0007_    x.test2 <- model.matrix(y~., test2)[,-1]_x0007_    _x0007_    head(x)_x0007_    head(x.test)_x0007_    head(x.test2)_x0007_    _x0007_    ### fit lasso model_x0007_        _x0007_    library(glmnet) _x0007_    lasso.fit <- glmnet(x, y, alpha=1)_x0007_    lasso.cv <- cv.glmnet(x, y, alpha=1)_x0007_    _x0007_    lasso.pred <- predict(lasso.fit, newx=x.test, s=lasso.cv$lambda.1se)_x0007_    _x0007_    lasso.pred2 <- predict(lasso.fit, newx=x.test2, s=lasso.cv$lambda.1se)_x0007_    _x0007_    ### Test set MSE_x0007_    _x0007_    MSE.correct <- mean((y.test - lasso.pred)^2)_x0007_    MSE.wrong <- mean((y.test - lasso.pred2)^2)_x0007_    _x0007_    MSE.correct_x0007_    MSE.wrong""",1,0,1
"In Lecture Ch4 Slide 17/40, there is a graph of Coefficient Variance Vs Control/Case Ratio. Please, what does mean \Coefficient Variance\"" here? Might be Coefficient Standard Error from the R outpout?_x0007__x0007_cheers,""",1,1,1
Thanks for your time and big effort. Thanks for the great course and the amazing book.,1,0,1
"Indeed, it was a great class.  Thank you very much!",1,0,1
Virtually the letters and pictures are not visible correctly. Is this common or there is a way to download it?,1,1,1
Thank you very much!,1,0,1
"Suppose I'm doing forward selection (with my dependent variable X) and I start by adding feature A, and then feature B. By the Hierarchy Principle my model is now_x0007_X = c + c_1A + c_2B + c_3(A*B)_x0007__x0007_I then add a third feature D. What will my mode look like now? Will it be_x0007_X = c + c_1A + c_2B + c_3D + c_4(A*B*D)_x0007_or will it be_x0007_X = c + c_1A + c_2B + c_3D + c_4(A*B) + c_5(A*D) + c_6(B*D) + c_7(A*B*D)_x0007_Anyone know how R does it?",1,1,1
<redacted>:_x0007__x0007_> I disagree: I don't see why your matrix pcall above has 1000_x0007_> rows._x0007__x0007_It doesn't — it has 1300 rows (300+1000).  This can be seen_x0007_by scanning down the first column.  The training and test_x0007_sub-matrices are labelled and numbered separate,1,0,1
coursera offers great R programming classes. https://www.coursera.org/course/rprog,1,0,1
"I also had problems with this question, in particular \Linear regression is very extensible and can be used to capture nonlinear effects\"". OK I know from previous knowledge that higher order terms may be added (x^2), or interactions (x1*x2) but the material thus far has not covered this so I find the question a bit unfair.""",0,0,0
I had the same answer as algorithmiac. I am completely baffled by this one.,1,1,1
"You are confusing rows and columns. The principal components are the columns in your notation -- that's why you have five. And the principal comps live in the space of predictors, so they have 200 elements, not 1000, not 1300.",1,0,0
"Kids whos parents have a high income tends to get better grades. Maybe due to better home enviroment (more money = doesnt have to chare a room with noisy siblings), more support/pressure from the parents, extra tutoring or whatever. Hight, maybe not.",1,0,1
"I'm having trouble getting a correct answer for the last part of this question. This is annoying because it seems like it should be pretty simple! Please would someone be kind enough to look through my code and tell me what I'm doing wrong._x0007__x0007_ols.train = data.frame(x,y)<br>_x0007_ols.test = data.frame(x.test,y.test)<br>_x0007_ols.fit = lm(data=ols.train, y~.)<br>_x0007_pred = predict(ols.fit,newdata = ols.test, se.fit = TRUE)_x0007__x0007_I'm getting an answer of about 1.072.",0,1,1
Thank you for this wonderful course. It was very well taught (of course)! You guys are great. Hopefully you'll consider more MOOCs.,1,0,1
"I assume you're talking about a features prescreen using bivariate relationships between each feature and the dependent variable. please read slides 19-22 of Ch 5 notes. the answer is, \be careful\"".""",1,0,1
"Question 10.R.1  says to \Load the data x, y, x.test, and y.test from 10.R.RData.\""  ... I've been scouring the course website and have completely lost track of where to find this data source so I can use it to answer these questions. Is there anyone still taking the last chapter or reading these discussion pages who can redirect me?  _x0007_kind regards...""",1,1,1
"You don't show how you got your final answer, but if you use_x0007__x0007_mean((pred-y.test)^2)_x0007__x0007_you get 3.657 (which I also got), which is closer to the actual answer but still out._x0007__x0007_This was using argument of predict with se.fit=F_x0007__x0007_if se.fit=T (as you have), you will get 3.657 if you type_x0007__x0007_mean((y.test-pred$fit)^2)",1,0,1
nevermind._x0007_read other posts... specifically _x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-c6-course-Winter2014-course-material-feedback/threads/530cc011b7bb<phoneRedac>3_x0007_which reminds me to read more closely... the dataset is a clickable link. (sigh),1,0,1
"Hello Everyone. My name is <nameRedac_<anon_screen_name_redacted>>, Bachelor Degree in Statistics & Economics, Master in Operation Research & Decision-Making Strategy with expertise in Operation Research. _x0007_Feel free to connect with me on Linkedin: http://linkedin.com/in/gianniderossi_x0007__x0007_Best wishes,_x0007_GDR",1,0,1
I have just finished. Thanks a lot for this great course. I enjoyed very much and learned a lot of new things. Best regards.,1,0,1
"This course was my first exposure to MOOC's and definitely will not be my last._x0007_The course has been more than I expected to be and I am impressed with what seems to be happening in statistics today given that my last formal exposure to the subject was an undergraduate course in the 1970's._x0007_The mathematical notations used were not always as clear as they might but that is another field I am rather farther away from in time than current students/practitioners._x0007_In summary, I enjoyed the course and learned some new things.",1,0,1
Never mind.. Got it!! Thank you alhf for a good explanation.,1,0,1
"<redacted>:_x0007__x0007_> You are confusing rows and columns. _x0007__x0007_I think they're OK._x0007__x0007_> The principal components are the columns in your notation_x0007_> -- that's why you have five._x0007__x0007_Yes, that's right._x0007__x0007_> And the principal comps live in the space of predictors,_x0007_> so ",1,0,1
try mean((pred$se.fit - y.test)^2),1,0,1
An I,1,0,1
"Yes, I looked recently and thought the same.  Pity.",0,0,1
"Without giving away the answer, could someone explain the explanation in the solution for 5.R.R3?  In particular, how does \i.i.d. bootstrap\"" work?  How did that \""same assumption mess up the lm\""?  Thanks in advance!""",1,1,1
"Hello,_x0007__x0007_I enjoyed the course very much (the technical content and the interviews).  Was wondering if it might be possible to order a autographed copy of Elements of Statistical Learning by Dr. Hastie and/or Dr. Tibshirani._x0007__x0007_Thank you,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>",1,1,1
"I would really like to thank Rob, Trevor and the team for this fantastic course. I really learned a lot about the basic ideas of statistical learning! The lecture is really instructive and interesting._x0007__x0007_Thank You!",1,0,1
The basic multiplication rule of event did the trick. Thanks !,1,0,1
"San Jose, Ca",1,0,1
"\i.i.d.\"" means \""independent and identically distributed\"". Normal bootstrapping tacitly assumes that there is no connection between subsequent data points, i.e. that they are independent. This makes sense for a lot of kinds of data: if you sample 100 voters, it doesn't matter whether John Smith is data point #2 or #99 - his voting behavior is determined by other things (political stance, campagin ads, whatever). A lot of statistical methods require that this is true for the data, so that you can in effect look at a single data point without considering which ones come before/after it._x0007__x0007_Some kinds of data, however, are NOT independent. If you take the weather of 100 consecutive days, the user ratings of 10 consecutive versions of a product, or other data that develops over time, it matters a great deal what came before a given data point: we all know that today's weather depends a lot on yesterday's weather, and so the weather of day #2 depends a lot on the weather of day #1. Product updates (and thus user satisfaction) depend a lot on user satisfaction with the previous version. By (wrongly) assuming that these data are independent, we lose a lot of the information that determines/predicts a data point._x0007__x0007_(Block bootstrapping addresses this by sampling not individual data points, but consecutive blocks of data points, so that at least the connections between the points in each block are preserved.)""",1,0,0
"You want to compare your model fit metrics, not observations, so a $t$-test is not appropriate. Your best bet may be to compare the AIC values. You can get the AIC with the `extractAIC` command on many fitted model objects in R. See the following link, where from the AIC's found, you implement the straightforward calculations for the delta values, and the \Akaike weights\"" here: http://theses.ulaval.ca/archimede/fichiers/<zipRedac>184<zipRedac>/apa.html""",1,0,0
I created a model with 13 features and 184 examples.  A colleague suggested a 14th feature which I included.  Both my CV logit regression w Lasso shrinkage and my random forest produced (slightly) worse results (started with the same seed).  By worse results I mean the percentage of my 184 training observations which were classified correctly. Is it the random nature (despite the same seed)? Or something else.,1,1,1
"hello, I have recently pace myself with materials in course. _x0007_Can you explain why 3 variables which all have two levels end up with different representation? they are all factor variables, right ?_x0007_Is this something with the order? Because the League is first of the three variables??_x0007_Please, help, thanks anyway!!",1,1,1
"Yes, in this regression setting, the fitted coefficient is a random variable so the graph refers to the variance of that random variable (which the standard error estimates).",1,0,0
> lemme check that and get back to you,1,0,1
How about a party for those who live near Stanford?,1,0,1
Thank you!,1,0,1
"After saving my answers in the quiz I get the following message:_x0007_\Your answers have been saved but not graded. Click 'Check' to grade them.\""_x0007_However I cannot see where to do this, there is no button or anything visible called \""check\"". Please help! Thanks""",1,1,1
"Hi,_x0007__x0007_I don't know how this is implemented in R, but one solution would be to transform the categorical variables into the underlying indicator (dummy variables). That is to say, if we have a variable for color with levels 'red', 'green', and 'blue', we create three binary variables that are 1 if the observation is of that color and 0 otherwise._x0007__x0007_This is what R does silently when you introduce a factor in a regression (it also drops the first variable by default, to avoid multi-collinearity). These variables would be treated like numerical variables in the clustering algorithm._x0007__x0007_This is just the first solution that came to my mind. There may be a more efficient way to do this._x0007__x0007_Cheers,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,0
This is the best online course I took so far.  thank you very much!,1,0,1
"It could be that the 14th variable contains information that were already contained in the other 13 variables. In which case, adding the 14th variable would increase your test set error (due to overfitting, aka higher variance).",1,0,1
Click the submit button.,1,0,1
You have until 11:00pm PDT on April 4 to complete all the work.,1,0,1
"Thanks - you've cleared up something for me.  I was using prcomp(..,scale=T) - your example made me realise why this is wrong for the given question (however, whilst my answer to 10.R.2 was about 0.1 to little, it's now 0.1 too much!)._x0007__x0007_EDIT.  scale=T is correct (its given in answer for 10.R.1)",1,0,0
"Hi All,_x0007__x0007_I was wondering how you deal with that fact that real world data has Nan's for features. How do account for this when you are fitting your model and when you are applying the model out of sample where Nan's pop in randomly. _x0007__x0007_Any insight on this would be appreciated._x0007__x0007_Thanks",1,1,1
"Thank you so much for the excellent course! I really enjoyed every minute of it! This was one of the best courses I have taken so far! Thank you so much professors for the several hours you've put in to produce this course, for writing and making the text books freely available (I will definitely be buying both the books now that I can decipher most of the material presented in the book), and the course staff for making this possible! _x0007__x0007_My sincere thanks once again to each and everyone who made this possible.",1,0,1
9.R.*,1,0,1
What's wrong with scale=T ? It does not make noticeable difference for the given problem anyway,1,1,1
Great! Thanks for the comment!,1,0,1
"Dear Prof. Hastie & Tibshirani,_x0007__x0007_I have been using R for more than two years. Still, I found your comments on the pros and cons of different techniques invaluable. The text book is also full of insights. Enjoyed your jokes in the videos too. Thank you so much for being my teacher. _x0007__x0007_Best regards,_x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,0
Thanks Jim_x0007_Ok I did this before clicking the save button. I guess I dont need to submit again (otherwise I get marked down another point!)_x0007_Thanks.,1,0,1
"Thanks. But, I have seen published papers in important Journals using a t-test to compare two different classification models.",1,0,1
"Dear Prof. Hastie and Prof. Tibshirani,_x0007_Thank you very much indeed for your effort._x0007_Thank you",1,0,1
Great course! Thank you Prof. Hastie & Tibshirani!,1,0,1
I agree in this case.,1,0,1
Thank you.  Looking forward to it.,1,0,1
"Thanks for the example. I found the catch. Instead of scale=T pass variance. Scale original data as well._x0007__x0007_    library (pls)_x0007_    set.seed(1)_x0007_    nrows = 100_x0007_    x1 = runif(nrows)_x0007_    x2 = runif(nrows)_x0007_    d = cbind(x1,x2)_x0007_    _x0007_    m = apply(d, 2, mean) # get mean values_x0007_    v = apply(d, 2, var)  # get variance values_x0007_    mx = matrix(rep(m, times = nrows), byrow=T, nrow=nrows)_x0007_    vx = matrix(rep(v, times = nrows), byrow=T, nrow=nrows)_x0007_    _x0007_    pc = prcomp(d, scale=v) # pass variance vector, not T_x0007_    _x0007_    d.cent = (d-mx)/vx      # center and scale data_x0007_    _x0007_    print(\d.cent * rotation\"")_x0007_    print(head(as.matrix(d.cent)%*%as.matrix(pc$rotation[,1])))_x0007_    print(\""d * rotation\"")_x0007_    print(head(d%*%as.matrix(pc$rotation[,1])))_x0007_    print(\""pc$x\"")_x0007_    print(head(pc$x[,1]))""",1,0,0
"Dear Prof. Hastie and Prof. Tibshirani,_x0007_Thank you for the wonderful course. I really enjoyed it. _x0007_Though not the most important thing, I did want to have the certificate and put it one my resume, both as an encouragement for following through the full course and also as an acknowledgement for your work. :) _x0007_I am wondering, when do we get the certificate, after the course ends? I seem to complete all the quiz questions by now, although not 100% correct,my grade doesn't look bad. :)_x0007_Again, thank you!_x0007_<nameRedac_<anon_screen_name_redacted>>",1,1,1
"Is there an error in grading 7.1.R1? Apparently, I responded correctly but it was graded wrong. The result of this don't change my grade but could be useful to check by the staff for the other classmates. _x0007_ Thank you",1,1,1
"That's clearer thanks.  FYI I found this in another thread which uses libary(pls)_x0007__x0007_    train.data = data.frame(x,y)_x0007_    pcr.fit = pcr(y~.,data=train.data, scale=T)_x0007_    pcr.pred = predict(pcr.fit,x.test,ncomp=1:5)_x0007_    mean((pcr.pred-y.test)^2)",1,0,1
"If your objective is to find a model with the minimum test set error (i.e., the most predictive model), then p-values don't matter.",1,0,0
"when you added $x_{14}$ and ran cv.glmnet(), did lasso choose $x_{14}$ to be in the model?",1,1,1
"For ordinary least squares (and logistic regression), adding irrelevant variables to your model never **increases** training set error. This is why training set error is so unreliable. If you keep adding variables, the training set error always decrease or stay the same. Here's some code that demonstrates the need for a test set or cross-validation._x0007__x0007_    # generate random data_x0007_    _x0007_    set.seed(1)_x0007_    _x0007_    x1 <- rnorm(100, 10, 5)_x0007_    x2 <- rnorm(100, 20, 10)_x0007_    x3 <- rnorm(100, 30, 20)_x0007_    x4 <- rnorm(100, 40, 10)_x0007_    x5 <- rnorm(100,50,20)_x0007_    _x0007_    y <- 50 + 50*x1 + rnorm(100,0, 200)_x0007_    _x0007_    plot(x1,y)_x0007_    _x0007_    # fit the true model_x0007_    _x0007_    reg.true <- lm(y~x1)_x0007_    summary(reg.true)_x0007_    _x0007_    # overfit the model_x0007_    _x0007_    reg.overfit <- lm(y~x1 + x2 + x3 + x4 + x5)_x0007_    summary(reg.overfit)",1,0,0
"Thanks for the reply!  As a follow up, when do p-values matter?  Or do they ever matter?",1,1,1
"If your data set was generated from a well-designed experiment, then p-values can be used for inference.",1,0,0
"In Chapter 3, slide 22 of the notes, it says that to answer the question of \Is at least one predictor useful?\"", to use the F-statistic.  My questions are:_x0007__x0007_1. Is this (F statistic) only valid for multiple linear regression (as it's calculated with RSS)?  _x0007_2. If only valid for linear regression, is there an analogous statistic for other types of regression?  Logistic, polynomials, splines, etc?  _x0007__x0007_My question comes from the fact that if the F statistic is only for linear regression and it shows that none of the single or combination of predictors reduces the training error (as compared to no predictor model), then do you stop here and say there is no effect from any of the predictors?  Or do you need to try this for non-linear combinations before you can definitively say the predictors have no effect on the outcome?""",1,1,1
"In your counter examples, have you tried adding irrelevant variables? The *training set* R-square will continue to increase or stay the same. In other words, over fitting your model (by adding irrelevant variables) will inflate your *training set* R-square._x0007__x0007_Residual standard error measures the variance of your residuals. Not the same as MSE or SSE or RSS._x0007__x0007_    # generate some data_x0007_    _x0007_    set.seed(1)_x0007_    x1 <- (1:100)+rnorm(100, 10, 5)_x0007_    x2 <- log(x1)+rnorm(100, 0, 2)_x0007_    x3 <- rnorm(100,20,5)_x0007_    _x0007_    # true model_x0007_    _x0007_    y <- 50 + 10*x1 +0.2*x2+ rnorm(100,0, 200)_x0007_    _x0007_    # under-fit model_x0007_    reg.one <- lm(y~x1)_x0007_    summary(reg.one)_x0007_    _x0007_    # true model_x0007_    reg.true <- lm(y~x1 + x2 )_x0007_    summary(reg.true)_x0007_    _x0007_    # over-fit model_x0007_    reg.overfit <- lm(y~poly(x1, 3) + poly(x2, 3) + poly(x3, 10))_x0007_    summary(reg.overfit)",1,1,1
It should come shortly after the course ends. It is sent out by the Stanford EdX staff._x0007_Glad you enjoyed the class.,1,0,1
"Glad you enjoyed it._x0007_As you can imagine, it was a lot of work making the course, and so we ran out of time. We may try and add some coding assignments for the next offering, whenever that will be.",1,0,1
I join everyone else in thanking professors Hastie and Tibshirani for this fantastic course!. I learnt a lot while laughing every now and then with the jokes (at home they wouldn't believe I was learning anything) and the interviews you added to the course were extremely interesting. The book is excellent and can't wait for the reprint of ESL to buy it._x0007_Thank you so much from northern Spain (very close to the French border),1,0,1
"Thanks a lot, Prof. Hastie & Tibshirani!_x0007_It surely was a great & very pedagogical course!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>, Paris",1,0,1
"The F-test would only apply to regression, not classification (so that rules out logistic regression). _x0007__x0007_However, you can compare two nested models with ANOVA which performs an F-test, and those models could be more general and include say a higher order or spline term, e.g. `I(x^2)` or `ns(x, df=5)`. _x0007__x0007_After running a linear regression with non-significant F-test, a transformation of a predictor or interaction could lead to a better model which has a significant F-statistic.",1,0,0
"How do you want to adjust them? You can just predict on the oversampled data if you trust in it, or subset the `newdata` to original data set, or if you have class imbalance use an $F_1$ score or other metric that takes into account class imbalance. There are many options.",1,1,1
"In my original post, I noted that I'm using CV (cross-validation with lasso shrinkage), specifically the cv.glmnet function.  I understand and appreciate the points about standard regression approaches with the examples you provide.  If you run your examples with the cv.glmnet function in the glmnet package you will note that your extra random variables do not appear.  My question remains why would an extra variable cause worse results on a training set? _x0007__x0007_In the examples you provide, there is overfitting and that makes sense, but the SSE (on the training set) will go down.  I'm confused why my error is not going down as I add variables.",1,1,1
"I think that means they are assuming that the residuals will be serially uncorrelated. So for example, it is not time-series data.",1,0,1
When will the certificate be provided? Only after April 4?,1,1,1
"Let me add my thanks.  A couple of suggestions.  I'd love another course. Specifically, I envision a survey of methods (logistics, random forests, knn, etc) and illustrations of the kinds of problems best and worst suited for each.  I imagine several data sets and each method is applied (if vaguely appropriate) so the results can be compared.  _x0007__x0007_Finally, do you (the instructor) have a charity or area at Stanford to which you'd like donations made?  A link or name would be fine to show my appreciation in a more material way.",1,1,1
"I experimented with some made up data using standard linear regression and a cross-validated regression.  The code is at the end.  Both the standard and CV regressions show 1 significant variable.  The SSE of the  standard regression is 3,945,055.  The SSE of the CV regression with lambda.min is 3,900,435, and 4,547,455 with lambda.1se_x0007__x0007_On one level I get the idea that lambda.1se produces a simpler model than the lambda.min model while still being within the margin of error.  In this example, I'm struggling with using the 1 variable model from the non-CV regression or the 1 variable model from the CV regression.  Imagine using CV to determine which variables to use and then using normal (not CV) regression to fit the model.  I have the same number of variables, but I get a lower SSE (than the lambda.1se).   _x0007__x0007__x0007__x0007_ _x0007__x0007_library(glmnet)_x0007_set.seed(1)_x0007_# generate 100 observations of some x data_x0007_x1 <- (1:100)+rnorm(100, 10, 5)  # a variable_x0007_x2 <- x1+rnorm(100, 0, 10) # a variable highly correlated to x1_x0007_x3 <- rnorm(100,20,10) #completely random and independent of the other two_x0007__x0007_# true model_x0007_y <- 50 + 10*x1 - 2*x2 + rnorm(100,0, 200) # y is directly related to x1 and x2 and has no relation to x3_x0007__x0007_#create three standard regressions_x0007_reg.one <- lm(y~x1) # under-fit regression model, missing x2'_x0007_reg.two <- lm(y~x1 + x2 ) #better in that it has both the first and second_x0007_reg.three <- lm(y~x1+x2+x3) #should be overfit_x0007_# and one cross validated one_x0007_x.cv<-as.matrix(data.frame(x1,x2,x3))_x0007_reg.cv<-cv.glmnet(x.cv,y)",1,0,1
"I was wondering how to do bootstrap sampling when data arrives one at a time? In a nutshell how to do sampling with streaming data? _x0007_Pointers will be useful. Thanks,",1,1,1
"Dear Prof. Hastie and Prof. Tishbirani,_x0007__x0007_Thank you for a great class! It was very enjoyable with a light course load and the book is excellent.",1,0,1
"Thank you for the class professors--A charity link suggested above is a great idea.  I did buy the intro book to read through when I wanted a break from the computer screens. _x0007__x0007_I really enjoyed the class introducing me to some new techniques that I had heard of before but I hadn't used.  It was great learning more about R from master builders, even though I've used R for 3+ years.  And I like the duo lectures which were fun and held my attention better._x0007__x0007_I would definitely be interested in a second class if that is in the plans, either with much more data analysis work required or a course going through the advanced book._x0007__x0007_Thanks again for all of your hard work in putting together this offering.",1,0,1
"I'm not sure I follow; there is no streaming algorithm here, unless you consider the random bootstrap sampling procedure the algorithm. I think you would just want to take a fresh bootstrap sample after every new time point is added. If that's not feasible, for each new incoming data point, choose randomly with probability $2/3$ whether to add that to your bootstrap sample. I believe that will be the streaming version of bootstrap since in a bootstrap, as we've seen in quiz question, the probability of a given point being in boostrap sample is approximately $2/3$ -- in the code you can be precise and specify it exactly as $1 - 1/e$.",1,0,0
people might also try the following as opposed to python script. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2<zipRedac>14/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2<zipRedac>14-course-material-feedback/threads/52dfa51e6afc9dd<zipRedac>39<zipRedac><zipRedac><zipRedac><zipRedac>df,1,0,1
Prof  Hastie posted that the certificates should come shortly after the class is finished. I usually figure 2 to 3 weeks after the class is complete.,1,0,1
"please reference them, a $t$-test is a difference of means; you have no means, you have two classification accuracies (or you could consider them \means\"" of their own accuracy, but minor). to use $n$ from the original data set sample size in a \""$t$-test\"" on the classification accuracies strikes me as very suspect. one could compute the mean classification via CV error, but I don't know how one would justify the independence of the samples.""",1,0,0
"this paper says that a paired t-test on random train/test splits should \never be used\"" and cautions against a paired t-test on 10-fold CV results.  _x0007__x0007_[Approximate Statistical Tests for Comparing Supervised Classi_x000C_cation Learning Algorithms][1]_x0007__x0007__x0007_  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf""",1,0,1
"How do we apply cross-validation for time series type of data, where the sequence is important/relevant ?",1,1,1
we were demonstrated earlier that is it possible to include interaction terms in GAM model in any case it is quite easy to code this by hand for 3x3 matrix. I was looking for this specifically in formulation of the question do I miss anything?,1,1,1
"I personally prefer the EdX platform to coursera. The videos here are HD which make it much easier to read (especially important for programming classes). The discussion areas on coursera, while easier to read, fill up with discussions that have no connection with class material.",1,0,1
"Dear all,_x0007_I believe that most of us have many common interests, and I would like to suggest to connect each other on Linkedin._x0007__x0007_I am an italian entrepreneur, I have a PhD in Astrophysics and am now working in the world of digital communication._x0007_I believe in data-driven business, and that's why I decided to improve my knowledge about statistics._x0007__x0007_If you have a Linkedin profile, I encourage you to leave the link to your profile in this conversation, so that we can connect each other (not only with me, but all those who reply)._x0007_If you are reading this discussion you have probably completed the course (or you are getting there), therefore I would also suggest that we could give each other an endorsement on \Statistics\""._x0007__x0007_Of course most of us do not each other in person, so please do not leave the link if you do not wish to connect with people you do not know._x0007__x0007_Here is my profile:_x0007_<redacted>_x0007__x0007__x0007_@moderators_x0007_I hope that this post does not violate any of the forum rules, it was done with good faith. Nevertheless, if you find that there is something wrong, please accept my apologies and remove/modify the post accordingly._x0007__x0007_Best regards,_x0007_<nameRedac_<anon_screen_name_redacted>>-""",1,0,1
It was a fantastic learning experience over the last ten weeks. I cannot thank the professors enough for all that they have done to make it all so enjoyable. Special thanks to all who were involved in offering the course._x0007_  _x0007_I know Stanford is offering the two day course based on the advanced book in Washington DC later this year. The fee is steep and I am wondering whether there is any discount for Federal Government employees.,1,1,1
"\Introduction to Statistical Learning\"" was_x0007_the book I have always wanted to read and never have been able _x0007_to find time._x0007__x0007_So I was just looking around, and then the course simply has sucked me in._x0007__x0007_Thank You so much!""",1,0,1
"Okay, that's a good idea. I10.Q know we haven't covered that but I wanted to know how to adapt bootstrap sampling to my online application.",1,0,1
"I also had another related issue when trying using Firefox on Windows Vista on a laptop, when trying to use the full-screen mode. The first time I tried, it worked OK. Then (in the same time frame that the CC toggling was reported to stop working), when I tried to go to full screen the video actually got MUCH smaller (about 1/4 the size of the standard format)!_x0007__x0007_At it appears, I suspect that someone has be making modifications to the course platform without adequate testing, and without considering the usability and robustness of such changes on users with different browsers and operating systems.",0,1,1
"I know at least two other people who would be interested in seeing the videos and the book.  Can I direct them somewhere, or is this only available during the course?",1,1,1
Can someone help me create a matrix for x please?  I need to run OLS on y~x where x is a matrix.,1,1,1
"there's several forum threads on this, here's one. make sure to download the videos by april 4 as they may be taken down. _x0007__x0007_https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/discussion/forum/i4x-HumanitiesScience-Stats216-course-Winter2014-course-material-feedback/threads/531b32bb2cee0f<phoneRedac>013",1,0,1
"the paper used it, without justifying it. in contrast, the paper i linked explicitly discusses the use of the test, which is quite different from evidence that someone has applied it. i wanted to see the papers because i hoped they would motivate the use of the test; they did not. i suggest looking the paper i linked, esp Sects. 3.3-3.4 about the t-test.",1,0,1
"Thanks fo the great course._x0007__x0007_A tip for next time that would make things simpler for those downloading the videos. It would be great if the file naming convention you use fit with the naming convention in the Courseware section. For example, the video for Chapter 6.4, when downloaded, has the filename StatsLearning_Lect8de_<zipRedac><zipRedac>09<zipRedac>3. It would be great if the file name contained something like \Lect6.4\""_x0007__x0007_Just a suggestion that would make things easier for those of us downloading the videos and watching them offline then having to come back and work out which video relates to which quiz. Obviously, we can make this association when downloading the videos but it's an extra step that makes the whole thing take quite a lot longer._x0007__x0007_Thanks again,_x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
Thanks for your response! that makes sense.,1,0,1
"I am still trying to finish all the quiz.  However I feel like I should thank you first before this course closes. This is great class, which really encouraged me to explore more classes offered by Stanford online.  I can't say I understood everything taught in the class but I did enjoy every bit of it.  Thank you so much._x0007_p.s. I am going to do PCA using R at work pretty soon.  Great timing!",1,0,1
"Hello! I have a question:_x0007__x0007_The last notice about the course was: \The new course termination date is April 4, an extension by two weeks. This applies to all the quizzes, and access to course materials\""._x0007__x0007_That last part **\""access to course materials\""**  is worrying me, **what does it mean as access?** Because what I used to do was download the videos from youtube and watch them in my ipad (for convinience because sometimes I didn't have access to internet), I did this for most of them so I'm worried that my record will not count in that matter. I've already watched every video, some online, some in my ipad._x0007__x0007__x0007_ _x0007_Thank you.""",1,1,1
"Any luck with Springer yet on this one. I would still be keen to purchase Elements of Statistical Learning, especially if it put back on discount.",1,0,1
"Thanks Professors. This was a wonderful course, much different from the MOOCs I have previously tried. What makes a big difference is the accompanying book, which not only makes it easier, rather enhances the whole experience. Thanks again.",1,0,1
Thank you.  I will try this.,1,0,1
Thanks very much. I am learning statistics and the use of a *t*-test for this kind of comparison did not look to me very appropriate. That was the reason to open this threat. _x0007__x0007_Thanks again for your help and time and best regards,1,0,0
Hi!_x0007__x0007_I would be really grateful if someone could help me find R script for doing Principal Components Regression._x0007__x0007_Thank you!,1,0,1
"I agree..the naming convention doesn't make sense after downloading. I renamed all the videos to match the titles as I downloaded them over the weeks. _x0007__x0007_For example, **2-1 Introduction to Regression Models** makes more sense to me, especially for future reference.",1,1,1
"I found Stanford EdX experience inferior to Coursera. I understand Stanford reasons for pushing with its own offering. However the student experience at Coursera from my point of view is better._x0007__x0007_Specifically, the discussion forums on Coursera are much easier to use and navigate. Coursera also offers better testing options. Just as the poster above I have reservations about peer assessment option, but it's an option and having an option cannot be worse than not having it. Other options that I've seen on Coursera, are easier to use and do not get in a way of testing as much as EdX's JavaScript heavy parsing. I also found that I use 1.25 video speed on Coursera a lot. Unfortunately it's not available on EdX.",0,1,1
"I had no problem with the SVM, in theory, but then, after simulating for as many experiments as suggested here (1000), I expected to nail the correct percentages down to within 10%, as required.  But no, the first answer I gave was graded wrong.  I tried computing the averages using two different methods, by averaging the hits or by summing the hits and then dividing by the total number of instances and, in each case, by subtracting the end result from 1, yielding the same, presumably wrong, answers both times.",1,0,1
"Why don't you do just_x0007__x0007_    olsx = data.frame(y, x)_x0007_    fit  = lm(y~., olsx)",1,0,1
"Hi All,_x0007__x0007_After saving the answers, it says \you need to click on check to grade yours answers\""_x0007__x0007_How can I do that ?_x0007__x0007_Thanks_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>""",1,1,1
Here is the error I got in in step 5:_x0007_[youtube] setting language_x0007_WARNING: Unable to set language: <urlopen error [Errno <zipRedac><zipRedac>003] getaddrinfo failed>_x0007__x0007_Please help.,1,1,1
"Did you click the \submit\"" button?""",1,0,1
look on page 256 of the textbook,1,0,1
"Dear Students_x0007__x0007_Thanks for nice comments about our course! It means a lot to us that_x0007_so many of you appreciated our efforts._x0007__x0007_Here is a charity that we like, if you'd like to make a donation:_x0007__x0007_http://www.zimbabweparaguay.net/_x0007__x0007_sincerely_x0007__x0007_Trevor & <nameRedac_<anon_screen_name_redacted>>",1,0,1
"It's been hours and still no output. What can I do different?_x0007__x0007_install.packages(\caret\"")_x0007_library(caret)_x0007__x0007_nrow(dataset<zipRedac>)_x0007_<zipRedac>606<zipRedac><zipRedac>_x0007__x0007_ncol(dataset<zipRedac>)_x0007_3<zipRedac>_x0007__x0007_data.unknown <- subset(dataset<zipRedac>,dataset<zipRedac>$hicov==\""na\"")   # Hold this dataset out for the test dataset to predict healthcare coverage_x0007_dim(data.unknown)_x0007_[<zipRedac>] <zipRedac>878<zipRedac>9     3<zipRedac>_x0007__x0007_data.known <- subset(dataset<zipRedac>,dataset<zipRedac>$hicov!=\""na\"")_x0007__x0007_dim(data.known)_x0007_[<zipRedac>] <zipRedac>7<zipRedac>76<zipRedac>     3<zipRedac>_x0007__x0007_intrain <- createDataPartition(y = data.known$hicov,p=0.<zipRedac>0,list=F)_x0007__x0007_(originally I tried p=0.50)_x0007__x0007_training <- data.known[intrain,]_x0007__x0007__x0007_validation <- data.known[-intrain,]_x0007__x0007__x0007_training_noMissing <- na.omit(training)_x0007__x0007_model<zipRedac> <- glm(training_noMissing$hicov~.-aa_primarykey,data=training_noMissing,family=\""binomial\"")_x0007__x0007_model_backward <- step(model<zipRedac>,direction=\""backward\"",trace=FALSE)""",1,1,1
"For some reason, in the code above, my \$\"" was translated into backslash close parenthesis.""",1,0,1
CV error is not training set error.,1,0,1
is there some reason you don't use the `rfe` function in `caret`?_x0007_see backwards feature selection example here: http://caret.r-forge.r-project.org/featureselection.html,1,1,1
Block resampling.  It was one of the HW questions and there are forum posts about.,1,0,1
"Spot-on question by William, that answers it if a single (granted full) model takes 10-15 mins. At this point, I would try to get that time down. Either call `glm.fit` directly on a single model, or try out some other packages on a single model. I initially thought `biglm` as I've had good results with that, but I came across `speedglm` and `LiblineaR` as well that look quite promising. Once performance is much less of an issue, then (and only then) would I try backward selection.",1,0,0
There was a whole convex optimization class from Stanford run in parallel to this one. Both l1 norm and l2 norms were covered in detail.,1,0,1
"Forward, backward, and hybrid stepwise are very slow (12+ hours) for a \moderate\"" data set with 100,000 observations and 100 variables. I recommend trying lasso or elastic net to subset your variables. Then use glm() to fit your final model.""",1,0,1
Thanks!,1,0,1
http://en.savefrom.net/_x0007_    or_x0007_    http://www.4kdownload.com/ - requires registration to download subtitles,1,0,1
https://www.linkedin.com/profile/view?id=<phoneRedac>&trk=nav_responsive_tab_profile,1,0,1
"Thank you very much for this great course and your book on the same topic._x0007__x0007_I knew at the begining that I will not be able to complete all the materials, due to lack of time in this run. However a low \pass\"" bar and extra time you provided has motivated me to do atleast about 60% of the materials (initially I thought of watching just few lectures to see if it's worth taking next time it runs). I know if I want to pursue this field that I should do more, but I must say I gained a lot from this as well, much more than I planed.""",1,0,1
"Thank you! :)_x0007_The figure 5.1 on page 142, \The Elements of Statistical Learning\"" is useful too!""",1,0,0
Very useful!_x0007_Thanks!,1,0,0
ca.linkedin.com/pub/<nameRedac_<anon_screen_name_redacted>>-<nameRedac_<anon_screen_name_redacted>>-dds/35/835/31/,1,0,1
"My progress says 99% done (I got some incorrect answers)._x0007__x0007_Do I do nothing now?_x0007__x0007_Also, how do you know my mailing address. I look at my profile and I don't see an address.",1,1,1
"And I too agree with the general sentiment here. Navigating the forum is almost terrible. Very difficult to see the list of forum topics. For example, if I need to look into discussions from week 1, it may take me forever to keep going down (loading more ) and at some point, I am going to give up. Not very user friendly at all. And I personally feel that for MOOCs, discussion forum is its best benefit. _x0007__x0007_And there are Stanford professors offering the course on Coursera. Offering there would be good too (And it would be awesome if edX is able to create a user-friendly forum.)",0,1,1
http://ch.linkedin.com/pub/<nameRedac_<anon_screen_name_redacted>>-<nameRedac_<anon_screen_name_redacted>>/3a/742/594?trk=pub-pbmap,1,0,1
"Dear Prof. Hastie and Prof. Tishbirani,_x0007__x0007_I do agree with all the comments hereabove._x0007_Great course, really and the book is very easy to read and to understand for me. _x0007_English isn't my mother tongue._x0007__x0007_Best Regards,",1,0,1
"You want this book by John Foreman: http://www.amazon.com/Data-Smart-Science-Transform-Information/dp/<zipRedac><zipRedac><zipRedac>866<zipRedac>46X_x0007__x0007_It's absolutely excellent, and examples are in Excel.",1,0,1
It did not work on the network drive but worked on my desktop.  Thank you for your help!,1,0,0
"Hi everyone. This question may be out of topic, but I'd like to know deal with time series where the data is dependent. Assuming that I have a system that for certain input produces certain output which may depend also on previous states, I understand the best way to work with this problem is with a moving block bootstrap. If I apply such technique, should I take the same sample (same indexes) for both the input and the output of the system?_x0007__x0007_Thanks!",1,1,1
I know there was a discount for ESL Book._x0007__x0007_Was there ever a discount for ISLR Book? Interested in buying a hardcopy._x0007__x0007_Thx,1,1,1
There was. The Amazon price was about the same as the discounted price.,1,0,1
Is this different from the tsboot() function?,1,1,1
I loved the R exercises. I actually use the R code examples for my work!,1,0,1
Thanks for the great course.  Just what I needed R and ML.   Murray Hill lives!,1,0,1
"did you just add a weight variable? that seems like for random forest it would make it really easy to predict the binary class, as there is a one-to-one correspondence between an observation's weight and its class. there is a parameter weights used in logistic regression in R as well, but in a different way; it seems to be just a different way to represent the data. http://psychweb.psy.umt.edu/denis/datadecision/binary_logistic_R/index.html,_x0007_http://r.<zipRedac>89695.n4.nabble.com/Weights-in-binomial-glm-td<zipRedac>99<zipRedac>249.html",1,0,1
"Having exhausted my tries, I must concede defeat on 9.R.  If a correct worked solution could be sent to me by email, I would very appreciative.  I clearly had problems getting the data structure right.  I read earlier posts relating to this question and thought I saw where I was going wrong, but my final attempt was also wrong! I would be most interested in learning what I was doing wrong.  Frustratingly, I thought I understood what was required to answer the three components of the question, but clearly I did not._x0007__x0007_Thanks",0,1,1
"hi there,_x0007_i've been trying to use regsubsets on my own dataset and i'm getting warnings about \linear dependencies found\"".  Does anyone know how i can identify said linear dependencies?  am i right in thinking i could get a better model if i remove these linear dependencies? i've had a look online and can't find anything helpful on this subject..._x0007_thanks very much!""",1,1,1
"Dear Prof/s Rob and Trevor,_x0007__x0007_Thank you very much for offering and conducting this wonderful course. I learned quite a few things out of it. Providing a text book was an added plus point of the course._x0007__x0007_I shall look forward to other suitable on-line courses from Stanford.",1,0,1
http://www.linkedin.com/in/<nameRedac_<anon_screen_name_redacted>>ivantchev,1,0,1
"This doesn't hold as a hard rule but so far, none of the courses here on Stanford's OpenEdx have ever been locked or taken down. You can sign up even after the course has finished - you just can't submit answers for credit anymore._x0007__x0007_P.S.: This holds true for edX, too. It's only Coursera where **some** courses are subsequently locked (mostly because the course is starting again).",1,0,1
"when I fit the randomForest, I didn't use weights. So I applied the offset after the model was built. Weights is an option that I want to explore.",1,0,1
"Frank Harrell's rms package (  Frank E Harrell Jr (2013). rms: Regression Modeling Strategies. R package version 4.0-0. http://CRAN.R-project.org/package=rms) has two functions: \redun\"" and \""varclus\"" that may be helpful in identifying redundant variables. _x0007__x0007_From ?varclus - \""Does a hierarchical cluster analysis on variables, using the Hoeffding D statistic, squared Pearson or Spearman correlations, or proportion of observations for which two variables are both positive as similarity measures. Variable clustering is used for assessing collinearity, redundancy, and for separating variables into clusters that can be scored as a single variable, thus resulting in data reduction\""_x0007__x0007_From ?redun:  \""Uses flexible parametric additive models (see areg and its use of regression splines) to determine how well each variable can be predicted from the remaining variables.\""""",1,0,1
"I agree with the comment regarding Analytics Edge. I have taken many courses that use R, and that class has given me the most amount of practice than any other, simply because it has a high workload with 3-4 different problem sets with different data sets to work with each week. I am a polyglot programmer, so it was easy for me to pick up R, but all I needed was lots of hands-on R practice and that class provides a ton. You'd love it if that's what you are looking for. https://www.edx.org/course/mitx/mitx-15-071x-analytics-edge-1416_x0007__x0007_I think you may be too late to get a certificate in that class but if learning is your aim, it should not matter.",1,0,1
"Hello, _x0007_After loading the given dataset in R and running the lm command, I get the following error: Error in eval(predvars, data, env) : _x0007_  invalid 'envir' argument of type 'character'_x0007__x0007_Any suggestions on how to fix and run the command?_x0007__x0007_Thanks!",1,1,1
There is this [HowTo][1] on resampling from large data set in parallel when you can't access all the data at once. Can you adapt this to your case?_x0007__x0007__x0007_  [1]: http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/,1,0,1
Thank you so much!,1,0,1
"Hello,_x0007__x0007_Just a guess, but you have a row that is missing data._x0007__x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
>  I would very much appreciate a more detailed treatment of statistical learning applied to time series especially non stationary time series._x0007__x0007_Ooooh... good one! I wish I had thought to put that in too.,1,0,1
Thanks for this great course! It was an absolute pleasure.,1,0,1
Maybe EdX could fix improve the Discussion experience to get it up to Coursera standards,0,0,1
"It would have been really helpful if the Quiz at the end of the R section of Chapter 7 said \Load the data from the workspace 7.R.RData\"" instead of \""Load the data from the file 7.R.RData,\"".  I tried opening in Excel - as suggested on pages 48-49 of the text. I did click in R Studio but the page came up with stuff about \""Frisbee Sailing\"" - for some reason R Studio doesn't have a really good view of everything when you open a workspace - not least it doesn't fill my screen.  I should have looked under Type in Windows to see R Workspace as the type.  _x0007_This wasted about 30 minutes! Finally I came to this forum.""",0,1,1
"Hi all,_x0007__x0007_I'm confused with what \within <zipRedac>0 %\"" means. If I get an error rate of, say, 0.<zipRedac>23456, What does my answer should be?_x0007__x0007_Thank you for advance.""",1,1,1
"Many thanks to Trevor and Rob for a great course. A bit of humour makes a big difference and tag-teaming works well to aid understanding. Like many participants, I did not devote as much time to the course as I would have liked, given the pressures of work. For this reason especially, I am grateful that the materials could all be downloaded for later study. The free PDF version of the book was a very generous gesture. Our library already has a copy of ESL2, so although the maths looks a bit scary, I intend to dip into this too.",1,0,1
0.1 is ok,1,0,1
"0.12 is safer, though (I got the answer marked as wrong when the difference to the correct answer was 0.04)",1,0,1
"Thank you for a great course. I did not realize that we had to finish the course by March 15th in order to receive the accomplishment statement. I thought it was by the end of the course, April 4th._x0007_Can you make an exception? Part of the reason I took this course is for the accomplishment state.",1,1,1
"To create the linear model, how do you deal with this error? _x0007_> fit1=lm(y~x)_x0007_Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE) : _x0007_  invalid type (list) for variable 'x'",1,1,1
I too want to thank Prof. Hastie and Prof. Tibshirani for this great course. This is really a great course. I really appreciate your effort. I am start reading the ESL2 book. It becomes much easy to understand after this course. It is really a good book. I'll keep a copy on my desk. Hopefully an advanced course will be provided later :) Thanks again.,1,0,1
Thank you!,1,0,1
Thank you for this extraordinary effort and generosity in offering this great course to us.,1,0,1
install Mozilla Firefox and the add-in_x0007__x0007_https://addons.mozilla.org/en-US/firefox/addon/download-youtube/,1,0,1
"There are some other discussions that give most of the work for this problem; try searching for \mvrnorm\"".  Even after the class, I think it would be useful to you (all of us in the course) to work through it.""",1,0,1
"if you finish the assignments by April 4th, you should get the statement of accomplishment",1,0,1
"I'd happily add to this thread rather than starting a new one. I wish to express my admiration to the teachers, I am honored to have listened to virtual \real flesh\"" lectures by some half-gods of statistics. I appreciated the relaxed pace, the humour, and overall a feeling that students are worth attention. I really enjoyed the extra lectures and the interviews with postdocs._x0007_On a costructive-critique side, I should say that pressure to do something in practice was low, and somewhat sparse: I am presently fighting with 10.R and I guess I won't add those points, but apparently so far I made it anyway. Being an MD with some experience in statistics, it seems too easy. This is my fourth MOOC, first at Stanford, third on R, and I loved it._x0007__x0007_Thanks again, I appreciated the pdf handouts and the books, but I am going to buy the ISL book anyway._x0007__x0007_Great course!""",1,0,1
"wow thanks, I was fighting with unlist()",1,0,1
"Will I receive a certificate of accomplishment even though I finished the course today, April 2nd. I thought that since the course was extended that the certificate of accomplishment would be extended to the new class end time.",1,1,1
# to calculate proportion of variance explained....from book page 418_x0007_    pr.var =pca.out$sdev ^2_x0007_    _x0007_    # variance_x0007_    pve=pr.var/sum(pr.var ),1,0,1
"Hello Fellow Statisticians,_x0007_I have 2 queries regarding the certificate :_x0007_1) When will we receive Statement of Accomplishment?_x0007_2) Will the certificate of accomplishment be emailed to us or we Need to download it from Course site?",1,1,1
"I saw the same thing.  It will offer me the discount price on ESL, but it will not let me complete the purchase of one book:_x0007__x0007_\This product does not have sufficient stock to fulfill the requested number of copies. Please reduce the numbers of copies or remove the item from the cart.\""_x0007__x0007_Elsewhere on the website, it says that the next printing will be in about 6 weeks, by which time this discount will have expired at April 22nd.  Unfortunately it seems that it is not possible to buy ESL printed copy at the discount.""",1,0,1
"zhq, THANK YOU SO SO MUCH!!!",1,0,1
"Yes I just cannot add how much I looked forward to getting done with my day job and other responsibilities at home to listen to Rob and Trevor present the material. Taking complex concepts and giving the intuition behind them, working on the course, the R exercises were moments of bliss for me._x0007__x0007_Thank You",1,0,1
I didn't even looked at week one material as i got busy with my curriculum but i certainly hope that i'll finish this course latest by June and make good use of opportunity and resources. ISLR and ESL is a delight to read and  thanks for providing them for free.,1,0,1
"First of all, Thank you so much to Trevor, Rob and the staffs to making this excellent online course. Well prepared materials and excellent video lectures. It was a wonderful learning experience. _x0007__x0007_I am interested to have further research and study on Statistical Learning area. Can you suggest any good postgraduate courses or research programs that fit well for students finish this course?_x0007__x0007_Regards",1,1,1
Same applies to me. Can anyone reply ?,1,0,1
"You must have an email explaining things about the certificate, quoting: _x0007__x0007_We will begin generating the Statements of Accomplishment on Monday,_x0007_   April 7th and we cannot regenerate them. You have until Sunday, April_x0007_   6th at 11:59pm PDT to make any changes to your Full Name. We anticipate_x0007_   that all Statements of Accomplishment will be generated and available_x0007_   via download from your Dashboard page by Friday, April 11th.",1,0,1
Just wanted to sincerely thank the instructors for this awesome course and hope that they have plans to do a follow-up.,1,0,1
"I would like to thank the coordinators of the statistical learning course for organising this inspiring course and my fellow participants for providing constructive feedback._x0007__x0007_I have learned a lot in a very short period._x0007__x0007_Best wishes,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>>",1,0,1
"We covered Logistic loss as well as L1 penalties.  Neither of those are squared error._x0007__x0007_These methods fall into the general category of regularized M-estimators, but I wouldn't go down that path without a LOT of comfort with math.",1,0,1
"Or, would it be possible to add the book to the cart at the current (discounted) price, then click on the \Checkout\"" button to actually purchase it later when it is in stock?""",1,0,1
"You should try on Amazon.com. In my experience, Amazon shipping is better and the price is discounted there too.",1,0,1
Thanks for sharing.,1,0,1
http://datasciencemasters.org/,1,0,1
"Dear Coleagues,_x0007__x0007_After trying to watch original videos and not being able to read written parts, I found a \less geck\"" way to download video lessons in HD._x0007__x0007_To download lessons' videos in HD (1280 x 720) one needs to get Youtube address of each video and paste them to aTube Catcher (for Windows), which is a free program. When using it the program offers to download in several resolutions, in MP4 format, without needing to convert video formats..._x0007__x0007_Interviews with John Chambers, Brad Efron and Jerome Friedman are also available in Full HD (1920 x 1080)._x0007__x0007_Hope this help everyone._x0007__x0007_<nameRedac_<anon_screen_name_redacted>> @ Brazil""",1,0,1
"I will also join in and praise this course! Learning methods that weren't even taught when I was in graduate school has been great. I really appreciate being able to have copies of both books. I will even buy ESL. More courses on advanced statistical modeling would be amazing. But regardless, thank you!!!",1,0,1
Totally agree!!,1,0,1
"In the e-edition of the book, the bookmarks need completing - I did them manually where it was useful in my copy. They don't link to anything.",1,1,1
I would like to know how many complete. I'd also like to know how many crammed at the end.,1,0,1
I'd like a lot more on forecasting - maybe something about climate models?,1,1,1
Started late in the 4th week and had to rush through the material in order to catch up. Even so it would have been impossible to finish without the extension. Thank you very much for everything. I feel grateful for this fantastic course._x0007_You are great teachers and persons!_x0007__x0007_Looking forward to see you soon in the followup course!:-),1,0,0
"So, we are now at the end of the course. What a journey it was ! This was the first ever MOOC  I have participated and I will cherish it all my life. This was so different from a brick-and-mortar class. I enjoyed the challenge of balancing work life, family life with this course. I wish I were the same eager student in my (mis)spent college/university life !_x0007_No amount of words can express my heartfelt gratitude, but still I will take this opportunity- My profound thanks to you Sirs, Prof. Hastie and Prof. Tibshirani, also to Daniela, Gareth and all those who have silently worked in the background to bring this course to us._x0007_Signing off with the hope that we (you, your staff, me, my classmates) will again meet in such type of MOOC (Applied R? ESL? Time Series? Whatever…) in the near future and will have blast.",1,1,1
"I congratulate Mr. Hastie and Mr. Tibshirani and all the Statistical Learning staff for this course. I have taken previously other courses about similar material (usually under the name of \machine learning\"" or similar) and I must say that this course is excellent. I would also like to thank the authors of the books ISL and ESL for allowing us to access these books.    Very good job!... and, please, let me know if you plan more courses like these!""",1,0,1
SVD is a linear algebra technique to perform PCA,1,0,1
Just found the class was closed. I'm working on the assignments :(,0,1,1
"Yes, what happenned? I recall that the deadlines were 16:30 PDT but suddenly I see that they are 11:00 PDT and past due???",1,1,1
without them an the the jokes the course would'n be the same,1,0,1
yes,1,0,1
"same here , desdline was 16h30 PDT, I was in the middle of answering a question at 11:30 suddenly the deadline changed to 11:00 PDT and the submission button disappeared.",0,1,1
"I agree with everyone, very well done and fun course, a very high learning to time invested ratio - the way I like it!  Thanks so much.",1,0,1
"Yes, unfortunately that was a shocking surprise for me as well... :(",0,0,1
"Actually 6.10 showed 16:30 to me without submission and I was able to submit an answer to that question upon anuragchandra's remark. But I think if all the material have the same day as the deadline, their time should also be the same._x0007__x0007_I hope this confusion may provide an extension for a day or two.",1,1,1
"This is unfortunate, I booked out leave today to finish off the last chapter. Have you received any response from the lecturers?",0,1,1
"Okay, strange thing, but I now see the submit button, do you as well? I submitted for ch.9.",1,1,1
Any possibility of making all the deadlines to 16.30 PDT?,1,1,1
"Thank you! I enjoyed the course, the book, and the lectures.  Moreover, I found much of the material applicable to improving my start-up's portfolio-optimization software (sigma1.com). The concepts of standardization, scale-independence, and PCA were immediately helpful._x0007__x0007_My background was strongly influenced by AI and machine learning; statistical learning is wonderful compliment to that background.  Again, thanks for a great course, and for explaining complex material in a way that is generally easy to understand._x0007__x0007_Dave",1,0,0
"yes, same here. Strange things are happening, I wonder if they extended the deadline to 11:00 pm :/",0,1,1
"GO BACK TO YOUR QUIZZES, THEY ARE FIXED! Submit button has appeared again. I don't know why or how.",1,0,1
"Great course- my first foray into statistics and modeling, and it gave me a great overview and base from which to turn to the advanced book from the same authors.  Great experience!_x0007__x0007_(Funny aside- I was in Palo Alto just yesterday on business, and wished I had had time to drop in and say hello, but could not!)",1,0,1
"Orlando, FL.  Hotbed of all things intellectual.",1,0,1
Great - all done! good luck guys :),1,0,1
"It was a great course, and I really want to say \Thank you\"" to Professor Trevor Hastie and Professor Robert Tibshirani and the Teaching Assistant._x0007_With kind regards,_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>> (KSA)""",1,0,1
Can anyone help with this quiz? Are they looking for an numerical answer or in terms of n and k.,1,1,1
How can I enter factorial in the answer? i.e. n choose k.,1,1,1
"Hi,Yes , you just use  n and k for making the formula.Use online calculator to see the symbols.",1,0,1
me too,1,0,1
what online calculator?,1,0,1
+1 for Coursera,1,0,1
It is. I can't submit any answers,0,1,1
"Yes, but the problem should STATE the name of the matrix or dataframe.  We should not need to search for it.",0,0,1
Do you see PM next to due hour?,1,0,1
"The deadline shows as 11:00 (am) PDT...if it was pm it would be 23:00.  But, it is odd the some of the assignments now have different due times.",0,1,1
I have the same problem. The hardcopy of ESL is not available anymore:(,0,1,1
Great course! It addresses the material at a high level making it greate for people like me who take the course as part of life long learning.,1,0,1
"Now that the course is over. Here is the code for the exercise_x0007__x0007_    #10.R.2_x0007_    xxTest = rbind(x,x.test)_x0007_    pca.out=prcomp(xxTest, scale=TRUE)_x0007_    data.frame(pca.out$x[1:300,1:5],y=y)_x0007_    fit = lm(y~.,data.frame(pca.out$x[1:300,1:5],y=y))_x0007_    p = predict.lm(fit,data.frame(pca.out$x[301:1300,1:5]))_x0007_    mean((p-y.test)^2)_x0007__x0007_    #10.R.3_x0007_    dat=data.frame(x,y)_x0007_    fit=lm(y~.,data=dat)_x0007_    datTest = data.frame(x.test,y.test)_x0007_    pred = predict(fit,newdata=datTest, se.fit=TRUE)_x0007_    mean((pred$se.fit-y.test)^2)_x0007__x0007_I hope this helps. If you have any questions about how this works and I will answer just post.",1,0,1
"I began the course, watching the videos and doing the quiz... but I had problems with them. I didn´t find the concept to do the quiz in the class. So I decided to give up the course. But, I continued doing the quiz and because I can submit 5 times I got 73% of the score! Something it´s wrong....",0,1,1
"ha yes, I heard about that thanks! I ought to learn more about that.",1,0,1
thanks <redacted>. I had found similar results than you but the correct answer in the quizz are 1.088 and 3.<zipRedac>0714 respectively.,1,0,1
Thanks!,1,0,1
"For those of you wanting to download all the videos with a better resolution that the one provided in the \download video\"" functionality do the following:_x0007__x0007_1. Download youtube-dl http://rg3.github.io/youtube-dl/download.html_x0007__x0007_2. Create a list.txt file with the list of the videos urls. For this course all the videos are here below (copy/paste the list to the list.txt file without the '> ' prefix):_x0007__x0007__x0007_>     > https://www.youtube.com/watch?v=2wLfFB_6SKI_x0007_>     > https://www.youtube.com/watch?v=LvaTokhYnDw_x0007_>     > https://www.youtube.com/watch?v=WjyuiK5taS8_x0007_>     > https://www.youtube.com/watch?v=UvxHOkYQl8g_x0007_>     > https://www.youtube.com/watch?v=VusKAosxxyk_x0007_>     > https://www.youtube.com/watch?v=vVj2itVNku4_x0007_>     > https://www.youtube.com/watch?v=jwBgGS_4RQA_x0007_>     > https://www.youtube.com/watch?v=jk9S3RTAl38_x0007_>     > https://www.youtube.com/watch?v=PsE9UqoWtS4_x0007_>     > https://www.youtube.com/watch?v=J6AdoiNUyWI_x0007_>     > https://www.youtube.com/watch?v=1hbCJyM9ccs_x0007_>     > https://www.youtube.com/watch?v=3T6RXmIHbJ4_x0007_>     > https://www.youtube.com/watch?v=IFzVxLv0TKQ_x0007_>     > https://www.youtube.com/watch?v=5ONFqIk3RFg_x0007_>     > https://www.youtube.com/watch?v=sqq21-VIa1c_x0007_>     > https://www.youtube.com/watch?v=31Q5FGRnxt4_x0007_>     > https://www.youtube.com/watch?v=MpX8rVv_u4E_x0007_>     > https://www.youtube.com/watch?v=GavRXXEHGqU_x0007_>     > https://www.youtube.com/watch?v=RfrGiG1Hm3M_x0007_>     > https://www.youtube.com/watch?v=QG0pVJXT6EU_x0007_>     > https://www.youtube.com/watch?v=X4VDZDp2vqw_x0007_>     > https://www.youtube.com/watch?v=6FiNGTYAOAA_x0007_>     > https://www.youtube.com/watch?v=TxvEVc8YNlU_x0007_>     > https://www.youtube.com/watch?v=2cl7JiPzkBY_x0007_>     > https://www.youtube.com/watch?v=9TVVF7CS3F4_x0007_>     > https://www.youtube.com/watch?v=6l9V1sINzhE_x0007_>     > https://www.youtube.com/watch?v=_2ij6eaaSl0_x0007_>     > https://www.youtube.com/watch?v=nZAM5OXrktY_x0007_>     > https://www.youtube.com/watch?v=S06JpVoNaA0_x0007_>     > https://www.youtube.com/watch?v=p4BYWX7PTBM_x0007_>     > https://www.youtube.com/watch?v=BzHz0J9a6k0_x0007_>     > https://www.youtube.com/watch?v=6dSXlqHAoMk_x0007_>     > https://www.youtube.com/watch?v=YVSmsWoBKnA_x0007_>     > https://www.youtube.com/watch?v=MEMGOlJxxz0_x0007_>     > https://www.youtube.com/watch?v=91si52nk3LA_x0007_>     > https://www.youtube.com/watch?v=nLpJd_iKmrE_x0007_>     > https://www.youtube.com/watch?v=NJhMSpI2Uj8_x0007_>     > https://www.youtube.com/watch?v=LkifE44myLc_x0007_>     > https://www.youtube.com/watch?v=3p9JNaJCOb4_x0007_>     > https://www.youtube.com/watch?v=cSKzqb0EKS0_x0007_>     > https://ww""",1,0,1
"Hi Rob, Trevor and the team, many thanks for a very useful and enjoyable course! The R sessions really brought the topics to life. With this knowledge, I finally feel ready for ESL which I bought few years ago but never managed to finish. Many thanks again!",1,0,0
I'm having trouble viewing some of the downloaded videos. They seem to be very dark and muddy. Is there a replay setting that I am missing? Is anyone else experiencing the same?_x0007__x0007_Thanks!_x0007__x0007_Chris,0,1,1
"I felt it was a bit like driving a car without looking under the hood - the profs did suggest some \exercises for the reader\"" and I think I'll go through the notes again with different datasets to see what works and what doesn't and try and bash through some of the math proofs where applicable (and not too hard!).""",1,0,1
"Dear All,_x0007__x0007_I am trying to verify the signature using gpg.win and Kleopatra_x0007__x0007_When I import the OpenEdX public key file (openedx.pub), Kleopatra says \Could not determine certificate type...\"". Probably Kleopatra can not import .pub files._x0007__x0007_Do anybody have similar problems?""",1,1,1
It requires sending an inmail to connect with you. Maybe you can send me the connection request yourself._x0007_thanks,1,0,1
When will the course be offered next? I was unable to complete it this time due to other engagements.,1,1,1
"I would like to also say thank you very much. The course was great. It clarified some basic things in Elements of Statistical Learning, which I just couldn't follow just reading the book.",1,0,1
Thank you very much for this MOOC. It was fantastic! The theory concepts are explained clearly and now I feel more confident in the basics. Now I feel ready to tackle more difficult methods. Thank you again!,1,0,0
"I truly enjoyed this course.  Much thanks to Rob, Trevor and the rest of the team for taking the time to provide such a valuable resource!!",1,0,0
"Let's say we're doing a logistic regression with 10 fold cross-validation with lasso regularization. We have 200 examples (training observations)  I want to understand the steps because I'm fuzzy.  I'm pretty sure I get the first part about dividing the entire data into 10 sets with 180 observations as training data and the other 20 as test data.  Each observation is used 1 time as test data and 9 times as training. _x0007__x0007_Let's assume the algorithm is going to choose 60 lambdas (I don't really care about that detail, but that seems to be what glmnet does).  Are the following steps correct?_x0007__x0007_Does the algo performs 60 regressions (one for each lambda) on each of the 10 folds.  For each lambda we have 10 folds. THE QUESTION REALLY BUGGING ME IS: to calculate the coefficients lambda, does it just average the respective coefficients from the 10 regressions (60 averages, one for each lambda, of 10 sets of coefficients)?  Then does it calculate a mean and standard deviation of the cost function?  My understanding is that it finds the lambda with the lowest mean cost (error).  Then there is a final step of finding the highest lambda whose mean error is within 1 standard deviation of the error associated with the minimum lambda.  _x0007__x0007_My main concern is that I understand how it comes up with the coefficients.  I'm worried I'm missing something because it doesn't make sense to me that one might use a 3 fold CV because 3 is a small number to average and to calculate a standard deviation.  _x0007__x0007_I know this is a long description. If you've read this far, thanks and I appreciate any reply.",1,1,1
"Yes, a great course indeed!!! To me, it has been a refreshing set of new ideas on how to enhance my practice with financial econometric tools and I have been able to reset some algorithms that I employ for securities trading. I hope to share some of them quite soon. Again, thanks a million to Robert, Trevor and all the team!_x0007__x0007_<nameRedac_<anon_screen_name_redacted>> <nameRedac_<anon_screen_name_redacted>>, from Lima, Peru",1,0,0
Thank you.  So the primary purpose of CV is to determine the lambda parameter so that you can build the full model.  Seems almost obvious now. I appreciate your help.,1,0,0
"You could assign a special value like -1 and create a dummy variable that captures that behavior (e.g., d_never_purchase=1).",1,0,1
"Is it possible to modify the k-fold <nameRedac_<anon_screen_name_redacted>>-validation procedure to get an unbiased estimate true prediction error?  For example, a training/test set-CV hybrid could be used as follows:_x0007__x0007_1. Divide the data into k+1 randomly selected sets, setting one aside._x0007_2. Perform k-fold CV on the remaining k sets for model selection._x0007_3. Fit the selected model to the k-sets and use the predictions compared to the remaining set to get an idea of the true error._x0007_4. Finally, fit the selected model to the full data set to best train the parameters._x0007__x0007_Is this process similar to something that has a name and is often used, or is there something better to be used to determine an unbiased estimate of the true error?  I ask because in another course I am taking it has been mentioned that the simple act of using the <nameRedac_<anon_screen_name_redacted>>-validation sets to actually pick the model biases the test error estimate (if the test error estimate is taken as the CV error)._x0007__x0007_Thanks!",0,1,1
"Just found an example of what I was describing in the article at this link:_x0007__x0007_[Article][<zipRedac>]_x0007__x0007__x0007_  [<zipRedac>]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC<zipRedac>397873/_x0007__x0007_In the article, it is referred to as nested <nameRedac_<anon_screen_name_redacted>>-validation.  Basically, <zipRedac>0-fold <nameRedac_<anon_screen_name_redacted>>-validation was used except one observation was left out of the folds.  Once the model was selected, the left-out observation was used to estimate the test error.  This was then repeated until each observation was left out.  Is there more literature on whether this is worth the additional computational effort as opposed to say LOOCV?  If not, then the authors appear to be fitting 9 times more models than they would have to in LOOCV.",1,1,1
"How do you present results from LASSO in a medical journal?  Do you present standard errors, confidence intervals, and/or p-values?  Do you have to use a bootstrap method to generate standard errors?",1,1,1
https://www.linkedin.com/pub/g-<nameRedac_<anon_screen_name_redacted>>-<nameRedac_<anon_screen_name_redacted>>/0/61a/38,1,0,1
"Same here, I am very grateful for the extension, I could not have made it without it._x0007_Thanks._x0007_I think the extension should be by default in all online classes.",1,0,1
"Your #4 would prevent you from getting an unbiased error, for the same reason that you say \the simple act of using the cross-validation sets to actually pick the model biases the test error estimate (**if the test error estimate is taken as the CV error**)\"" (emphasis mine). _x0007__x0007_The fundamental idea to get unbiased error is not to test on data that you trained on. In k-fold CV, this is true: you train on (k-1) subsets and test on a single subset on didn't train on, and iterate. _x0007__x0007_While your #4 may get you the best parameters, it prevents you from *testing* the performance of those parameters since you've committed to using all of your data. In some form or another  -- CV, nested CV, a single training/test split, etc. -- you're gonna have to leave out some of your data.""",1,0,0
"Small note: I don't think you can get truly unbiased error with any kind of $K$-fold CV. In fact, there is a sort of bias-variance tradeoff with the choice of $K$. But 5- or 10-fold is thought to be good estimates. See slides 13-14 here: http://www.stat.cmu.edu/~ryantibs/datamining/lectures/19-val2-marked.pdf",1,0,1
Same here..,1,0,1
"I have put my certificate on public folder of my Dropbox and then shared a link to the certificate. I could also use other cloud sharing solutions/sites e.g. slideshare, box etc._x0007__x0007_Hope this helps!",1,0,1
gosh I hope not!  Really looking forward to being able to take this course soon!,1,0,1
"I'm very grateful to the course, but also to the entertaining and insightful interviews that inform us about the history of the field, from the very folks that wrote important recent chapters; your course will preserve a fair amount of history of statistics to be enjoyed by future generations as well; thanks!",1,0,0
"There are tons of time series-related books (some specifically address e.g. financial markets, others focus more on electrical or biological signal processing); \Introductory Time Series with R\"" by Paul Cowpertwait and Andrew Metcalfe (Springer), to name but one from the pile, has good coverage of the core topics (correlation, auto-correlation, MA/ARMA/ARIMA, ...)._x0007__x0007_There are \""Bad Data Handbook\"" by Ethan McCallum (O'Reilly), \""Flexible Imputation of Missing Data\"" by Stef van Buuren (Chapman & Hall/CRC), both of which I haven't seen, and there's \""Mining Imperfect Data\"" by Ronald K. Pearson, which I happened to have used and found reasonably informative.""",1,0,1
"Hi: I tried nabilM's suggestion above and checked today but the book was no longer in my cart. It does appear to be in stock, however.",0,0,1
"I am attempting to create a ten fold cross validation for each model created through a forward selection process.  We did something just like this in Chapter 6 using the leaps package for linear regression.  _x0007__x0007_I have been trying out a package called bestglm which is supposed to extend leaps for glm's.  The thechique with the nested for loops does not seem to work because bestglm does not seem to store all coefficents for each step, only for the best model._x0007__x0007_bestglm claims to have a native CV functionality, but it does not work with factor varibles with more than two level.  I like the nested for loop approuch anyways because I can define my own error tests.  I am doing logistic regressiion and I want to calculate AUC and Normalized Gini for each step.          _x0007__x0007_I am sure that this type of CV proceedure on a GLM is a common task so I wanted to reach out to the community and see if anyone has solved this. I guess worst case I could inspect the varibles used in each step and mannual build the cross validation for each model, but there has to be a beter way to extract this information from a list somewhere in bestglm. _x0007__x0007_Thanks in advance,_x0007__x0007_<redacted> <redacted>",1,1,1
