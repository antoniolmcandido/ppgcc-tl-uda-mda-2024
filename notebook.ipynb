{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### [PPGCC / LADESC] MDA 2024\n",
    "##### Aprendizagem de transferência (TL - Transfer Learning) e Adaptação de Domínio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Introdução\n",
    "\n",
    "Neste tutorial, abordaremos:\n",
    "\n",
    "- Contexto(s) de aprendizagem de transferência\n",
    "- Aproveitamento de modelos pré-treinados\n",
    "- Adaptação de domínio não supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "##### Lembretes da teoria"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### O contexto de aprendizagem supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Temos um conjunto de dados rotulado de $N$ amostras rotuladas: $\\left\\{ (\\vec{x}^i,y^i) \\right\\}_{i=1}^N$, onde\n",
    "- $\\vec{x}^i = \\left(x^i_1, \\dots, x^i_D\\right) \\in \\mathcal{X}$ é uma **amostra** ou **vetor de características**.\n",
    "- $y^i \\in \\mathcal{Y}$ é o **rótulo**.\n",
    "- Para classificação com $C$ classes, $\\mathcal{Y} = \\{0,\\dots,C-1\\}$, então cada $y^i$ é um **rótulo de classe**.\n",
    "- Normalmente, assumimos que cada amostra rotulada $(\\vec{x}^i,y^i)$\n",
    "é extraída de uma distribuição conjunta\n",
    "$$P(X|Y)=P(X)\\cdot P(Y|X)$$\n",
    "- Assumimos que alguma distribuição marginal de amostra $P(X)$ existe.\n",
    "- Queremos aprender $P(Y|X)$ a partir dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Até agora, consideramos principalmente a configuração tradicional de **aprendizado supervisionado**:\n",
    "\n",
    "Assumimos que os conjuntos de **treinamento** e **teste** (que supostamente representam dados futuros não vistos)\n",
    "são ambos da mesma **distribuição** e ambos rotulados.\n",
    "\n",
    "Conseguimos assumir isso porque queríamos resolver uma tarefa com um conjunto de dados e poderíamos\n",
    "portanto, dividir nosso conjunto de dados em tais conjuntos.\n",
    "\n",
    "O que acontece quando esse não é o caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Transfer learning (Aprendizagem de transferência)\n",
    "\n",
    "No mundo real, muitas vezes não temos o conjunto de treinamento perfeito para o nosso problema.\n",
    "\n",
    "O que devemos fazer quando a suposição de aprendizado supervisionado é inválida?\n",
    "\n",
    "<img src=\"./img/transfer_learning_digits.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Domínios e tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vamos começar com algumas definições para explicar o problema.\n",
    "\n",
    "- Imagine que temos um **espaço de características**, $\\mathcal{X}$\n",
    "- Por exemplo, $\\mathcal{X}$ é o espaço de imagens coloridas de tamanho 32x32, cada pixel no intervalo de 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10^7398.1131734380015\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# tamanho deste espaço de recurso \"limitado\"\n",
    "print(f'10^{math.log10(256**(32**2*3))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como de costume, temos um conjunto de treinamento $X=\\{\\vec{x}^{(i)}\\}_{i=1}^{N},\\ \\vec{x}^{(i)}\\in\\mathcal{X}$.\n",
    "- Por exemplo, CIFAR-10\n",
    "\n",
    "<img src=\"img/cifar10.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Existe alguma **distribuição de probabilidade** $P(X)$ (também conhecida como $P_{X}(\\vec{x})$) sobre nosso conjunto de treinamento.\n",
    "    - Note que não nos importamos com a distribuição sobre $\\mathcal{X}$.\n",
    "    - Por exemplo, se $X$ for CIFAR-10, a probabilidade de uma imagem toda preta deve ser muito baixa\n",
    "    - Se as classes forem desbalanceadas, probabilidades muito diferentes para membros de classes grandes e pequenas\n",
    "\n",
    "    <img src=\"img/data_dist.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Nosso **espaço de rótulos**, $\\mathcal{Y}$ inclui os rótulos possíveis para amostra em nosso problema.\n",
    "    - Por exemplo $\\mathcal{Y}=\\{0,1\\}$ em classificação binária.\n",
    "    - Podemos ter também $Y = \\{y^{(i)}\\}_{i=1}^{N}$, o conjunto de rótulos para nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Queremos aprender a função alvo $\\hat{y}=f(\\vec{x})$ que prevê um rótulo dada uma imagem.\n",
    "    - Da perspectiva probabilística, aprenda $P(\\hat{y}|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finalmente,\n",
    "    - Um **domínio** de aprendizagem $\\mathcal{D}$, é definido como $\\mathcal{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "    - Uma **tarefa** de aprendizagem $\\mathcal{T}$ é definida como $\\mathcal{T}=\\{\\mathcal{Y},P(Y|X)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Configurações do Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definição** (Pan & Yang, 2010):\n",
    "\n",
    "Dado\n",
    "- Um domínio de **origem** $\\mathcal{D}_S$ (source) e a tarefa de aprendizagem $\\mathcal{T}_S$\n",
    "- Um domínio de **alvo** $\\mathcal{D}_T$ (target) e a tarefa de aprendizagem $\\mathcal{T}_T$\n",
    "\n",
    "*A aprendizagem por transferência* visa melhorar a aprendizagem da função alvo\n",
    "usando *conhecimento* em $\\mathcal{D}_S$ e $\\mathcal{T}_S$, quando\n",
    "- $\\mathcal{D}_S \\neq \\mathcal{D}_T$, ou\n",
    "- $\\mathcal{T}_S \\neq \\mathcal{T}_T$\n",
    "\n",
    "Normalmente também há outras restrições no domínio alvo, como poucos ou nenhum rótulo disponível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quando $\\mathcal{D}_S=\\mathcal{D}_T$ e $\\mathcal{T}_S=\\mathcal{T}_T$ estamos no cenário regular de aprendizado supervisionado\n",
    "que vimos até agora.\n",
    "\n",
    "Por exemplo, dividindo o CIFAR-10 aleatoriamente em um conjunto de treinamento e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": "##### Mesmo domínio, tarefa diferente"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se, uma **tarefa** de aprendizado $\\mathcal{T}$ é definida como $\\mathcal{T}=\\{\\mathcal{Y},P(Y|X)\\}$.\n",
    "\n",
    "Então há dois casos (não mutuamente exclusivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 1: Os espaços de rótulo são diferentes, $\\mathcal{Y}_S \\neq \\mathcal{Y}_T$\n",
    "\n",
    "Por exemplo, o domínio alvo tem mais classes.\n",
    "\n",
    "<img src=\"img/cifar10_100.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 2: As distribuições condicionais do alvo são diferentes, $P(Y_S|X_S)\\neq P(Y_T|X_T)$.\n",
    "\n",
    "Este pode ser o caso quando o equilíbrio de classe é muito diferente nas distribuições de origem e de destino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Mesma tarefa, domínio diferente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se, um **domínio** de aprendizagem $\\mathcal{D}$, é definido como $\\mathcal{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "\n",
    "Novamente, dois casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caso 1: Diferentes espaços de características, $\\mathcal{X}_S \\neq \\mathcal{X}_T$.\n",
    "\n",
    "Por exemplo: $\\mathcal{X}_S$ é um espaço de imagens em tons de cinza enquanto $\\mathcal{X}_T$ é um espaço de imagens coloridas;\n",
    "documentos em diferentes idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Caso 2: Distribuições de dados diferentes, $P(X_S)\\neq P(X_T)$.\n",
    "\n",
    "Por exemplo: o domínio de origem contém imagens desenhadas à mão, enquanto o domínio de destino contém fotografias;\n",
    "documentos no mesmo idioma sobre tópicos diferentes.\n",
    "\n",
    "<img src=\"img/tl_example.png\" width=\"400\"/>\n",
    "\n",
    "Este é um cenário muito comum e geralmente chamado de **adaptação de domínio**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TL é um campo de pesquisa enorme, divido em:\n",
    "\n",
    "<img src=\"img/pan_yang.png\" width=\"1000\" />\n",
    "\n",
    "Neste tutorial, veremos dois exemplos simples, mas comuns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Parte 1: Ajuste fino de um modelo pré-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nós treinamos um modelo em um domínio de origem,\n",
    "e agora queremos usá-lo para acelerar o treinamento para um domínio diferente.\n",
    "\n",
    "Em algumas aplicações, podemos ter muito menos dados rotulados no domínio alvo, tornando inviável treinar um modelo profundo do zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exemplo comum: pré-treinar no ImageNet (1M+ imagens, 1000 classes) e então classificar, por exemplo, imagens médicas.\n",
    "\n",
    "<img src=\"img/transfer-learning-medical.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por que isso funcionaria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs capturam características hierárquicas, com camadas mais profundas capturando características de nível superior e específicas de classe\n",
    "(Zeiler & Fergus, 2013).\n",
    "\n",
    "<img src=\"img/zf1.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/zf2.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Portanto, podemos começar de um modelo pré-treinado e,\n",
    "    - \"Ajustar\" os filtros convolucionais, principalmente nas camadas mais profundas.\n",
    "    - Alterar o classificador (ou removê-lo completamente) para se adequar à nossa tarefa e treiná-lo do zero."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "# remove warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('D:/Downloads/mda-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "\n",
    "# Carregando uma CNN profunda pré-treinada no ImageNet\n",
    "# Usando ResNet18, para reduzir o tamanho do download, se necessário use algo mais profundo\n",
    "resnet18 = tv.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Congelando todas as camadas: desabilitar o rastreamento de gradiente\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Descongelando a última camada (ou o que for relevante para você)\n",
    "for p in resnet18.layer4.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "# Outra maneira de congelar: taxas de aprendizagem zero para parâmetros específicos\n",
    "opt = torch.optim.SGD([\n",
    "    dict(params=resnet18.layer1.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer2.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer3.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer4.parameters(), lr=1e-4),\n",
    "    dict(params=resnet18.fc.parameters()),\n",
    "], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Substituir a parte totalmente conectada por algum outro classificador, por exemplo:\n",
    "\n",
    "cnn_features = resnet18.fc.in_features\n",
    "num_classes = 13\n",
    "\n",
    "resnet18.fc =  nn.Sequential(\n",
    "    nn.Linear(cnn_features, 100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes, bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "# Importante: precisamos dimensionar nossos dados da mesma forma que os dados de treinamento do ImageNet\n",
    "tf = tvtf.Compose([\n",
    "    tvtf.Resize(224),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Carregando os dados do domínio alvo (CIFAR-10 usado apenas como um exemplo simples)\n",
    "ds_train = tv.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tf)\n",
    "ds_test = tv.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tf)\n",
    "\n",
    "batch_size = 8\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=2)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0517, -0.1231, -0.0341,  0.2530,  0.3909, -0.2339,  0.5353,  0.1670,\n",
       "          0.0644, -0.3603,  0.1518, -0.1385, -0.2485]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18(ds_train[0][0].unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Importante: Somente parâmetros que rastreiam gradientes podem ser passados ​​para o otimizador\n",
    "params_non_frozen = filter(lambda p: p.requires_grad, resnet18.parameters())\n",
    "opt = optim.SGD(params_non_frozen, lr=0.05, momentum=0.9)\n",
    "\n",
    "# O ajuste fino geralmente significa que queremos taxas de aprendizado menores do que o normal e\n",
    "# decaindo-as para continuar melhorando os pesos\n",
    "lr_sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.05, patience=5,)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, loss_fn, opt, lr_sched, dl_train, dl_test):\n",
    "    # Assim como no treinamento regular do classificador, basta chamar lr_scheduler.step() a cada época.\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "##### Parte 2: Adaptação de domínio não supervisionada"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vamos considerar um problema com domínios diferentes, mas uma tarefa idêntica:\n",
    "\n",
    "- Domínio de origem: MNIST\n",
    "- Domínio de destino: MNIST-M, uma versão colorida e texturizada do MNIST\n",
    "\n",
    "A tarefa em ambos os casos é a classificação usual de 10 dígitos de classe.\n",
    "\n",
    "<img src=\"img/mnist_m.png\" />"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Exemplo de UDA com treinamento adversarial\n",
    "> DANN (Ganin et al. 2015) - Domain-Adversarial Training of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Configuração de DA **Não supervisionado**:\n",
    "\n",
    "Presumimos que não há **rótulos disponíveis** para o domínio alvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Precisamos forçar uma CNN a aprender características das formas dos dígitos apenas, não das distribuições de cores.\n",
    "\n",
    "A abordagem, (com base em Ganin et al. 2015):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Treinar um classificador para o **domínio** de uma imagem com base em recursos convolucionais profundos.\n",
    "    - Tentar maximizar a perda do classificador ```domain classifier``` ao treinar a CNN (**perda de confusão** - $Loss$ $L_D$).\n",
    "    - Simultaneamente, minimizar a perda de classificação do ```label predictor``` no domínio de origem (**perda de rótulos** - $Loss$ $L_y$). usando os mesmos recursos convolucionais.\n",
    "- Treinar o classificador de dígitos com dados do domínio de origem e o classificador de domínio com dados de ambos os domínios.\n",
    "\n",
    "<img src=\"img/ganin_da.png\" width=\"1400\" />"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "L_{total} = \\frac{1}{n} \\sum_{i=1}^{n} L_y^i (\\theta_f,\\theta_y) - \\lambda \\left( \\frac{1}{n} \\sum_{i=1}^{n} L_d^i (\\theta_f,\\theta_d) + \\frac{1}{n^\\prime} \\sum_{i=1}^{n^\\prime} L_d^i (\\theta_f,\\theta_d) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Dados de domínio fonte e alvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Observação: para que o próximo bloco seja executado, você deve [baixar](https://1drv.ms/u/c/1ce6b4820c5c5590/EXf0iwWLDdFPn9pxMxqSPugB9vrtoWygqlEzfJ9ws8vQjw?e=JrIcf2) manualmente o conjunto de dados MNIST-M e descompactá-lo em `data_dir` que você definiu no início deste notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data import MNISTMDataset\n",
    "\n",
    "image_size = 28\n",
    "batch_size = 4\n",
    "\n",
    "tf_source = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "tf_target = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "ds_source = tv.datasets.MNIST(root=data_dir, train=True, transform=tf_source, download=True)\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "\n",
    "ds_target = MNISTMDataset(os.path.join(data_dir, 'mnist_m', 'mnist_m_train'),\n",
    "                          os.path.join(data_dir, 'mnist_m', 'mnist_m_train_labels.txt'),\n",
    "                         transform=tf_target)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.plots import dataset_first_n\n",
    "\n",
    "dataset_first_n(ds_source, 3, cmap='gray');\n",
    "dataset_first_n(ds_target, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nosso modelo consistirá em três partes, como na figura:\n",
    "\n",
    "- Uma CNN \"profunda\" para extração de características de imagem (2x Conv, ReLU, MaxPool)\n",
    "- Um classificador de dígitos (3x FC, ReLU)\n",
    "- Um classificador de domínio (2x FC, ReLU), com uma **camada de reversão de gradiente** (GRL).\n",
    "\n",
    "<img src=\"img/ganin_da2.png\" width=\"1400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lembre-se: GRL não é utilizado no forward (passe para a frente), mas aplica-se o fator $-\\lambda$ ao gradiente no backward (passe para trás).\n",
    "\n",
    "Como podemos implementar isso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):        \n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # gradiente reverso\n",
    "        output = grad_output.neg() * ctx.alpha        \n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DACNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64), nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 50, kernel_size=5),\n",
    "            nn.BatchNorm2d(50), nn.Dropout2d(), nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100), nn.BatchNorm1d(100), nn.Dropout2d(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, grl_lambda=1.0):        \n",
    "        x = x.expand(x.data.shape[0], 3, image_size, image_size)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(-1, 50 * 4 * 4)\n",
    "        reverse_features = GradientReversalFn.apply(features, grl_lambda)\n",
    "        \n",
    "        class_pred = self.class_classifier(features)\n",
    "        domain_pred = self.domain_classifier(reverse_features)\n",
    "        return class_pred, domain_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Por que deixar $\\lambda$ (`grl_lambda` no código) mudar durante o treinamento (por exemplo, a cada época)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No início do treinamento, a perda de domínio é extremamente ruidosa, pois os recursos da CNN ainda não são bons.\n",
    "- Portanto, lambda é gradualmente alterado de 0 para 1 no curso do treinamento.\n",
    "$$\n",
    "\\lambda_p = \\frac{2}{1+\\exp(-10\\cdot p)} -1,\n",
    "$$\n",
    "onde $p\\in[0,1]$ é o progresso do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source domain:  torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "target domain:  torch.Size([4, 3, 28, 28]) torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.1019, -2.8803, -1.3769, -1.7143, -2.4222, -2.5921, -2.7698, -2.4844,\n",
       "          -2.5541, -2.5375],\n",
       "         [-3.4192, -1.8426, -2.1175, -2.2688, -1.6946, -2.2629, -2.9015, -2.6262,\n",
       "          -2.5002, -2.4302],\n",
       "         [-2.0990, -2.3966, -2.0615, -2.3983, -2.3497, -2.6480, -2.3784, -1.7760,\n",
       "          -2.9630, -2.4259],\n",
       "         [-2.0230, -2.3446, -2.5302, -1.9031, -2.8394, -2.1098, -2.4986, -2.7489,\n",
       "          -1.9648, -2.5640]], grad_fn=<LogSoftmaxBackward0>),\n",
       " tensor([[-0.8835, -0.5333],\n",
       "         [-0.5510, -0.8589],\n",
       "         [-0.7486, -0.6406],\n",
       "         [-0.7506, -0.6388]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DACNN()\n",
    "\n",
    "x0_s, y0_s = next(iter(dl_source))\n",
    "x0_t, y0_t = next(iter(dl_target))\n",
    "\n",
    "print('source domain: ', x0_s.shape, y0_s.shape)\n",
    "print('target domain: ', x0_t.shape, y0_t.shape)\n",
    "\n",
    "model(x0_s)\n",
    "model(x0_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "\n",
    "model = DACNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Duas funções de perda: uma para classificação e outra para domínio\n",
    "loss_fn_class = torch.nn.NLLLoss()\n",
    "loss_fn_domain = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)\n",
    "\n",
    "# Vamos treinar o mesmo número de lotes de ambos os conjuntos de dados\n",
    "max_batches = min(len(dl_source), len(dl_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 / 0001\n",
      "=================\n",
      "[1/58] class_loss: 2.3930 s_domain_loss: 0.5917 t_domain_loss: 0.8492 grl_lambda: 0.000 \n",
      "[2/58] class_loss: 2.2022 s_domain_loss: 0.6275 t_domain_loss: 0.8146 grl_lambda: 0.086 \n",
      "[3/58] class_loss: 2.0432 s_domain_loss: 0.6475 t_domain_loss: 0.7572 grl_lambda: 0.171 \n",
      "[4/58] class_loss: 1.9751 s_domain_loss: 0.6730 t_domain_loss: 0.7350 grl_lambda: 0.253 \n",
      "[5/58] class_loss: 1.8643 s_domain_loss: 0.6947 t_domain_loss: 0.7020 grl_lambda: 0.332 \n",
      "[6/58] class_loss: 1.7871 s_domain_loss: 0.7108 t_domain_loss: 0.6868 grl_lambda: 0.406 \n",
      "[7/58] class_loss: 1.7116 s_domain_loss: 0.7169 t_domain_loss: 0.6739 grl_lambda: 0.476 \n",
      "[8/58] class_loss: 1.6576 s_domain_loss: 0.7133 t_domain_loss: 0.6732 grl_lambda: 0.539 \n",
      "[9/58] class_loss: 1.6038 s_domain_loss: 0.6966 t_domain_loss: 0.6854 grl_lambda: 0.598 \n",
      "[10/58] class_loss: 1.4946 s_domain_loss: 0.6816 t_domain_loss: 0.6931 grl_lambda: 0.650 \n",
      "[11/58] class_loss: 1.4315 s_domain_loss: 0.6682 t_domain_loss: 0.7085 grl_lambda: 0.697 \n",
      "[12/58] class_loss: 1.3991 s_domain_loss: 0.6515 t_domain_loss: 0.7241 grl_lambda: 0.739 \n",
      "[13/58] class_loss: 1.3901 s_domain_loss: 0.6403 t_domain_loss: 0.7361 grl_lambda: 0.776 \n",
      "[14/58] class_loss: 1.3391 s_domain_loss: 0.6319 t_domain_loss: 0.7478 grl_lambda: 0.808 \n",
      "[15/58] class_loss: 1.2922 s_domain_loss: 0.6247 t_domain_loss: 0.7593 grl_lambda: 0.836 \n",
      "[16/58] class_loss: 1.2435 s_domain_loss: 0.6248 t_domain_loss: 0.7632 grl_lambda: 0.860 \n",
      "[17/58] class_loss: 1.1879 s_domain_loss: 0.6257 t_domain_loss: 0.7620 grl_lambda: 0.881 \n",
      "[18/58] class_loss: 1.1559 s_domain_loss: 0.6318 t_domain_loss: 0.7572 grl_lambda: 0.899 \n",
      "[19/58] class_loss: 1.0867 s_domain_loss: 0.6344 t_domain_loss: 0.7532 grl_lambda: 0.914 \n",
      "[20/58] class_loss: 1.0611 s_domain_loss: 0.6467 t_domain_loss: 0.7354 grl_lambda: 0.927 \n",
      "[21/58] class_loss: 1.0355 s_domain_loss: 0.6576 t_domain_loss: 0.7246 grl_lambda: 0.938 \n",
      "Bom, apenas uma demonstração...\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "    print(f'Epoch {epoch_idx+1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "    dl_source_iter = iter(dl_source)\n",
    "    dl_target_iter = iter(dl_target)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        optimizer.zero_grad()        \n",
    "        # Progresso do treinamento e lambda GRL\n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # Treino do domínio fonte\n",
    "        X_s, y_s = next(dl_source_iter)\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long) # rótulos do domínio fonte\n",
    "\n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
    "\n",
    "        # Treino do domínio alvo\n",
    "        X_t, _ = next(dl_target_iter) # ignorando os rótulos\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long) # rótulos do domínio alvo\n",
    "\n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain)\n",
    "        \n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'[{batch_idx+1}/{max_batches}] '\n",
    "              f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "              f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "             )\n",
    "        if batch_idx == 20:\n",
    "            print('Bom, apenas uma demonstração...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualização dos embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "O espaço das features convolucionais aprendidas pelo modelo.\n",
    "\n",
    "Lembre-se, a perda de confusão de domínio deveria fazer com que as imagens de ambos os domínios parecessem iguais para o classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/ganin_da3.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Créditos de imagens**\n",
    "\n",
    "Algumas imagens neste tutorial foram tiradas e/ou adaptadas de:\n",
    "\n",
    "- Pan & Yang, 2010, A Survey on Transfer Learning\n",
    "- Zeiler & Fergus, 2013, Visualizing and Understanding Convolutional Networks\n",
    "- Y. Ganin et al. 2015, Unsupervised Domain Adaptation by Backpropagation \n",
    "- M. Wulfmeier et al., https://arxiv.org/abs/1703.01461v2\n",
    "- Sebastian Ruder, http://ruder.io/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
